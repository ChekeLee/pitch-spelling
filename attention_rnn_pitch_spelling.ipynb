{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fosfrancesco/pitch-spelling/blob/main/rnncrf_pitch_spelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fifMg_hxNSOg",
    "outputId": "a8b320f9-4231-40d9-8f10-a7b8acd1da37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hsciybUBNkur"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "import music21 as m21\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn\n",
    "import music21 as m21\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchcrf import CRF\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, notebook\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Naq6UG5jRbyK"
   },
   "source": [
    "# RNN-CRF for Pitch Spelling\n",
    "\n",
    "Dataset: different authors from ASAP collection\n",
    "Challenges:\n",
    "- extremely long sequences\n",
    "- small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mHJvrZ2Wo_e",
    "outputId": "1319f78b-cfb1-4e2a-937c-c47cf952757c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'C'), (1, 'B#'), (2, 'D--'), (3, 'C#'), (4, 'B##'), (5, 'D-'), (6, 'D'), (7, 'C##'), (8, 'E--'), (9, 'D#'), (10, 'E-'), (11, 'F--'), (12, 'E'), (13, 'D##'), (14, 'F-'), (15, 'F'), (16, 'E#'), (17, 'G--'), (18, 'F#'), (19, 'E##'), (20, 'G-'), (21, 'G'), (22, 'F##'), (23, 'A--'), (24, 'G#'), (25, 'A-'), (26, 'A'), (27, 'G##'), (28, 'B--'), (29, 'A#'), (30, 'B-'), (31, 'C--'), (32, 'B'), (33, 'A##'), (34, 'C-')]\n",
      "['D--', 'B##', 'C##', 'E--', 'F--', 'D##', 'G--', 'E##', 'F##', 'A--', 'G##', 'B--', 'C--', 'A##']\n",
      "[(0, 'P1'), (1, 'd2'), (2, 'A7'), (3, 'm2'), (4, 'A1'), (5, 'M2'), (6, 'd3'), (7, 'AA1'), (8, 'm3'), (9, 'A2'), (10, 'M3'), (11, 'd4'), (12, 'AA2'), (13, 'P4'), (14, 'A3'), (15, 'd5'), (16, 'A4'), (17, 'P5'), (18, 'd6'), (19, 'AA4'), (20, 'm6'), (21, 'A5'), (22, 'M6'), (23, 'd7'), (24, 'AA5'), (25, 'm7'), (26, 'A6'), (27, 'M7'), (28, 'd1'), (29, 'AA6')]\n"
     ]
    }
   ],
   "source": [
    "pitches_dict = {\n",
    "    0 : [\"C\",\"B#\",\"D--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    1 : [\"C#\",\"B##\",\"D-\"], # nn.Linear(input_size+context_size, 2)\n",
    "    2 : [\"D\",\"C##\",\"E--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    3 : [\"D#\",\"E-\",\"F--\"],\n",
    "    4 : [\"E\",\"D##\",\"F-\"],\n",
    "    5 : [\"F\",\"E#\",\"G--\"],\n",
    "    6 : [\"F#\",\"E##\",\"G-\"],\n",
    "    7 : [\"G\",\"F##\",\"A--\"],\n",
    "    8 : [\"G#\",\"A-\"],\n",
    "    9 : [\"A\",\"G##\",\"B--\"],\n",
    "    10 : [\"A#\",\"B-\",\"C--\"],\n",
    "    11 : [\"B\",\"A##\",\"C-\"]\n",
    "}\n",
    "\n",
    "accepted_pitches = [ii for i in pitches_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_pitches)])\n",
    "\n",
    "double_acc_pitches = [ii for i in pitches_dict.values() for ii in i if ii.endswith(\"##\") or  ii.endswith(\"--\") ]\n",
    "print(double_acc_pitches)\n",
    "\n",
    "def score2midi_numbers(score):\n",
    "    return [p.midi%12 for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "def score2pitches(score):\n",
    "    return [p.name for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "interval_dict = {\n",
    "    0 : [\"P1\",\"d2\",\"A7\"], \n",
    "    1 : [\"m2\",\"A1\"], \n",
    "    2 : [\"M2\",\"d3\",\"AA1\"], \n",
    "    3 : [\"m3\",\"A2\"],\n",
    "    4 : [\"M3\",\"d4\",\"AA2\"],\n",
    "    5 : [\"P4\",\"A3\"],\n",
    "    6 : [\"d5\",\"A4\"],\n",
    "    7 : [\"P5\",\"d6\",\"AA4\"],\n",
    "    8 : [\"m6\",\"A5\"],\n",
    "    9 : [\"M6\",\"d7\",\"AA5\"],\n",
    "    10 : [\"m7\",\"A6\"],\n",
    "    11 : [\"M7\",\"d1\",\"AA6\"]\n",
    "}\n",
    "\n",
    "accepted_intervals = [ii for i in interval_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_intervals)])\n",
    "\n",
    "def transp_score(score):\n",
    "    \"\"\" For each input return len(accepted_intervals) transposed scores\"\"\"\n",
    "    return [score.transpose(interval) for interval in accepted_intervals]\n",
    "\n",
    "def smart_transp_score(score):\n",
    "    \"\"\" For each chromatic interval chose the interval that lead to the smallest number of accidentals\"\"\"\n",
    "    scores = []\n",
    "    for chromatic_int in interval_dict.keys():\n",
    "        temp_scores = []\n",
    "        temp_acc_number = []\n",
    "        for diat_interval in interval_dict[chromatic_int]:\n",
    "            new_score = score.transpose(diat_interval)\n",
    "            temp_scores.append(new_score)\n",
    "            temp_acc_number.append(sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in score2pitches(new_score)]))\n",
    "            # print(\"choice:\", [note.name for note in temp_scores[-1].flat.notes][0:10],\"acc:\",temp_acc_number[-1] )\n",
    "        #keep only the one with the lowest number of accidentals\n",
    "        min_index = np.argmin(temp_acc_number)\n",
    "        # print(\"preferred the number\", min_index)\n",
    "        scores.append(temp_scores[min_index])\n",
    "    return scores\n",
    "\n",
    "def acc_simple_enough(score,accepted_ratio = 0.2 ):\n",
    "    pitches = score2pitches(score)\n",
    "    double_acc = sum(el in double_acc_pitches for el in pitches)\n",
    "    if double_acc/len(pitches) < accepted_ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "diatonic_pitches = [\"C\",\"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n",
    "\n",
    "# #test acc_simple_enough()\n",
    "# score = m21.converter.parse(paths[356])\n",
    "# scores = smart_transp_score(score)\n",
    "# #delete the pieces with non accepted pitches (e.g. triple sharps)\n",
    "# scores = [s for s in scores if all(pitch in accepted_pitches for pitch in score2pitches(s))]\n",
    "# for s in scores:\n",
    "#     print(s.parts[0].flat.getElementsByClass(m21.key.KeySignature)[0], \"simple enough:\", acc_simple_enough(s))\n",
    "#     print([n.name for n in s.flat.notes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw-SuL4ZB_2C"
   },
   "source": [
    "## Import ASAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrZ0BwLEj2Gp",
    "outputId": "6b521f78-28db-4d4f-f4ef-9345c5d91db9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/fosfrancesco/pitch-spelling.git\n",
    "\n",
    "basepath = \"./\" #to change if running locally or on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WaINjDS2kpxE"
   },
   "outputs": [],
   "source": [
    "# load the asap datasets\n",
    "with open(Path(basepath,'datasets','baroque_aug_asap.pkl'), 'rb') as fid:\n",
    "     dataset_baroque = pickle.load( fid)\n",
    "\n",
    "with open(Path(basepath,'datasets','classical_aug_asap.pkl'), 'rb') as fid:\n",
    "     dataset_classical = pickle.load( fid)\n",
    "\n",
    "with open(Path(basepath,'datasets','romantic_aug_asap.pkl'), 'rb') as fid:\n",
    "     dataset_romantic = pickle.load( fid)\n",
    "        \n",
    "# with open(Path(basepath,'datasets','remaining_aug_asap.pkl'), 'rb') as fid:\n",
    "#      dataset_remaining = pickle.load( fid)\n",
    "\n",
    "# merge the three files together\n",
    "# full_dict_dataset = dataset_baroque + dataset_classical + dataset_romantic + dataset_remaining\n",
    "full_dict_dataset = dataset_baroque + dataset_classical + dataset_romantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYQdkJAiTS6_",
    "outputId": "72835948-2884-40bb-e5b4-6ebbc8488895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 different pieces\n",
      "Average number of notes:  2219.6285387474977\n"
     ]
    }
   ],
   "source": [
    "paths = list(set([e[\"original_path\"] for e in full_dict_dataset ]))\n",
    "\n",
    "# print(paths)\n",
    "print(len(paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_dict_dataset ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMdKkzNnCTrK"
   },
   "source": [
    "## Chose the convenient data augmentation\n",
    "Two possibilities:\n",
    "- for each chromatic interval, take only the diatonic transposition that produce the smallest number of accidentals (or the original if present)\n",
    "- take only a certain interval of time signatures\n",
    "\n",
    "The second is probably better for a smaller and simple dataset, but gives the problem of chosing between for example F# and G# that are both present in this bigger dataset. So we go for the first criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oign7EZ9BSX4",
    "outputId": "9aa3f694-289a-43fd-fd14-9cad3b8dd3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  1\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  4\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  6\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_2/1st_no_repeat/xml_score.musicxml . Chromatic:  1\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  1\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  3\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  8\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  10\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Ballades/4/xml_score.musicxml . Chromatic:  6\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml P1 0\n",
      "['A', 'C', 'E', 'A', 'A#', 'B', 'C', 'C#', 'E', 'A']\n",
      "[9, 0, 4, 9, 10, 11, 0, 1, 4, 9]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml m2 -5\n",
      "['B-', 'D-', 'F', 'B-', 'B', 'C', 'D-', 'D', 'F', 'B-']\n",
      "[10, 1, 5, 10, 11, 0, 1, 2, 5, 10]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml M2 2\n",
      "['B', 'D', 'F#', 'B', 'B#', 'C#', 'D', 'D#', 'F#', 'B']\n",
      "[11, 2, 6, 11, 0, 1, 2, 3, 6, 11]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml m3 -3\n",
      "['C', 'E-', 'G', 'C', 'C#', 'D', 'E-', 'E', 'G', 'C']\n",
      "[0, 3, 7, 0, 1, 2, 3, 4, 7, 0]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml M3 4\n",
      "['C#', 'E', 'G#', 'C#', 'C##', 'D#', 'E', 'E#', 'G#', 'C#']\n",
      "[1, 4, 8, 1, 2, 3, 4, 5, 8, 1]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml P4 -1\n",
      "['D', 'F', 'A', 'D', 'D#', 'E', 'F', 'F#', 'A', 'D']\n",
      "[2, 5, 9, 2, 3, 4, 5, 6, 9, 2]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml d5 -6\n",
      "['E-', 'G-', 'B-', 'E-', 'E', 'F', 'G-', 'G', 'B-', 'E-']\n",
      "[3, 6, 10, 3, 4, 5, 6, 7, 10, 3]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml P5 1\n",
      "['E', 'G', 'B', 'E', 'E#', 'F#', 'G', 'G#', 'B', 'E']\n",
      "[4, 7, 11, 4, 5, 6, 7, 8, 11, 4]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml m6 -4\n",
      "['F', 'A-', 'C', 'F', 'F#', 'G', 'A-', 'A', 'C', 'F']\n",
      "[5, 8, 0, 5, 6, 7, 8, 9, 0, 5]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml M6 3\n",
      "['F#', 'A', 'C#', 'F#', 'F##', 'G#', 'A', 'A#', 'C#', 'F#']\n",
      "[6, 9, 1, 6, 7, 8, 9, 10, 1, 6]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml m7 -2\n",
      "['G', 'B-', 'D', 'G', 'G#', 'A', 'B-', 'B', 'D', 'G']\n",
      "[7, 10, 2, 7, 8, 9, 10, 11, 2, 7]\n",
      "Chopin/Etudes_op_10/2/xml_score.musicxml M7 5\n",
      "['G#', 'B', 'D#', 'G#', 'G##', 'A#', 'B', 'B#', 'D#', 'G#']\n",
      "[8, 11, 3, 8, 9, 10, 11, 0, 3, 8]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml P1 2\n",
      "['D', 'B', 'F#', 'B', 'D', 'C#', 'D', 'F#', 'F#', 'B']\n",
      "[2, 11, 6, 11, 2, 1, 2, 6, 6, 11]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml m2 -3\n",
      "['E-', 'C', 'G', 'C', 'E-', 'D', 'E-', 'G', 'G', 'C']\n",
      "[3, 0, 7, 0, 3, 2, 3, 7, 7, 0]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml M2 4\n",
      "['E', 'C#', 'G#', 'C#', 'E', 'D#', 'E', 'G#', 'G#', 'C#']\n",
      "[4, 1, 8, 1, 4, 3, 4, 8, 8, 1]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml m3 -1\n",
      "['F', 'D', 'A', 'D', 'F', 'E', 'F', 'A', 'A', 'D']\n",
      "[5, 2, 9, 2, 5, 4, 5, 9, 9, 2]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml d4 -6\n",
      "['G-', 'E-', 'B-', 'E-', 'G-', 'F', 'G-', 'B-', 'B-', 'E-']\n",
      "[6, 3, 10, 3, 6, 5, 6, 10, 10, 3]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml P4 1\n",
      "['G', 'E', 'B', 'E', 'G', 'F#', 'G', 'B', 'B', 'E']\n",
      "[7, 4, 11, 4, 7, 6, 7, 11, 11, 4]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml d5 -4\n",
      "['A-', 'F', 'C', 'F', 'A-', 'G', 'A-', 'C', 'C', 'F']\n",
      "[8, 5, 0, 5, 8, 7, 8, 0, 0, 5]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml P5 3\n",
      "['A', 'F#', 'C#', 'F#', 'A', 'G#', 'A', 'C#', 'C#', 'F#']\n",
      "[9, 6, 1, 6, 9, 8, 9, 1, 1, 6]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml m6 -2\n",
      "['B-', 'G', 'D', 'G', 'B-', 'A', 'B-', 'D', 'D', 'G']\n",
      "[10, 7, 2, 7, 10, 9, 10, 2, 2, 7]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml M6 5\n",
      "['B', 'G#', 'D#', 'G#', 'B', 'A#', 'B', 'D#', 'D#', 'G#']\n",
      "[11, 8, 3, 8, 11, 10, 11, 3, 3, 8]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml m7 0\n",
      "['C', 'A', 'E', 'A', 'C', 'B', 'C', 'E', 'E', 'A']\n",
      "[0, 9, 4, 9, 0, 11, 0, 4, 4, 9]\n",
      "Bach/Prelude/bwv_893/xml_score.musicxml d1 -5\n",
      "['D-', 'B-', 'F', 'B-', 'D-', 'C', 'D-', 'F', 'F', 'B-']\n",
      "[1, 10, 5, 10, 1, 0, 1, 5, 5, 10]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml P1 2\n",
      "['F#', 'F#', 'F#', 'F#', 'E#', 'E#', 'E#', 'E#', 'F#', 'F#']\n",
      "[6, 6, 6, 6, 5, 5, 5, 5, 6, 6]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml m2 -3\n",
      "['G', 'G', 'G', 'G', 'F#', 'F#', 'F#', 'F#', 'G', 'G']\n",
      "[7, 7, 7, 7, 6, 6, 6, 6, 7, 7]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml M2 4\n",
      "['G#', 'G#', 'G#', 'G#', 'F##', 'F##', 'F##', 'F##', 'G#', 'G#']\n",
      "[8, 8, 8, 8, 7, 7, 7, 7, 8, 8]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml m3 -1\n",
      "['A', 'A', 'A', 'A', 'G#', 'G#', 'G#', 'G#', 'A', 'A']\n",
      "[9, 9, 9, 9, 8, 8, 8, 8, 9, 9]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml P4 1\n",
      "['B', 'B', 'B', 'B', 'A#', 'A#', 'A#', 'A#', 'B', 'B']\n",
      "[11, 11, 11, 11, 10, 10, 10, 10, 11, 11]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml d5 -4\n",
      "['C', 'C', 'C', 'C', 'B', 'B', 'B', 'B', 'C', 'C']\n",
      "[0, 0, 0, 0, 11, 11, 11, 11, 0, 0]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml P5 3\n",
      "['C#', 'C#', 'C#', 'C#', 'B#', 'B#', 'B#', 'B#', 'C#', 'C#']\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml m6 -2\n",
      "['D', 'D', 'D', 'D', 'C#', 'C#', 'C#', 'C#', 'D', 'D']\n",
      "[2, 2, 2, 2, 1, 1, 1, 1, 2, 2]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml m7 0\n",
      "['E', 'E', 'E', 'E', 'D#', 'D#', 'D#', 'D#', 'E', 'E']\n",
      "[4, 4, 4, 4, 3, 3, 3, 3, 4, 4]\n",
      "Chopin/Etudes_op_25/10/xml_score.musicxml d1 -5\n",
      "['F', 'F', 'F', 'F', 'E', 'E', 'E', 'E', 'F', 'F']\n",
      "[5, 5, 5, 5, 4, 4, 4, 4, 5, 5]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml P1 -5\n",
      "['B-', 'F', 'B-', 'F', 'B-', 'B-', 'G-', 'D-', 'G-', 'D-']\n",
      "[10, 5, 10, 5, 10, 10, 6, 1, 6, 1]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml A1 2\n",
      "['B', 'F#', 'B', 'F#', 'B', 'B', 'G', 'D', 'G', 'D']\n",
      "[11, 6, 11, 6, 11, 11, 7, 2, 7, 2]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml M2 -3\n",
      "['C', 'G', 'C', 'G', 'C', 'C', 'A-', 'E-', 'A-', 'E-']\n",
      "[0, 7, 0, 7, 0, 0, 8, 3, 8, 3]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml A2 4\n",
      "['C#', 'G#', 'C#', 'G#', 'C#', 'C#', 'A', 'E', 'A', 'E']\n",
      "[1, 8, 1, 8, 1, 1, 9, 4, 9, 4]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml M3 -1\n",
      "['D', 'A', 'D', 'A', 'D', 'D', 'B-', 'F', 'B-', 'F']\n",
      "[2, 9, 2, 9, 2, 2, 10, 5, 10, 5]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml A3 6\n",
      "['D#', 'A#', 'D#', 'A#', 'D#', 'D#', 'B', 'F#', 'B', 'F#']\n",
      "[3, 10, 3, 10, 3, 3, 11, 6, 11, 6]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml A4 1\n",
      "['E', 'B', 'E', 'B', 'E', 'E', 'C', 'G', 'C', 'G']\n",
      "[4, 11, 4, 11, 4, 4, 0, 7, 0, 7]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml P5 -4\n",
      "['F', 'C', 'F', 'C', 'F', 'F', 'D-', 'A-', 'D-', 'A-']\n",
      "[5, 0, 5, 0, 5, 5, 1, 8, 1, 8]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml A5 3\n",
      "['F#', 'C#', 'F#', 'C#', 'F#', 'F#', 'D', 'A', 'D', 'A']\n",
      "[6, 1, 6, 1, 6, 6, 2, 9, 2, 9]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml M6 -2\n",
      "['G', 'D', 'G', 'D', 'G', 'G', 'E-', 'B-', 'E-', 'B-']\n",
      "[7, 2, 7, 2, 7, 7, 3, 10, 3, 10]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml A6 5\n",
      "['G#', 'D#', 'G#', 'D#', 'G#', 'G#', 'E', 'B', 'E', 'B']\n",
      "[8, 3, 8, 3, 8, 8, 4, 11, 4, 11]\n",
      "Chopin/Sonata_2/3rd/xml_score.musicxml M7 0\n",
      "['A', 'E', 'A', 'E', 'A', 'A', 'F', 'C', 'F', 'C']\n",
      "[9, 4, 9, 4, 9, 9, 5, 0, 5, 0]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml P1 -1\n",
      "['F', 'E', 'F', 'G', 'F', 'F', 'E', 'D', 'C', 'D']\n",
      "[5, 4, 5, 7, 5, 5, 4, 2, 0, 2]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml m2 -6\n",
      "['G-', 'F', 'G-', 'A-', 'G-', 'G-', 'F', 'E-', 'D-', 'E-']\n",
      "[6, 5, 6, 8, 6, 6, 5, 3, 1, 3]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml M2 1\n",
      "['G', 'F#', 'G', 'A', 'G', 'G', 'F#', 'E', 'D', 'E']\n",
      "[7, 6, 7, 9, 7, 7, 6, 4, 2, 4]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml m3 -4\n",
      "['A-', 'G', 'A-', 'B-', 'A-', 'A-', 'G', 'F', 'E-', 'F']\n",
      "[8, 7, 8, 10, 8, 8, 7, 5, 3, 5]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml M3 3\n",
      "['A', 'G#', 'A', 'B', 'A', 'A', 'G#', 'F#', 'E', 'F#']\n",
      "[9, 8, 9, 11, 9, 9, 8, 6, 4, 6]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml P4 -2\n",
      "['B-', 'A', 'B-', 'C', 'B-', 'B-', 'A', 'G', 'F', 'G']\n",
      "[10, 9, 10, 0, 10, 10, 9, 7, 5, 7]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml A4 5\n",
      "['B', 'A#', 'B', 'C#', 'B', 'B', 'A#', 'G#', 'F#', 'G#']\n",
      "[11, 10, 11, 1, 11, 11, 10, 8, 6, 8]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml P5 0\n",
      "['C', 'B', 'C', 'D', 'C', 'C', 'B', 'A', 'G', 'A']\n",
      "[0, 11, 0, 2, 0, 0, 11, 9, 7, 9]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml m6 -5\n",
      "['D-', 'C', 'D-', 'E-', 'D-', 'D-', 'C', 'B-', 'A-', 'B-']\n",
      "[1, 0, 1, 3, 1, 1, 0, 10, 8, 10]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml M6 2\n",
      "['D', 'C#', 'D', 'E', 'D', 'D', 'C#', 'B', 'A', 'B']\n",
      "[2, 1, 2, 4, 2, 2, 1, 11, 9, 11]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml m7 -3\n",
      "['E-', 'D', 'E-', 'F', 'E-', 'E-', 'D', 'C', 'B-', 'C']\n",
      "[3, 2, 3, 5, 3, 3, 2, 0, 10, 0]\n",
      "Bach/Prelude/bwv_880/xml_score.musicxml M7 4\n",
      "['E', 'D#', 'E', 'F#', 'E', 'E', 'D#', 'C#', 'B', 'C#']\n",
      "[4, 3, 4, 6, 4, 4, 3, 1, 11, 1]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml P1 -4\n",
      "['E-', 'A-', 'C', 'A-', 'C', 'E-', 'A-', 'A-', 'E-', 'G']\n",
      "[3, 8, 0, 8, 0, 3, 8, 8, 3, 7]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml A1 3\n",
      "['E', 'A', 'C#', 'A', 'C#', 'E', 'A', 'A', 'E', 'G#']\n",
      "[4, 9, 1, 9, 1, 4, 9, 9, 4, 8]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml M2 -2\n",
      "['F', 'B-', 'D', 'B-', 'D', 'F', 'B-', 'B-', 'F', 'A']\n",
      "[5, 10, 2, 10, 2, 5, 10, 10, 5, 9]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml A2 5\n",
      "['F#', 'B', 'D#', 'B', 'D#', 'F#', 'B', 'B', 'F#', 'A#']\n",
      "[6, 11, 3, 11, 3, 6, 11, 11, 6, 10]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml M3 0\n",
      "['G', 'C', 'E', 'C', 'E', 'G', 'C', 'C', 'G', 'B']\n",
      "[7, 0, 4, 0, 4, 7, 0, 0, 7, 11]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml P4 -5\n",
      "['A-', 'D-', 'F', 'D-', 'F', 'A-', 'D-', 'D-', 'A-', 'C']\n",
      "[8, 1, 5, 1, 5, 8, 1, 1, 8, 0]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml A4 2\n",
      "['A', 'D', 'F#', 'D', 'F#', 'A', 'D', 'D', 'A', 'C#']\n",
      "[9, 2, 6, 2, 6, 9, 2, 2, 9, 1]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml P5 -3\n",
      "['B-', 'E-', 'G', 'E-', 'G', 'B-', 'E-', 'E-', 'B-', 'D']\n",
      "[10, 3, 7, 3, 7, 10, 3, 3, 10, 2]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml A5 4\n",
      "['B', 'E', 'G#', 'E', 'G#', 'B', 'E', 'E', 'B', 'D#']\n",
      "[11, 4, 8, 4, 8, 11, 4, 4, 11, 3]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml M6 -1\n",
      "['C', 'F', 'A', 'F', 'A', 'C', 'F', 'F', 'C', 'E']\n",
      "[0, 5, 9, 5, 9, 0, 5, 5, 0, 4]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml m7 -6\n",
      "['D-', 'G-', 'B-', 'G-', 'B-', 'D-', 'G-', 'G-', 'D-', 'F']\n",
      "[1, 6, 10, 6, 10, 1, 6, 6, 1, 5]\n",
      "Beethoven/Piano_Sonatas/18-2/xml_score.musicxml M7 1\n",
      "['D', 'G', 'B', 'G', 'B', 'D', 'G', 'G', 'D', 'F#']\n",
      "[2, 7, 11, 7, 11, 2, 7, 7, 2, 6]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml P1 0\n",
      "['G', 'G', 'A', 'A', 'G', 'G', 'E', 'E', 'C', 'C']\n",
      "[7, 7, 9, 9, 7, 7, 4, 4, 0, 0]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml m2 -5\n",
      "['A-', 'A-', 'B-', 'B-', 'A-', 'A-', 'F', 'F', 'D-', 'D-']\n",
      "[8, 8, 10, 10, 8, 8, 5, 5, 1, 1]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml M2 2\n",
      "['A', 'A', 'B', 'B', 'A', 'A', 'F#', 'F#', 'D', 'D']\n",
      "[9, 9, 11, 11, 9, 9, 6, 6, 2, 2]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml m3 -3\n",
      "['B-', 'B-', 'C', 'C', 'B-', 'B-', 'G', 'G', 'E-', 'E-']\n",
      "[10, 10, 0, 0, 10, 10, 7, 7, 3, 3]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml M3 4\n",
      "['B', 'B', 'C#', 'C#', 'B', 'B', 'G#', 'G#', 'E', 'E']\n",
      "[11, 11, 1, 1, 11, 11, 8, 8, 4, 4]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml P4 -1\n",
      "['C', 'C', 'D', 'D', 'C', 'C', 'A', 'A', 'F', 'F']\n",
      "[0, 0, 2, 2, 0, 0, 9, 9, 5, 5]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml d5 -6\n",
      "['D-', 'D-', 'E-', 'E-', 'D-', 'D-', 'B-', 'B-', 'G-', 'G-']\n",
      "[1, 1, 3, 3, 1, 1, 10, 10, 6, 6]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml P5 1\n",
      "['D', 'D', 'E', 'E', 'D', 'D', 'B', 'B', 'G', 'G']\n",
      "[2, 2, 4, 4, 2, 2, 11, 11, 7, 7]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml m6 -4\n",
      "['E-', 'E-', 'F', 'F', 'E-', 'E-', 'C', 'C', 'A-', 'A-']\n",
      "[3, 3, 5, 5, 3, 3, 0, 0, 8, 8]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml M6 3\n",
      "['E', 'E', 'F#', 'F#', 'E', 'E', 'C#', 'C#', 'A', 'A']\n",
      "[4, 4, 6, 6, 4, 4, 1, 1, 9, 9]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml m7 -2\n",
      "['F', 'F', 'G', 'G', 'F', 'F', 'D', 'D', 'B-', 'B-']\n",
      "[5, 5, 7, 7, 5, 5, 2, 2, 10, 10]\n",
      "Schubert/Moment_Musical_no_1/xml_score.musicxml M7 5\n",
      "['F#', 'F#', 'G#', 'G#', 'F#', 'F#', 'D#', 'D#', 'B', 'B']\n",
      "[6, 6, 8, 8, 6, 6, 3, 3, 11, 11]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml P1 0\n",
      "['C', 'C', 'C', 'D', 'E', 'G', 'E', 'F', 'D', 'E']\n",
      "[0, 0, 0, 2, 4, 7, 4, 5, 2, 4]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml m2 -5\n",
      "['D-', 'D-', 'D-', 'E-', 'F', 'A-', 'F', 'G-', 'E-', 'F']\n",
      "[1, 1, 1, 3, 5, 8, 5, 6, 3, 5]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml M2 2\n",
      "['D', 'D', 'D', 'E', 'F#', 'A', 'F#', 'G', 'E', 'F#']\n",
      "[2, 2, 2, 4, 6, 9, 6, 7, 4, 6]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml m3 -3\n",
      "['E-', 'E-', 'E-', 'F', 'G', 'B-', 'G', 'A-', 'F', 'G']\n",
      "[3, 3, 3, 5, 7, 10, 7, 8, 5, 7]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml M3 4\n",
      "['E', 'E', 'E', 'F#', 'G#', 'B', 'G#', 'A', 'F#', 'G#']\n",
      "[4, 4, 4, 6, 8, 11, 8, 9, 6, 8]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml P4 -1\n",
      "['F', 'F', 'F', 'G', 'A', 'C', 'A', 'B-', 'G', 'A']\n",
      "[5, 5, 5, 7, 9, 0, 9, 10, 7, 9]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml A4 6\n",
      "['F#', 'F#', 'F#', 'G#', 'A#', 'C#', 'A#', 'B', 'G#', 'A#']\n",
      "[6, 6, 6, 8, 10, 1, 10, 11, 8, 10]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml P5 1\n",
      "['G', 'G', 'G', 'A', 'B', 'D', 'B', 'C', 'A', 'B']\n",
      "[7, 7, 7, 9, 11, 2, 11, 0, 9, 11]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml m6 -4\n",
      "['A-', 'A-', 'A-', 'B-', 'C', 'E-', 'C', 'D-', 'B-', 'C']\n",
      "[8, 8, 8, 10, 0, 3, 0, 1, 10, 0]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml M6 3\n",
      "['A', 'A', 'A', 'B', 'C#', 'E', 'C#', 'D', 'B', 'C#']\n",
      "[9, 9, 9, 11, 1, 4, 1, 2, 11, 1]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml m7 -2\n",
      "['B-', 'B-', 'B-', 'C', 'D', 'F', 'D', 'E-', 'C', 'D']\n",
      "[10, 10, 10, 0, 2, 5, 2, 3, 0, 2]\n",
      "Bach/Prelude/bwv_870/xml_score.musicxml M7 5\n",
      "['B', 'B', 'B', 'C#', 'D#', 'F#', 'D#', 'E', 'C#', 'D#']\n",
      "[11, 11, 11, 1, 3, 6, 3, 4, 1, 3]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml P1 1\n",
      "['G', 'B', 'G', 'G', 'C', 'E', 'G', 'A', 'G', 'B']\n",
      "[7, 11, 7, 7, 0, 4, 7, 9, 7, 11]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml m2 -4\n",
      "['A-', 'C', 'A-', 'A-', 'D-', 'F', 'A-', 'B-', 'A-', 'C']\n",
      "[8, 0, 8, 8, 1, 5, 8, 10, 8, 0]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml M2 3\n",
      "['A', 'C#', 'A', 'A', 'D', 'F#', 'A', 'B', 'A', 'C#']\n",
      "[9, 1, 9, 9, 2, 6, 9, 11, 9, 1]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml m3 -2\n",
      "['B-', 'D', 'B-', 'B-', 'E-', 'G', 'B-', 'C', 'B-', 'D']\n",
      "[10, 2, 10, 10, 3, 7, 10, 0, 10, 2]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml M3 5\n",
      "['B', 'D#', 'B', 'B', 'E', 'G#', 'B', 'C#', 'B', 'D#']\n",
      "[11, 3, 11, 11, 4, 8, 11, 1, 11, 3]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml P4 0\n",
      "['C', 'E', 'C', 'C', 'F', 'A', 'C', 'D', 'C', 'E']\n",
      "[0, 4, 0, 0, 5, 9, 0, 2, 0, 4]\n",
      "Haydn/Keyboard_Sonatas/39-3/xml_score.musicxml d5 -5\n",
      "['D-', 'F', 'D-', 'D-', 'G-', 'B-', 'D-', 'E-', 'D-', 'F']\n",
      "[1, 5, 1, 1, 6, 10, 1, 3, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# choose only one enharmonic version for each chromatic interval for each piece\n",
    "dict_dataset = []\n",
    "for path in paths:\n",
    "    for c in range(12):\n",
    "        pieces_to_consider = [opus for opus in full_dict_dataset \n",
    "                              if (opus[\"original_path\"] == path and opus[\"transposed_of\"] in interval_dict[c])  ]\n",
    "        # if the original is in pieces_to_consider, go with the original\n",
    "        originals = [opus for opus in pieces_to_consider if opus[\"transposed_of\"] == \"P1\"]\n",
    "        if len(originals) == 1:\n",
    "            dict_dataset.append(originals[0])\n",
    "        else: #we go with the accidental minization criteria\n",
    "            n_accidentals = [sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in opus[\"pitches\"]]) \n",
    "                            for opus in pieces_to_consider]\n",
    "            if len(pieces_to_consider)>0:\n",
    "                dict_dataset.append(pieces_to_consider[np.argmin(n_accidentals)])\n",
    "            else:\n",
    "                print(\"No options for\", path, \". Chromatic: \",c )\n",
    "\n",
    "# accepted_ks = range(-5,6)\n",
    "# dict_dataset = [e for e in full_dict_dataset if e[\"key_signature\"] in accepted_ks]\n",
    "\n",
    "#test if it worked\n",
    "for i,e in enumerate(dict_dataset):\n",
    "    print(e[\"original_path\"], e[\"transposed_of\"], e[\"key_signature\"])\n",
    "    print(e[\"pitches\"][:10])\n",
    "    print(e[\"midi_number\"][:10])\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgHkMeoJdb42",
    "outputId": "1f3d172f-3f93-47ce-aa1b-e37a2afba24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2271\n",
      "Counter({'Bach': 59, 'Beethoven': 57, 'Chopin': 34, 'Schubert': 13, 'Haydn': 11, 'Schumann': 10, 'Mozart': 6, 'Brahms': 1})\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_dataset))\n",
    "\n",
    "c = Counter()\n",
    "for p in paths:\n",
    "    c[p.split(\"/\")[0]] +=1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 initial pieces\n",
      "190 pieces after removing overlapping with musedata\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove pieces from asap that are in Musedata\n",
    "print(len(paths), \"initial pieces\")\n",
    "paths = [p for p in paths if p!= \"Bach/Prelude/bwv_865/xml_score.musicxml\"]\n",
    "print(len(paths), \"pieces after removing overlapping with musedata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GRCM2W3qqeW",
    "outputId": "d53c8146-8b32-43df-cd69-b7423f1ceda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation lenghts:  160 29\n"
     ]
    }
   ],
   "source": [
    "# Temporary remove composer with only one piece, because they create problems with sklearn stratify\n",
    "one_piece_composers = ['Balakirev','Prokofiev','Brahms','Glinka']\n",
    "paths = [p for p in paths if p.split(\"/\")[0] not in one_piece_composers]\n",
    "\n",
    "# Divide train and validation set\n",
    "path_train, path_validation = sklearn.model_selection.train_test_split(paths, test_size=0.15,stratify=[p.split(\"/\")[0] for p in paths ])\n",
    "print(\"Train and validation lenghts: \",len(path_train),len(path_validation))\n",
    "\n",
    "#Put back one piece composers in the validation dataset\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "KIZ2_bX1bom5",
    "outputId": "ae8d5b50-ed4d-4376-f9d6-e96c6aa73046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Haydn', 'Beethoven', 'Bach', 'Schumann', 'Chopin', 'Mozart', 'Schubert']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvcAAAHwCAYAAAAikkCeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAABYlAAAWJQFJUiTwAAA86ElEQVR4nO3deZgcVb3/8fc3iQSCWQgBg4IkKEsACSYIsge4KotsgoAiElERhZ/iehVBhisi6BVFuS5XhLBcAQERkX1JQBZvLgFEJCCoA4JEDSEhJCQQcn5/VHXo9HRPZpKe6cmZ9+t55qnk1Kmq011V3Z+uPnU6UkpIkiRJWv0NaHUDJEmSJDWH4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJysSgVjegJ0TEX4FhQHuLmyJJkqS8jQFeSCmNbXVDINNwDwxba621Ro4bN25kqxsiSZKkfM2cOZOXXnqp1c1YJtdw3z5u3LiRM2bMaHU7JEmSlLGJEydy//33t7e6HRX2uZckSZIyYbiXJEmSMmG4lyRJkjJhuJckSZIy0ZRwHxHtEZEa/M1qsMxOEXF9RMyJiJci4qGIODEiBjajTZIkSVJ/08zRcuYB36tT/mJtQUQcCFwFLAIuB+YA+wPfBXYG3t/EdkmSJEn9QjPD/dyUUtuKKkXEMOCnwKvApJTSfWX5KcDtwKERcURK6bImtk2SJEnKXivGuT8UWA+4qBLsAVJKiyLiZOA24JNAr4T7pUuXMmfOHObPn8/ixYtJKfXGZqUViggGDx7M0KFDGTlyJAMGeIuMJEnqXDPD/eCI+BDwZmAB8BBwZ0rp1Zp6e5bTG+us405gIbBTRAxOKS1uYvs6WLp0KX/7299YuHBhT25GWikpJRYtWsSiRYtYsGABG220kQFfkiR1qpnhfjRwcU3ZXyPiIymlO6rKNi+nf6pdQUppSUT8FdgK2ASY2dkGI6LRT9Bu0ZUGz5kzh4ULFzJo0CBGjx7N2muvbXhSn7F06VIWLFjArFmzWLhwIXPmzGHUqFGtbpYkSerDmpVkLwD2ogj4awNvA34CjAFuiIjxVXWHl9N5DdZVKR/RpLY1NH/+fABGjx7N0KFDDfbqUwYMGMDQoUMZPXo08NrxKkmS1EhTrtynlE6rKXoYOC4iXgQ+D7QBBzdjWzXbnVivvLyiP2FFyy9eXPT6WXvttZvbMKmJKsdn5XiVJElqpKcvVf+4nO5WVVa5Mj+c+irlc3uiQdUqN896xV59WUQAeLO3JElaoZ5Otf8qp9WXxh8rp5vVVo6IQcBYYAnwl55tmrR6qIR7SZKkFenpcP/Oclod1G8vp3vXqb8bMAS4p6dHypEkSZJys8rhPiLGRUSHTusRMQY4t/zvJVWzrgRmA0dExHZV9dcETi//+6NVbZckSZLU3zTjyv3hwKyIuC4ifhgRZ0XElRTDWL4VuB74z0rllNILwMeBgcC0iDgvIr4FPAjsSBH+L29Cu9SHtbW1ERFMmzat1U2RJEnKRjNGy5lKMXb924GdKfrXzwXuohj3/uJUcydgSulXEbE78FXgEGBN4Angc8D3a+u30pgvX9fqJnSq/cz9mrOe9nbGjh3L0UcfzZQpU5qyTkmSJPWuVQ735Q9U3bHCih2XuxvYd1W3r9XTCSecwBFHHMGb3/zmVjdFK6k3Pvg268OrJEn9RTN/oVbqslGjRvlrq5IkSU3mAO+ira2NsWPHAnDhhRcSEcv+pkyZwrRp04gI2tramD59Ovvttx8jR44kImhvbwdg6tSpHHvssWy55ZYMGzaMtdZai6233prTTjuNRYsW1d1mvT73EcGkSZOYPXs2xx57LBtssAGDBw9mq6224oILLujpp0KSJGm15pV7MWnSJObOncs555zD+PHjOeigg5bN23bbbZk7dy4A9957L9/85jfZZZddOOaYY5g9ezZrrLEGAGeddRaPPvooO+20E/vttx+LFi3i7rvvpq2tjWnTpnHrrbcycODALrVn7ty57LzzzqyxxhoceuihLF68mCuuuIJjjjmGAQMGcPTRRzf7KZAkScqC4V5MmjSJMWPGcM4557DtttvS1ta23PzK1fWbb76ZH//4x3ziE5/osI4f/vCHjB07tsMPLp1yyimcfvrpXHnllRx++OFdas/vf/97PvrRj/KTn/xk2QeCE088kW222YazzjrLcC9JktSA3XLUZdtuu23dYA+wySab1P0l1c9+9rMA3HTTTV3ezpAhQzj77LOXu9K/5ZZbsvPOOzNz5kxefPHFbrZckiSpfzDcq8u23377hvMWLFjAGWecwTve8Q6GDx/OgAEDiAjWXXddAJ555pkub2fTTTdl2LBhHco32mgjAJ5//vlutlySJKl/sFuOumz06NF1y1955RX23HNPpk+fztZbb83hhx/Oeuutx+te9zoATjvtNBYvXtzl7YwYMaJu+aBBxeH66quvdq/hkiRJ/YThXl1Wr9sNwDXXXMP06dOZPHlyhxFtnn32WU477bTeaJ4kSVK/Z7gXwLL+7StzVfyJJ54A4O27vZuHnp673LwbrrkegAWLlyw37x8vFMNj/vlfLzKyZpnaupIkSeoa+9wLgHXWWYeI4Kmnnur2smPGjAHgvnvvXq786SfbOeebbU1onSRJkrrCK/cC4PWvfz077LADv/3tbznyyCPZbLPNGDhwIAcccMAKl91///1561vfysU//S8ef/QRttj6bcx65mnuvO1mdt3zXTz7zNO98AgkSZJkuNcyF198MZ/97Ge58cYbufTSS0kpseGGGy67Mt/I2muvze23385xn/4c/3fv3Tww/V7etPEYjv3MFzjq48dz07VX984DkCRJ6ucipdTqNjRdRMyYMGHChBkzZnRab+bMmQCMGzeuN5qVvZ7uJ7/NhiN6dP19WV88Vsd8+boe30b7mfv1+DYkSVoVEydO5P77778/pTSx1W0B+9xLkiRJ2TDcS5IkSZkw3EuSJEmZMNxLkiRJmTDcS5IkSZkw3EuSJEmZMNxLkiRJmTDcS5IkSZkw3EuSJEmZMNxLkiRJmTDcS5IkSZkw3EuSJEmZMNxLkiRJmTDcq1fss+M27LPjNsuVXfOLnzN+o3W45hc/7/J6Jk+eTETQ3t7e5BYub8yYMYwZM6ZHtyFJktRsg1rdgD6vbXirW9C5tnmtbsFqadKkSdxxxx2klFrdFEmSpKYx3Ktl9tx7P7aZsB2j1n9Dq5vSwW233dbqJkiSJHWb4V4tM3TYcIYO65vfjLzlLW9pdRMkSZK6zT734ne/+x0RwcEHH9ywzrhx4xg8eDBz5szh5Zdf5txzz2Xfffdl4403ZvDgwYwcOZJjP3AQd029pcvb7azP/e9+O43J79uHHTZ7E7tuPZYTP3okjz76aMN1TZkyhUMOOYRNNtmEtdZai2HDhrHzzjtzySWXLFevvb2diOCOO+4AICKW/U2aNGlZvUZ97hcvXsyZZ57J2972NoYMGcKwYcPYdddd+cUvftGhbmVbkydPpr29nSOOOIJRo0ax5pprst122/Gb3/yma0+UJElSF3nlXrzzne9k88035/rrr+e5555j3XXXXW7+9OnTefTRRznkkEMYOXIks2bN4jOf+Qw77bQT73rXu1hvvfV49tln+dU1v+b4Dx/Gqd86h/d94MMr3Z5brruGL33qGF73ujV4z/4HM+oNb+CB6b9jxx13ZJtttqm7zCc/+Um22mordtttNzbYYAOee+45rr/+eo466igee+wxvv71rwMwYsQITj31VKZMmcKTTz7JqaeeumwdK7qB9uWXX+Y973kPd9xxB1tssQXHH388Cxcu5Morr+Twww/nwQcf5Iwzzuiw3JNPPsn222/PJptswlFHHcWcOXO4/PLLOfDAA7n11lvZY489Vvq5kiRJqma4FwBHH300J510EpdeeiknnHDCcvMuvPDCZXUA1llnHZ588kk23HDD5erd/ciTHH3w3nz3G6ey70HvZ8211up2OxYueJGvf/mzDBgwgAuuup6txr992bwLvnMa3/ve9+ou9/DDD3foSvPyyy+zzz77cOaZZ3Lcccfxpje9iREjRtDW1sa0adN48sknaWtr63LbvvOd73DHHXewzz778Otf/5pBg4rT59RTT2X77bfnm9/8Ju9973vZaaedlltu2rRptLW1LfdB4oMf/CB777033/72tw33kiSpaeyWIwCOOuooBgwYsCzIV7z88stcdtllrL/++uyzzz4ADB48uEOwh6IP/UGHf4gX5s3lj7+/f6XaMfXm65k393n2OejQ5YI9QFtbG8OH1++jX6+P/BprrMHxxx/PkiVLmnKD7Pnnn09EcPbZZy8L9gDrr78+p5xyCgDnnXdeh+U23nhjTj755OXK3vOe9/DmN7+Z6dOnr3K7JEmSKgz3AmDDDTdkr7324r777uORRx5ZVn7ttdcyZ84cjjzyyOUC7R//+EcmT568rI97RDB+o3X4zteLEPvPWc+uVDtm/uEhACbusHOHecOHD2fbbbetu9xTTz3F8ccfzxZbbMGQIUOW9aM/5JBDAHjmmWdWqj0V8+fP54knnuCNb3wjW2yxRYf5e+65JwAPPPBAh3nbbrstAwcO7FC+0UYb8fzzz69SuyRJkqrZLUfLTJ48mVtuuYULL7yQs846C+jYJQeKG3D33HNPlixZwl577cUBBxzAsGHD+NeLL/PYH//A1Juv5+WXX16pNrw4/wUA1l1vvbrzR48e3aHsL3/5C9tvvz3PP/88u+66K+9+97sZPnw4AwcOpL29nQsvvJDFixevVHsq5s0rfk9ggw02qDu/Uj537twO80aMGFF3mUGDBrF06dJVapckSVI1w72WOfjggxk2bBiXXHIJZ5xxBs899xw33HAD48ePZ/z48cvqnX766bz00ktMnTp1uRFmHnp6Lj8792ym3nz9Srfh9UOHAfDcv/5Vd/6sWbM6lJ199tk899xzXHDBBUyePHm5eZdeemmHrkYro9IdqN72AZ599tnl6kmSJLWC3XK0zFprrcVhhx3G3//+d2699VZ+/vOfs2TJkuWu2gM88cQTjBw5crlgX3Hf7+5ZpTaMe1sxGs6M/727w7x58+bx4IMPdih/4oknAJZ1walWGfKyVqWbzKuvvtqldg0dOpS3vOUtPPPMMzz++OMd5k+dOhWACRMmdGl9kiRJPcFwr+VUrnxfdNFFXHTRRQwaNIgjjzxyuTpjxoxhzpw5PPTQQ8uV//Kyi7nnjlW7cXWPd+/LsOEjuOFXV/LH3y/ff72trW1Z95ja9kAxKk21m266qe4NrsCy4T6feuqpLrftmGOOIaXEF7/4xeU+FMyePXvZUJvHHHNMl9cnSZLUbHbL0XJ23nln3vrWt3LFFVfwyiuvsP/++7P++usvV+fEE0/kpptuYpddduGwww5j+PDh3Hfffdx11128a78DueW6a1Z6+0PWfj1fO+t7fOlTx/CRQ/Zdbpz7vz7+KLvttht33nnncst86lOf4oILLuD9738/hx56KG984xt5+OGHufHGGznssMO4/PLLO2xnr7324oorruB973sf++67L2uttRYbb7wxRx11VMO2feELX+CGG27gmmuuYfz48ey7774sXLiQK664gn/+85986UtfYpdddlnpxy5JkrSqvHKvDo4++mheeeWVZf+utffee3Pttdey5ZZbcvnll/Ozn/2MwYMHc97lv2bXPd+9ytt/134H8sOLr2TcNuO5+Te/4opLLmD4iHW49957GTt2bIf622yzDVOnTmWnnXbiuuuu40c/+hEvvPACv/zlLznuuOPqbuNjH/sYX/nKV5g3bx7f+ta3OOWUU/jZz37WabvWWGMNbrnlFr7xjW8A8IMf/IALL7yQTTfdlJ///OfLbkKWJElqlUgptboNTRcRMyZMmDBhxowZndabOXMmAOPGjeuNZmXvoafn9uj6t9lwRI+uvy/ri8fqmC9f1+PbaD9zvx7fhiRJq2LixIncf//996eUJra6LeCVe0mSJCkbhntJkiQpE4Z7SZIkKROGe0mSJCkThntJkiQpE4Z7SZIkKROGe6mPy3G4WkmS1DP6dbiPCACWLl3a4pZIjVXCfeV4lSRJaqRfh/vBgwcDsGDBgha3RGqscnxWjldJkqRG+nW4Hzp0KACzZs1i/vz5LF261C4Q6hNSSixdupT58+cza9Ys4LXjVZIkqZFBrW5AK40cOZIFCxawcOFCnn766VY3Z7W35OVXe3T9M+c/26Pr78uGDBnCyJEjW90MSZLUx/XrcD9gwAA22mgj5syZw/z581m8eLFX7lfB4/98sUfXv82Gw3t0/X1NRDB48GCGDh3KyJEjGTCgX3/RJkmSuqBfh3soAv6oUaMYNWpUq5uy2tvnwut6dP3tZ76zR9cvSZK0uvNSoCRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlIkeCfcR8aGISOXfxxrUeW9ETIuIeRHxYkT8b0Qc3RPtkSRJkvqDpof7iNgIOBd4sZM6JwDXAlsDlwA/Bd4ITImI/2x2myRJkqT+oKnhPiICuAB4DvhxgzpjgP8E5gDbpZSOTyl9FtgG+DPw+YjYsZntkiRJkvqDZl+5/zSwJ/ARYEGDOscAg4FzU0rtlcKU0vPAGeV/j2tyuyRJkqTsNS3cR8Q44EzgnJTSnZ1U3bOc3lhn3g01dSRJkiR10aBmrCQiBgEXA08BJ62g+ubl9E+1M1JKz0bEAmDDiBiSUlq4gu3OaDBrixW0QZIkScpOU8I98DXg7cAuKaWXVlB3eDmd12D+PGDtsl6n4V6SJEnSa1Y53EfEDhRX67+TUrp31ZvUdSmliQ3aNAOY0JttkSRJklptlfrcl91xLqLoYnNKFxerXLEf3mD+iq7sS5IkSapjVW+ofT2wGTAOWFT1w1UJOLWs89Oy7Hvl/x8rp5vVriwiNqDokvP0ivrbS5IkSVreqnbLWQz8rMG8CRT98O+iCPSVLju3AzsDe1eVVexTVUeSJElSN6xSuC9vnv1YvXkR0UYR7i9MKZ1XNesC4EvACRFxQWWs+4hYh9dG2qn7A1iSJEmSGmvWaDldllL6a0R8Efg+cF9EXA68DBwKbEgLbsyVJEmSctDr4R4gpfSDiGgHvgB8mKLv/yPAySmlC1vRJkmSJGl112PhPqXUBrR1Mv9a4Nqe2r4kSZLU36zqaDmSJEmS+gjDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUiaaE+4g4KyJui4i/RcRLETEnIh6IiFMjYt0Gy+wUEdeXdV+KiIci4sSIGNiMNkmSJEn9TbOu3H8WWBu4BTgH+B9gCdAGPBQRG1VXjogDgTuB3YCrgXOBNYDvApc1qU2SJElSvzKoSesZllJaVFsYEd8ATgK+AnyqLBsG/BR4FZiUUrqvLD8FuB04NCKOSCkZ8iVJkqRuaMqV+3rBvvSLcrppVdmhwHrAZZVgX7WOk8v/frIZ7ZIkSZL6k56+oXb/cvpQVdme5fTGOvXvBBYCO0XE4J5smCRJkpSbZnXLASAivgC8HhgObAfsQhHsz6yqtnk5/VPt8imlJRHxV2ArYBNg5gq2N6PBrC2613JJkiRp9dfUcA98AXhD1f9vBCanlP5VVTa8nM5rsI5K+YjmNk2SJEnKW1PDfUppNEBEvAHYieKK/QMR8d6U0v3N3Fa5vYn1yssr+hOavT1JkiSpL+uRPvcppX+klK4G3g2sC1xUNbtyZX54hwWXL5/bE22TJEmSctWjN9SmlJ4EHgG2iohRZfFj5XSz2voRMQgYSzFG/l96sm2SJElSbnp6tByAN5bTV8vp7eV07zp1dwOGAPeklBb3dMMkSZKknKxyuI+IzSKiQxebiBhQ/ojV+hRh/fly1pXAbOCIiNiuqv6awOnlf3+0qu2SJEmS+ptm3FC7L/DNiLgL+CvwHMWIObtTDGc5C/h4pXJK6YWI+DhFyJ8WEZcBc4ADKIbJvBK4vAntkiRJkvqVZoT7W4G3Uoxp/3aKISwXUIxjfzHw/ZTSnOoFUkq/iojdga8ChwBrAk8Anyvrpya0S5IkSepXVjncp5QeBk5YieXuprjqL0lSnzTmy9f1+Dbaz9yvx7chqf/ojRtqJUmSJPUCw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpSJVQ73EbFuRHwsIq6OiCci4qWImBcRd0XERyOi7jYiYqeIuD4i5pTLPBQRJ0bEwFVtkyRJktQfDWrCOt4P/Ah4FpgKPAW8AXgfcB6wT0S8P6WUKgtExIHAVcAi4HJgDrA/8F1g53KdkiRJkrqhGeH+T8ABwHUppaWVwog4CZgOHEIR9K8qy4cBPwVeBSallO4ry08BbgcOjYgjUkqXNaFtkiRJUr+xyt1yUkq3p5SurQ72Zfks4MflfydVzToUWA+4rBLsy/qLgJPL/35yVdslSZIk9Tc9fUPtK+V0SVXZnuX0xjr17wQWAjtFxOCebJgkSZKUm2Z0y6krIgYBHy7/Wx3kNy+nf6pdJqW0JCL+CmwFbALMXME2ZjSYtUX3WitJkiSt/nryyv2ZwNbA9Smlm6rKh5fTeQ2Wq5SP6KF2SZIkSVnqkSv3EfFp4PPAo8BRPbENgJTSxAbbnwFM6KntSpIkSX1R06/cR8QJwDnAI8AeKaU5NVUqV+aHU1+lfG6z2yZJkiTlrKnhPiJOBH4APEwR7GfVqfZYOd2szvKDgLEUN+D+pZltkyRJknLXtHAfEf9O8SNUD1IE+382qHp7Od27zrzdgCHAPSmlxc1qmyRJktQfNCXclz9AdSYwA9grpTS7k+pXArOBIyJiu6p1rAmcXv73R81olyRJktSfrPINtRFxNPAfFL84+1vg0xFRW609pTQFIKX0QkR8nCLkT4uIy4A5FL9yu3lZfvmqtkuSJEnqb5oxWs7YcjoQOLFBnTuAKZX/pJR+FRG7A18FDgHWBJ4APgd8P6WUmtAuSZIkqV9Z5XCfUmoD2lZiubuBfVd1+5IkSZIKPfkjVpIkSZJ6keFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKxKBWN0CSJEmtM+bL1/X4NtrP3K/Ht6GCV+4lSZKkTDQl3EfEoRHxg4j4bUS8EBEpIi5ZwTI7RcT1ETEnIl6KiIci4sSIGNiMNkmSJEn9TbO65ZwMjAdeBJ4GtuisckQcCFwFLAIuB+YA+wPfBXYG3t+kdkmSJEn9RrO65XwW2AwYBnyys4oRMQz4KfAqMCml9NGU0heBbYF7gUMj4ogmtUuSJEnqN5oS7lNKU1NKj6eUUheqHwqsB1yWUrqvah2LKL4BgBV8QJAkSZLUUStuqN2znN5YZ96dwEJgp4gY3HtNkiRJklZ/rRgKc/Ny+qfaGSmlJRHxV2ArYBNgZmcriogZDWZ12udfkiRJylErrtwPL6fzGsyvlI/o+aZIkiRJ+Vitf8QqpTSxXnl5RX9CLzdHkiRJaqlWXLmvXJkf3mB+pXxuzzdFkiRJykcrwv1j5XSz2hkRMQgYCywB/tKbjZIkSZJWd60I97eX073rzNsNGALck1Ja3HtNkiRJklZ/rQj3VwKzgSMiYrtKYUSsCZxe/vdHLWiXJEmStFpryg21EXEQcFD539HldMeImFL+e3ZK6QsAKaUXIuLjFCF/WkRcBswBDqAYJvNK4PJmtEuSJEnqT5o1Ws62wNE1ZZuUfwBPAl+ozEgp/Soidge+ChwCrAk8AXwO+H4Xf+lWkiRJUpWmhPuUUhvQ1s1l7gb2bcb2JUmSJK3m49z3RWO+fF2Pb6P9zP16fBt9Uluj0VObuY1Gv62mlujpfe7+7jJf2yRp9dCKG2olSZIk9QDDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpSJQa1ugCRJ/Vrb8F7Yxrye30YGxnz5uh7fRvuZ+/X4NtS/eeVekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJyoThXpIkScqE4V6SJEnKhOFekiRJysSgVjdAK6FteC9sY17Pb0OSJPUPZpde45V7SZIkKROGe0mSJCkThntJkiQpE4Z7SZIkKROGe0mSJCkThntJkiQpE4Z7SZIkKROOcy9J6hscB1v9gce5ephX7iVJkqRMGO4lSZKkTLQ03EfEhhFxfkT8PSIWR0R7RHwvItZpZbskSZKk1VHL+txHxFuAe4D1gWuAR4Htgc8Ae0fEziml51rVPkmSJGl108or9z+kCPafTikdlFL6ckppT+C7wObAN1rYNkmSJGm105JwX161fzfQDvxXzexTgQXAURGxdi83TZIkSVptterK/R7l9OaU0tLqGSml+cDdwBDgnb3dMEmSJGl11ao+95uX0z81mP84xZX9zYDbGq0kImY0mDV+5syZTJw4ceVbuJKefabnx5adOODFHt8G13b/uevpx95XH3d/lcWx7v7usiz2N3R7n/fXx91f9df93V8fd7PMnDkTYExLNl5HpJR6f6MR/w18HPh4Sum8OvO/AZwEnJRS+mYn62kU7rcGXqTo9tMTtiinj/bQ+rVq3D99l/umb3P/9F3um77N/dN39ca+GQO8kFIa24Pb6LLV+hdqU0ot+YhW+VDRqu2rc+6fvst907e5f/ou903f5v7pu/rjvmlVn/vK9z+NfoO5Uj6355siSZIk5aFV4f6xcrpZg/mbltNGffIlSZIk1WhVuJ9aTt8dEcu1ISKGAjsDC4Hf9XbDJEmSpNVVS8J9SunPwM0UNyAcXzP7NGBt4OKU0oJebpokSZK02mrlDbWfAu4Bvh8RewEzgR0oxsD/E/DVFrZNkiRJWu20ZCjMZRuP2Aj4D2BvYF3gWeBq4LSU0vMta5gkSZK0GmppuJckSZLUPK26oVaSJElSkxnuJUmSpEwY7iVJkqRMGO4lSZKkTBjuJUmSpEwY7lskItoiIkXEpFa3ZXUWEdMiwiGf+oHyfJnW6nb0poiYXD7uya1ui7onIsaU+25Kq9tSzWNKfV1vHaMR0R4R7T25jVZZ7cN9eQB0Gu7KHZgiYkwvNWu1U3kea/4Wl8/dhRExrkXtmuK+63199XjoqyJiYER8PCLuiIg5EfFKRPwzIh6KiPMi4oBWt1HNERFbRMQPIuLhiJgXES9HxN8j4rqI+GhEDG51G/urqteqpRHxlk7qTa2qO7kXm9hUETGpfAxtLdq+r3urqKc+yLTyF2rVN51W9e/hwPbAh4FDImKXlNKDLWmVWsXjYQUiYiDwG4of45sLXAc8DawBbAV8ENgC+HWLmqgmiYivAadSXBi7F7gQeBF4AzAJOA/4JLBdi5rYFVcDv6P40cgcLaHINh8FTqqdGRGbUuyrSj2tBF/3+jYPbC0npdRWWxYRPwBOAE4EJvdui9RKHg9d8gGKN7jfA7unlOZVz4yIIcAOrWiYmiciTqL4sPs34P0ppf+tU+e9wOd7u23dUR6f81ZYcfX1D4oPLh+JiK+llJbUzP9YOb0WOLhXW5YXX/f6sNW+W86qiIiDIuKSiPhTRCwo/2ZExKcjYkBN3UvLr052b7CuQ8r559aUT4yIGyNifkS8EBG3RsSOnbQplf3IR0XEf0fEs2V3iD9GxEea88i77eZyul69mRHxgfJrzrkRsSgiZkbEyY2+ni6/1p4SEX8rv9L+R0T8PCI2r6mXgKPL//616mvU9jrrHBQRJ0XE4+Xz9beIOCsi1mjQhr3K/TKnrP+niDgzIobX1Hu0bOOoBuv597JNJ9SUbxgR50bEX8r1PxcRv46Id9RZx7L7LyLi0IiYHhELy7ZdFhFvqrftFqp7PETE8Ij4YkTcHhFPl8/bv8rH3dkxv0VEnB9Fl5/F5de6v42ITzao35fODYCdyumU2jc4gJTSwpTS1NryiDg8Im4r9/Oi8vFfGhF1r/pGxB7la0PlteS6qNM9Kjq5DyUafAVcbrs9Il4fEd8tz5+XIuLBiDiorDMoIr5anmOLIuLPtcd9WW+NiDghIq6PiCfLfTSnfO3bp0G7KttfOyK+HRFPlcs9UZ5jUVN/WX/28t+XRcTssl33RRGymyaKboFtwCvAvvWCPUBKqXIls8PyXW1jRAyOiC9HxB/K14EXyvPhsAbrrTwPW0TEr8rnekFE3BUR766zzIqOgS7tgz7up8BoYLnnOCJeR3FB4h7gkUYLR8SmEXFRRDwTr3W7uiiKq/7V9SrdYjr7m1RVv8uZo6xf6Za6SUT8vyi6u7xUnuNTgMrryqmNttmDVvvXvar5w6N4v36mbNMj5T6pe8xHxA4RcWVEzCqPj79FxE8i4o2N2hXF6+LXIuKx8ryaEsU9ZBeUVS+o2Ydj6m27q/r7lfszgaXA/wLPUHQ72BM4B3gHcFRV3R8BRwDHAnfUWdcnyumPKwURsRNwK8XXVL8EngC2BaYBt3fSrhHA3cDLwJXAYOD9wPkRsTSldGGXH2Fz/Fs5va92RkScD3yE4uu4qyi+nnsn8HVgr4h4V/WVk4jYm+K5eB3FlZMngA2B9wH7RcQeKaX7y+qnAQcB4yn2ydyyvDKt9nNgV+AG4AVgX+BLwPpl+6rb/AmK/bkAuAL4J8XXtP8O7B8RO6eUKtu4EDiD4irFD+ps92iK/fTzqvVPoAjAI4Gbysc7qnwsd0XEwSml6+us61PAARRfY95BcdXjcGB8RGybUlpcZ5lWaHQ8jAO+AdxJ8RXt88CbKR7TPhGxf0rpxuoFImI/in0wGLgRuJTi+B9Psf9+VLONEfStcwPguXK6WVcql28YF1AcO7Mpjo9/UZwHewCP0fG5fS9wIMXx/WNgS4pj/B0RsWVKafYqPgYozslbKI7bayhetz4AXFWGxE9RHJM3AIspnvcfRMS/UkqXV61nJMX5ek+5vn8BGwD7A9dHxMdTSuc12P5NwBvLbSyhOGfOBNZk+S5iFRsD04G/ABeX2z4cuCYi/q1euFhJHynbd1lK6eHOKtY5T7vcxiguRtwE7A48CvwXMAQ4FLi8fB3o0NUEGEvRTegPwE8onu/DgRsi4oM1+6czK7MP+qJLgbMprtL/qqr8AIr3hH8H3lpvwSguwNwKDKV4LX6EonvJh4ADy332f2X1duo/J68DPkfxnC2sKu9O5qh2DsX723XA9cCrQKUNR1O8X0yrqt/eYD3NlMvr3hoU+3sEcFn5/0MonvPNgeNrHscxwH9TvAb+muKbvE0pjrX9I+KdKaWn6mznKop9fAPFMflPin02t3yM1wAPVtWfu0qPKqW0Wv8Bqfxr6+RvbllnTM2yb6mzvgEUgS4BO9TMexhYBKxbU74JxQl7d1VZULw4J+DAmvqfqWr3pAaP5zxgYFX5lhQvtI/04vN4NvDb8rFdCwytWWZyucwvgbVq5rWV8z5TVbYOReCbDWxZU39rir6r99eUT6m376rmTyvnzwBGVpWvTfHB4VVgdFX5xhQn5QvAFjXr+mG5rv+uKtuwXMd9dbb9jrL+VVVlg8rtLqL4qrK6/hspXtCfBQbXea5eAN5Ws8zPy3mH9YHzakXHw3BgVJ11bQj8HZhZUz6KonvAy7XPVWW5vnBudOG5env5GJZShLf3ARt3Uv/Y8nFMB4bXzBsIbFD1/8o5tgTYq6buN8t5X6p3TjTYdmV9k2vK28vya2uOzV3L8jkUYWJE1bxNysf9QM26Btfuu6rj4+FyXbWvF5XtX189jyKIzS3/XldVPqbqeDi1Zl3vqayrifv4tnKdH+vGMt1uI/CVqudhUM3zUHmOdmqwjW/XrGs7im8angeGdeMY6NI+6It/ZfufLv99XnnebFg1/0aK15whwOm1zwPF+/bMsvzImnUfXpY/CgxYQTumlHW/W1Pe3cxRWc8zwNg6y04q57e14LnO6XXvLpZ/3RsJ/Lmct1tV+WblY34CeFPNuvaiyApX12sX8BD13x/rtm2V909vHxA9cIClbvyN6eI6J5T1v1ZTfnxZ/vkGB9uHq8p2LsvuqLP+geXBkagf7hdQ9WJcNe+Ocv7re/l5/CPwwTrLPEDx5jGiwWOcDUyvKvtMub7jG7Thu+X8LavKpnS276pOnH+rM++0ct57q8q+WpadUaf+OhQB+6WaE/3mcpmtauqfW5YfUFV2IHXeaOs8B/tWlbWVZafXqb9HOe8/+9B5Vfd4WMH6vl8u++aqss+XZed0o029fm50sW2HUXxoq36enqO4eXH/mrp/KOe/vQvrnVzWvaTOvLHlvCvrnRMrWN/kmvL2srxe+PhLOW/POvOmUrwGDFzRYynrf46aN8ya7b+1zjKV4LN1VdmYsqy93raBJ4HZTdy/j5Tb27sby3S7jcDjFGFpizr1P1qu7/w625hLzYftcv6Ucv7R3TgGurQP+uIfy4f7Hah6H6e4sPMq8MPy//XCfeV9+54G6/9tveO3ps7Xyjq/YgUfAqqWaZQ5KvvvMw2Wm0SLwn25/Vxe93btZJkLqsoqGWW/Btu5muIDydDadlFzkXdFbVvVv2y65aSUGvYHjKKP9sZ1ytcFvkjxNc8mFFd7q9X2db6I4mu1Y4HvlOuo9OF7HvhFVd0J5bRDF56U0qsRcRfQaKiux1NKL9Qp/1s5XYfiKnfTVT+PEbE2xV3vZwL/ExFbpZS+Ws4bQtF1YjZwYoOuaYspumpUVPpdj4/6Q3dVvt4bRyf9IRvo0GWI5Z+visp+6dAtKqX0fEQ8AOxG8TXs78tZU4B3UXyd+CVY9vX5Byi+WqvuYlN5jBs3eIyVPpvjapbrzmPoNV09Hqrq7EzxAWZHiit+tfc8vAmofGX5znJ6Qzea1LJzozMppV9ExNUUH8Z2obiqtQtFl4aDIuIiiteJIRTfUv0jpfRANzbRG8fG3JTSn+uU/53iDXVGnXnPUHxbNbr8NwARsRXFa+tuFF1E1qxZrt59JPNSSk/UKe/scT6YUnq1wTIN7/PoZV1qY0QMpegq8kxK6dE69SuvWW+vM+/+lNL8OuXTKF633k4R0FdkZfZBn5RS+t+I+ANwTEScTtFtYgBFf/xGGr4/VJVXzu87a2dGxJEUF5Xuo7gAsrRmfnczR8X0TtrcMpm87i2h6EJYa1o5rT7fKufr7lHn/jmK97yBFFmm9vWyV/dhNuG+uyJiBMXXzGMpnvSLKL4uXkLR9+ozFF8vL5NSmh8RlwDHlX3Dp1L04RsNfC+ltKiq+vBy+o8GTZjVSfPmNiiv9F0f2MmyTZNSWgBMj4j3UfSp/1JE/Dil9DeKEysobqo8tYurXLecfnwF9V6/Em2dW6e43vNV2S+NhoGrlI+oKrua4or+hyLiK+Ub9Xspvrr7Xlp+NIbKY3z/Cppc7zHOrVPWq/u8Mys4HoiIgyn6wS+i6Gv9Z4or7UsprjDtzvLn1Ihy+gxdN7dBecufp5TSKxTf8twMy4aKOwQ4n2L40Kt5rZ9sdx4z1HncKaUl5YfqZj3mRiOoLCm3V29+5Xl/XaUgIt5JEYIGUXRn+TXF+bOU4p6jA6l5bS3N7Wz71H+cnS3TzAEjnqX4QL4yN7fPbVBe28aVeW2qWNH7zPAG82vNbVDe8vNrJf2U4lvDfSjum5ixgnC50vsgisE2zqf4Rua9KaWFNfNH0M3MUaWzvNBSGbzuzW7w4bveuVN5f//iCtZZ7/29V/dhvw33FJ/ixwKnpZrh/qIY2eMzDZb7EXAcxQ20U3ntRtr/rqlXeSN8Q4P1jO5me1smpTQ3Ih6juKoxgeKTc+XxPZBSmtBw4eVVlhmfUnqoyc3sqkobRlN0L6m1QU09UkovRcQvKI6Zd1H02zy6nF17Nayy3IEppSzH921wPEBxE/XLwHYppZnVy0TETyjCfbW55fRNFF/ZZqV8w/hFRLwNOJnixrlbytk9OQLSUihGt0kdhwEc0YPbrTgZWAvYI6U0rXpGRHyFItyvbu6i2H97AT/roW1UvzbV0+G1qcqK3mdyHvqyMxcDZ1HcjPkm4D9WUH+l9kFEbEERYl+i6HJZ78PWymYOKLptrBZWw9e9URExsE7Ar3fuVP49vME3yA2lsg9Ob+nPQ2FW7pS/qs682hCyTBlK7wYOjogdKEYOubM2zACVEV86rKv8ZLtLt1vcWpWvwAYApJRepAjHW0XEyC6u43fldNdubLdywjXrU3rlqs2k2hnllZVtKa481+7PKeX06IhYj+JK0EOp4484rcxjXB0tdzyU3kpxU2ttsB9A/eO98lzVHR4xI5XuElF++/Ew8IaIqNe9ohmeL6cb1ZnXGz+u9FZgTm2wLzV8be3jLqC4t+CQiNiys4qxkr9QW3ar+TPwpqgZcrG0Rzm9v868CWW3nlqTyml3ukJko/xG90qKm/oXUIyi05mG7w+lDvugfD+4juJq7SEppUZdSlcqc6xAs98fm2l1ed0bxGvDelabVE6rz52eeH/vkX3Yn8N9ezmdVF1YHnhfWcGyP6LoS3wVRdeUH9epcw/F0E67RUTtlaoTaNzfvs+JYpzrsRRvbtV9086meB7OL4Nx7XLrlMNCVlxAcbX21IjYvk79AdFxfN7KcFtvXrnWd3AJxeP4fxFROxTa14FhFDfxLDecXUrpboqb3Q6k+ObmdbwW+KtdQ/EGfXxE7FuvARGxY3nPwmqpk+OhHdg0qsb6LYc/a6MY0abWhRTdNT4ZEbvV2c6GTWt0D4ridx7eFfXHqR7Na93QKn10v19OfxIdf1dhQERswKqp9O1crvtbROxFcZ9IT2sHRkbENjXb/yjFKDGrnZRSO8VxvAZwXTQek3tvuncPSa3zKd5Tvl1eBKqsdxRwSlWdWsMpbuSsbst2wJEUVxuvXoU2re5Opvixqvc0uC+h2t0U79u7RMSh1TPK/+8K/InimxwiYk2KbmebAJ9IKd3Wybrby+mkmvV2JXM00uz3xy7L7HXvm9UfyssLlieX/72gqt65FO97342IDkOARjGWfXeDf4/sw/7cLeciin5T34uIPSiC26YUfal/STHsVSNXUNw1/SZeG691OSmlVL6Z3UIxTnT1OPd7UXTt6PBjJ61WcxPo2hShrHJl9aTqrxtTSudHxESK8a//HBE3UdwsOZIi/O1GcWIcV9Z/rnyBvBr4XUTcRnH1P1F82t6Rok9b9c13t1Hsp59GxFUUVwPmppSW+7GwrkoptUfEiRTjR99fdrf5F8WVkx0phjn79waLX0TxAeAUin6S/1Nn/a+UfdJvoggB91CMXbuwfIzvoHgj2IDlxz/uk7pzPFCcEz8GHij31SsUo09sSTHE4v7V604pzY6ID1JcWZsaETdQDBc2DNiG4vka2+zH1AN2oPhKfVYUN8r/tSwfC+xH0UXlGorHCcUQfbtSjGn9eERcQ3EMvpHiK+zzKYLkyrqA4pz5SkSMp7g5fTOK/XY1RX/YnvQ9ihB/V3l+zaO4crYLxXNwaONF+66U0hkRMYjiHqP/K8/t+yhu4H4DxevdptS/CbCr/pNiPx0I/D4irqe4GfH9FDfrfSuldFed5e4EPlZ+m3w3r41zP4AidHarC0FOUjHmeL1xx+vVTRFxNMX79uXlufkoxXjnB1G8/3y46kbZT1MMDPAXGg+iMKX8cLgqmaORxyj6sR8REa9Q9PdPwMUppSdXYn3dkcvr3rMU9zo8HBG/prhwdyjFOfTDlNKyG6dTSo9GMc79+cAfI+JGig97r6MI57uWj2mLbrT7XooscGIUN1xX+ub/oMF9Tl3TzKF3WvFHOfzSCuq0U2c4RYrQ8WuKEU8WUNzd/DFeG15sSifrrAyJVHfIw6p6EymC/Pzy71aKENlG46EwpzVY15R6j6OZz2PN3xKKA/8a4F2dLPte4Dfl8/hyeXBOpxhqrN6QbmMoPgE/TtEF5gWKF9CLgYPq1P8cRTeZxWW72qvmTWu0/+lkiCng3RQ3AD1frvcJ4FvUGdazapk3U3yFloBrV/B8rk8xqszDFCfui+XjvZLix1Cqx7CueyxUPVedHos9eV5193gon/MHy/NpNsWL6ttW8Bi3onjje6Y8fv5BMcrUsX3h3OjCc7URxTC5V1O82b5QPo5nKUZE+hB1hsSjuKp6B0X4XUTx5vg/wISuHMOdPSflc3o9xWvOi+V5snuj9VG8RrY32EZn51jd553iNeF35fbnlufabiu5/Q7HzorOi87a3IT9PY7iB+0ertnXN1AMVzl4VdpIcXHjpHL9L5XP4V3AB+rUXbaNsl3XULymLaQI+e9pcI6u8j7oi39UDYXZhbodhsKsmrc5xfvRsxQXKZ6l+NZ38wbPS2d/1cdttzJHo/Orps47KC6CzaPod94r+4mMXvcovvn6L4r3oMUUeePTFF2K6m3/beW+ebKsP4fifP0JNUMG04XXIooLvfeWba4cNw33eVf+olyxuimKnw3ejeJkf7zFzZEk9TNR/ET9X4ELU0qTW9saSX1Ff+5zv9LK/uK7AzcZ7CVJktRX9Oc+990WEZ+k6Gf/EYqvv05tbYskSZKk1xjuu+ffKYbU+gtwVEqpT/5qnCRJkvon+9xLkiRJmbDPvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpQJw70kSZKUCcO9JEmSlAnDvSRJkpSJ/w+hy1xyp7+DfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 379
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to find a better way to visualize this\n",
    "composers = list(set([p.split(\"/\")[0] for p in paths ]))\n",
    "print(composers)\n",
    "\n",
    "train_composer = [composers.index(p.split(\"/\")[0]) for p in path_train]\n",
    "val_composer = [composers.index(p.split(\"/\")[0]) for p in path_validation]\n",
    "\n",
    "_ = plt.hist([train_composer, val_composer], label=['train', 'validation'])\n",
    "_ = plt.legend(loc='upper left')\n",
    "_ = plt.xticks(list(range(len(composers))), composers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-qaT10ApkCY"
   },
   "source": [
    "## Transform the input into a convenient format for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEYec2TDOWvj",
    "outputId": "1b69573b-cfeb-4dc9-8882-da42f9c25537"
   },
   "outputs": [],
   "source": [
    "# Helper functions to feed the correct input into the NN \n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "tag_to_ix = {p: accepted_pitches.index(p) for p in accepted_pitches}\n",
    "#add PADDING TAD\n",
    "tag_to_ix[PAD] = len(accepted_pitches)\n",
    "\n",
    "midi_to_ix = {m: m for m in range(12)}\n",
    "# #add PADDING TAD\n",
    "# midi_to_ix[PAD] = 12\n",
    "\n",
    "# print(midi_to_ix[1])\n",
    "# print(len(midi_to_ix))\n",
    "\n",
    "duration_delimiter = [0.125,0.25,0.5,1,2,4]\n",
    "\n",
    "\n",
    "class Pitch2Diatonic():\n",
    "    def __call__(self, in_seq,weights = None):\n",
    "        return [p for p in in_seq]\n",
    "\n",
    "class Diatonic2Int():\n",
    "    def __call__(self, in_seq,weights = None):\n",
    "        idxs = [tag_to_ix[w] for w in in_seq]\n",
    "        return idxs\n",
    "\n",
    "class Int2Pitch():\n",
    "    def __call__(self, in_seq):\n",
    "        return [accepted_pitches[i] for i in in_seq]\n",
    "\n",
    "class OneHotEncoder():\n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample,weights = None):\n",
    "        onehot = np.zeros([len(sample), self.alphabet_len])\n",
    "        tot_chars = len(sample)\n",
    "        onehot[np.arange(tot_chars), sample] = 1\n",
    "        return onehot\n",
    "\n",
    "# class WeightedOneHotEncoder():\n",
    "#     def __init__(self, alphabet_len):\n",
    "#         self.alphabet_len = alphabet_len\n",
    "        \n",
    "#     def __call__(self, sample, weights=None):\n",
    "#         if weights == None:\n",
    "#             weights = np.ones(len(sample))\n",
    "#         onehot = torch.nn.functional.one_hot(sample,self.alphabet_len)\n",
    "#         return (onehot.t()*torch.Tensor(weights)).t() #transpositions to allow the broadcasting\n",
    "    \n",
    "class DurationOneHotEncoder():\n",
    "    def __init__(self, pitch_alphabet_len, duration_delimiter):\n",
    "        self.pitch_alphabet_len = pitch_alphabet_len\n",
    "        self.dur_alphabet_len = len(duration_delimiter)+2\n",
    "        self.duration_delimiter = duration_delimiter\n",
    "        \n",
    "    def __call__(self, sample, durs):\n",
    "        sample = torch.tensor(sample,dtype=torch.long)\n",
    "        onehot_pitch = torch.nn.functional.one_hot(sample,self.pitch_alphabet_len)\n",
    "        quantized_durations = np.digitize(durs,self.duration_delimiter)\n",
    "        quantized_durations = torch.tensor(quantized_durations,dtype=torch.long)\n",
    "        onehot_duration = torch.nn.functional.one_hot(quantized_durations,self.dur_alphabet_len)\n",
    "        return torch.cat([onehot_pitch,onehot_duration],1)\n",
    "        \n",
    "class ToTensorFloat():\n",
    "    def __call__(self, sample, weight = None):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.float()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.float)\n",
    "\n",
    "class ToTensorLong():\n",
    "    def __call__(self, sample, weights = None):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.long()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.long)\n",
    "    \n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, sample, weights):\n",
    "        for t in self.transforms:\n",
    "            sample = t(sample, weights)\n",
    "        return sample\n",
    "\n",
    "pitches_len = len(accepted_pitches)\n",
    "midinote_len = 12\n",
    "\n",
    "### Define the preprocessing pipeline\n",
    "transform_diat = Compose([Pitch2Diatonic(),Diatonic2Int(),ToTensorLong()])\n",
    "transform_chrom = Compose([DurationOneHotEncoder(len(midi_to_ix),duration_delimiter),ToTensorFloat()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =[list(range(10)) for e in range(4)]\n",
    "torch.Tensor(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jV9boYaToUs2",
    "outputId": "92bcd8d3-87a2-4d2c-8e92-4a106139f59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 29\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n",
      "tensor([ 9,  0,  4,  9, 10, 11,  0,  1,  4,  9,  4,  9,  0,  2,  3,  4,  5,  9,\n",
      "         2,  9,  6,  7,  8,  9,  2,  5,  5,  9,  2, 10])\n",
      "['A', 'C', 'E', 'A', 'A#', 'B', 'C', 'C#', 'E', 'A', 'E', 'A', 'C', 'D', 'D#', 'E', 'F', 'A', 'D', 'A', 'F#', 'G', 'G#', 'A', 'D', 'F', 'F', 'A', 'D', 'A#']\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "# TODO: order sequences for better memory efficiency\n",
    "\n",
    "class PSDataset(Dataset):\n",
    "    def __init__(self, dict_dataset, paths, transf_c, transf_d, augment_dataset, sort =False, truncate = None):\n",
    "        if sort:\n",
    "            dict_dataset = sorted(dict_dataset, key = lambda e: (len(e['midi_number'])))\n",
    "        if augment_dataset:\n",
    "            self.chromatic_sequences = [e[\"midi_number\"] for e in dict_dataset if e[\"original_path\"] in paths]\n",
    "            self.diatonic_sequences = [e[\"pitches\"]\n",
    "                                       for e in dict_dataset \n",
    "                                       if e[\"original_path\"] in paths]\n",
    "            self.durations = [e[\"duration\"] for e in dict_dataset if e[\"original_path\"] in paths]\n",
    "        else: #consider only non transposed pieces\n",
    "            self.chromatic_sequences = [e[\"midi_number\"] for e in dict_dataset \n",
    "                                        if (e[\"original_path\"] in paths and e[\"transposed_of\"]==\"P1\")]\n",
    "            self.diatonic_sequences = [e[\"pitches\"]\n",
    "                                       for e in dict_dataset \n",
    "                                       if (e[\"original_path\"] in paths and e[\"transposed_of\"]==\"P1\")]\n",
    "            self.durations = [e[\"duration\"] \n",
    "                              for e in dict_dataset \n",
    "                              if (e[\"original_path\"] in paths and e[\"transposed_of\"]==\"P1\")]\n",
    "        #the transformations to apply to data\n",
    "        self.transf_c = transf_c\n",
    "        self.transf_d = transf_d\n",
    "        self.truncate = truncate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chromatic_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chromatic_seq = self.chromatic_sequences[idx]\n",
    "        diatonic_seq = self.diatonic_sequences[idx]\n",
    "        duration_seq = self.durations[idx]\n",
    "        weights = [dur/4  if dur<=4 else 1 for dur in duration_seq  ] # limit the weights to (0,4)       \n",
    "\n",
    "        #transform\n",
    "        chromatic_seq = self.transf_c(chromatic_seq,weights)\n",
    "        diatonic_seq = self.transf_d(diatonic_seq,None)\n",
    "\n",
    "        if not self.truncate is None:\n",
    "            if len(diatonic_seq) > self.truncate:\n",
    "                chromatic_seq = chromatic_seq[0:self.truncate]\n",
    "                diatonic_seq = diatonic_seq[0:self.truncate]\n",
    "\n",
    "        #sanity check\n",
    "        assert len(chromatic_seq) == len(diatonic_seq)\n",
    "        seq_len = len(diatonic_seq)\n",
    "        \n",
    "        return chromatic_seq, diatonic_seq, seq_len\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,True)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat, False)\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset))\n",
    "\n",
    "\n",
    "\n",
    "# test if it works\n",
    "for chrom,diat,seq_len in train_dataset:\n",
    "    print(chrom[0:30])\n",
    "    print(torch.argmax(chrom[0:30],1))\n",
    "    # print([diatonic_pitches[p.item()] for p in diat[0:30]])\n",
    "    print([accepted_pitches[p.item()] for p in diat[0:30]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAubyjw2LC8P",
    "outputId": "d6f8574f-8901-41e7-fd7a-9a0c6091eafa"
   },
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy, l) = zip(*batch)\n",
    "    \n",
    "    xx_pad = pad_sequence(xx)\n",
    "    yy_pad = pad_sequence(yy, padding_value=tag_to_ix[PAD])\n",
    "\n",
    "    #sort the sequences by length\n",
    "    seq_lengths, perm_idx = torch.Tensor(l).sort(0, descending=True)\n",
    "    xx_pad = xx_pad[:,perm_idx,:]\n",
    "    yy_pad = yy_pad[:,perm_idx]\n",
    "\n",
    "    return xx_pad, yy_pad, seq_lengths\n",
    "\n",
    "# data_loader = DataLoader(dataset=validation_dataset, batch_size=4, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "\n",
    "# #test if it work\n",
    "# for batch in data_loader:\n",
    "#     print(batch[0].shape,batch[1].shape,batch[2])\n",
    "#     print(batch[1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O1OA1AjGWO-"
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QEkFBY5cUsmN"
   },
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum',ignore_index = tag_to_ix[PAD])\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out = self.top_layer(rnn_out)\n",
    "       \n",
    "        # # Find the positions where the token is a dummy padding token.\n",
    "        # pad_mask = (sentences == self.pad_word_id).float()\n",
    "\n",
    "        # # For these positions, we add some large number in the column corresponding\n",
    "        # # to the dummy padding label.\n",
    "        # out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "            \n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        return [predicted[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QE7itQ88Qx17"
   },
   "outputs": [],
   "source": [
    "class RNNCRFTagger(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNCRFTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        self.crf = CRF(self.n_labels)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        ## should I initialize here??\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out = self.top_layer(rnn_out)\n",
    "      \n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # Compute the outputs of the lower layers, which will be used as emission\n",
    "        # scores for the CRF.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
    "        # the *negative* log likelihood as the loss value.            \n",
    "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
    "        # log likelihood.\n",
    "\n",
    "        pad_mask = torch.arange(max(sentences_len))[:, None] < sentences_len[None, :]\n",
    "        pad_mask = pad_mask.byte().to(device)\n",
    "        return -self.crf(scores, labels, mask = pad_mask )\n",
    "            \n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the emission scores, as above.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
    "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
    "        # of shape (n_sentences, max_len).\n",
    "        pad_mask = torch.arange(max(sentences_len))[:, None] < sentences_len[None, :]\n",
    "        pad_mask = pad_mask.byte().to(device)\n",
    "        return self.crf.decode(scores,mask = pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttentionTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum',ignore_index = tag_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        # TODO : set right parameters\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        # use attention\n",
    "        attn_applied = self.attention(rnn_out,rnn_out,sentences_len)\n",
    "        \n",
    "        out = self.top_layer(attn_applied) #maybe remove this one?\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "            \n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        return [predicted[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention,self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "            hidden_dim, hidden_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "    def forward(self,\n",
    "                query, # [seq_len, batch, hidden_dim]\n",
    "                values, # [seq_len, batch, hidden_dim]\n",
    "                sentences_len\n",
    "               ):\n",
    "        weights = self._get_weights(query, values) # [batch,seq_length,hidden_dim]\n",
    "        \n",
    "        weights = torch.nn.functional.softmax(weights, dim=2)\n",
    "        # mask the weights\n",
    "        inverted_pad_mask = torch.arange(max(sentences_len))[None,:] > sentences_len[:,None]\n",
    "        inverted_pad_mask = (inverted_pad_mask.float()*(-10000))[:,:,None].to(device)\n",
    "#         print(weights.shape,inverted_pad_mask.shape )\n",
    "        #apply the mast\n",
    "        weights = weights - inverted_pad_mask\n",
    "        out = torch.transpose((weights @ torch.transpose(values,0,1)),0,1)\n",
    "#         print(\"ATT out shape\", out.shape)\n",
    "        return out # [seq_len,batch,encoder_dim]\n",
    "\n",
    "    def _get_weights(self,\n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        #transpose to batch first to correctly handle batch multiplications\n",
    "#         print(\"shape\",query.shape,self.W.shape, values.shape)\n",
    "        query,values = torch.transpose(query,0,1),torch.transpose(values,0,1)\n",
    "#         print(\"shape\",query.shape,self.W.shape, values.shape)\n",
    "#         print(\"stape values.t\", torch.transpose(values,1,2).shape)\n",
    "        weights = query @ self.W @ torch.transpose(values,1,2)  # [seq_length]\n",
    "#         print(\"out att shape\", weights.shape)\n",
    "        return weights/np.sqrt(self.hidden_dim)  # [seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMultAttentionTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNMultAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum',ignore_index = tag_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        # TODO : set right parameters\n",
    "        self.attention = torch.nn.MultiheadAttention(hidden_dim,num_heads= 1)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        # compute padding mask (True when padded, ignore True)\n",
    "        inverted_pad_mask = torch.arange(max(sentences_len))[None,:] > sentences_len[:,None]\n",
    "        \n",
    "        # use attention\n",
    "        attn_applied, _ = self.attention(rnn_out,rnn_out,rnn_out,key_padding_mask = inverted_pad_mask.to(device))\n",
    "        \n",
    "        out = self.top_layer(attn_applied) #maybe remove this one?\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "            \n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        return [predicted[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXixmVQfvw8T"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3m_nj4HCuCe1"
   },
   "outputs": [],
   "source": [
    "# TODO: search over the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uAMSIlw0AJb6"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_dataloader, val_dataloader, n_epochs):\n",
    "    history = defaultdict(list)  \n",
    "    for i_epoch in range(1,n_epochs +1):\n",
    "        t0 = time.time()\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        model.train()\n",
    "        for seqs, targets, lens in train_dataloader: #seqs, targets, lens are batches\n",
    "            seqs, targets = seqs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "#             print(\"input seq shape:\",seqs.shape)\n",
    "\n",
    "            loss = model(seqs,targets,lens) / sum(lens) #normalize for the number of symbol considered (without padding)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            predicted = model.predict(seqs,lens)\n",
    "            for i,p in enumerate(predicted):\n",
    "                acc= accuracy_score(p,targets[:,i][:len(p)].cpu()) #compute the accuracy without considering the padding\n",
    "                accuracy_sum += acc/len(lens) #normalize according to the number of sequences in the batch\n",
    "\n",
    "        train_loss = loss_sum/len(train_dataloader)\n",
    "        train_accuracy = accuracy_sum/len(train_dataloader) #normalize according to the number of batches\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        all_predicted = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for seqs,targets, lens in val_dataloader:\n",
    "                # Predict the model's output on a batch\n",
    "                predicted = model.predict(seqs.to(device),lens)                   \n",
    "                # Update the lists that will be used to compute the accuracy\n",
    "                for i,p in enumerate(predicted):\n",
    "                    all_predicted.append(torch.Tensor(p))\n",
    "                    all_targets.append(targets[0:int(lens[i]),i])\n",
    "                \n",
    "        # Compute the overall accuracy for the validation set\n",
    "        val_accuracy = accuracy_score(torch.cat(all_predicted),torch.cat(all_targets))\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "#         save the model\n",
    "        torch.save(model, \"./models/temp/model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "#         files.download(\"model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "\n",
    "    \n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {i_epoch}: train loss = {train_loss:.4f}, train_accuracy: {train_accuracy:.4f},val_accuracy: {val_accuracy:.4f}, time = {t1-t0:.4f}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "2g2st8znpGW_",
    "outputId": "fe232e42-e270-4f5d-9bd8-718bba5e7151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n",
      "Epoch 1: train loss = 2.1270, train_accuracy: 0.3498,val_accuracy: 0.8941, time = 146.1986\n",
      "Epoch 2: train loss = 0.4894, train_accuracy: 0.8669,val_accuracy: 0.9160, time = 146.6378\n",
      "Epoch 3: train loss = 0.3586, train_accuracy: 0.8902,val_accuracy: 0.9312, time = 146.2931\n",
      "Epoch 4: train loss = 0.3192, train_accuracy: 0.9039,val_accuracy: 0.9380, time = 145.7303\n",
      "Epoch 5: train loss = 0.3018, train_accuracy: 0.9111,val_accuracy: 0.9247, time = 146.5489\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.05\n",
    "WEIGHT_DECAY = 0 #1e-4\n",
    "BATCH_SIZE = 2\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,True,sort=True, truncate = None)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat, False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False,collate_fn=pad_collate)\n",
    "\n",
    "# model = torch.load(\"./models/temp/model_temp_epoch30-to_restart.pkl\")\n",
    "# model = RNNCRFTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "model = RNNMultAttentionTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "model = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_WEIGHT)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_WEIGHT, momentum = MOMENTUM,weight_decay=WEIGHT_DECAY)\n",
    "# otimizer = torch.optim.Adam(model.parameters(),lr = 0.05, weight_decay=1e-4)\n",
    "\n",
    "history = training_loop(model,optimizer,train_dataloader,val_dataloader, n_epochs)\n",
    "\n",
    "# After the final evaluation, we print more detailed evaluation statistics,\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8d5b94b58cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#find the best working model on the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best validation accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"at epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#find the best working model on the accuracy\n",
    "max_accuracy = np.max(history['val_accuracy'])\n",
    "best_epoch = np.argmax(history['val_accuracy'])\n",
    "print(\"Best validation accuracy: \",max_accuracy, \"at epoch\",best_epoch)\n",
    "\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tTv2BrGbvSHh",
    "outputId": "801ff32d-24dc-4434-a436-17d9ca832214"
   },
   "outputs": [],
   "source": [
    "# torch.save(model, \"./models/model_asap_crf200dur.pkl\")\n",
    "# files.download(\"model_asap_crf300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXcmLpCKt28l"
   },
   "source": [
    "## Test on Mdata dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./models/best_multiAttn_acc9730.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BacUqgD5usGL"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open(Path(basepath,'./datasets/aug_musedata.pkl'), 'rb') as fid:\n",
    "     full_mdata_dict_dataset = pickle.load( fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgaUwW2fuwg0",
    "outputId": "89fc5f7d-884a-4ee6-fc34-b3d512f0c4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 different pieces\n",
      "Average number of notes:  858.6319771007974\n"
     ]
    }
   ],
   "source": [
    "mdata_paths = list(set([e[\"original_path\"] for e in full_mdata_dict_dataset ]))\n",
    "\n",
    "# # remove the symbphony No.100 from Haydn because of the enharmonic transposition\n",
    "# paths.remove(\"datasets\\\\opnd\\\\haydndoversyms-10004m.opnd-m\")\n",
    "\n",
    "# print(paths)\n",
    "print(len(mdata_paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_mdata_dict_dataset ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "f-Fg6gJKvNLt"
   },
   "outputs": [],
   "source": [
    "mdata_dataset = PSDataset(full_mdata_dict_dataset,mdata_paths, transform_chrom,transform_diat,False)\n",
    "mdata_dataloader  = DataLoader(mdata_dataset,  batch_size=16, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ml905Mtdvj-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/native/cuda/Reduce.cuh\":928, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c182dde4b990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Predict the model's output on a batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Update the evaluation statistics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a333ed8ac9b4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Compute the outputs from the linear units.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Select the top-scoring labels. The shape is now (max_len, n_sentences).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a333ed8ac9b4>\u001b[0m in \u001b[0;36mcompute_outputs\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# use attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mattn_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverted_pad_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#maybe remove this one?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4322\u001b[0m         \u001b[0;31m# average attention weights over heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4323\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4325\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/native/cuda/Reduce.cuh\":928, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "all_targets = []\n",
    "model.eval() # Evaluation mode (e.g. disable dropout)\n",
    "with torch.no_grad(): # Disable gradient tracking\n",
    "    for seqs, targets,lens in mdata_dataloader:\n",
    "        # Move data to device\n",
    "        seqs = seqs.to(device)\n",
    "\n",
    "        # Predict the model's output on a batch.\n",
    "        predicted = model.predict(seqs,lens)                   \n",
    "        # Update the evaluation statistics.\n",
    "        for i,p in enumerate(predicted):\n",
    "            all_inputs.append(torch.argmax(seqs[0:int(lens[i]),i,:].cpu(),1).numpy())\n",
    "            all_outputs.append(torch.Tensor(p))\n",
    "            all_targets.append(targets[0:int(lens[i]),i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsefdSHxvrWy",
    "outputId": "33785b90-5e03-4a6d-86dd-92b6b763a505"
   },
   "outputs": [],
   "source": [
    "# Divide accuracy according to author\n",
    "authors = []\n",
    "\n",
    "for sequence in all_inputs:\n",
    "    author = [e[\"original_path\"].split(\"\\\\\")[-1][:3] for e in full_mdata_dict_dataset\n",
    "              if len(e[\"midi_number\"]) == len(sequence) and\n",
    "              list(e[\"midi_number\"]) ==list(sequence) ]\n",
    "    # assert len(author) == 1\n",
    "    authors.append(author[0])\n",
    "\n",
    "considered_authors = list(set(authors))\n",
    "print(considered_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgNt_yG7vrfd",
    "outputId": "8473393f-dac6-4930-9d9e-da9c1046f1ab"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'considered_authors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-748dbb7b388d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maccuracy_per_author\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnotes_per_author\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mca\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsidered_authors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# ca_inputs = torch.cat([all_inputs[i] for i,a in enumerate(authors) if a == ca])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mca_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'considered_authors' is not defined"
     ]
    }
   ],
   "source": [
    "errors_per_author = {}\n",
    "accuracy_per_author = {}\n",
    "notes_per_author = {}\n",
    "for ca in considered_authors:\n",
    "    # ca_inputs = torch.cat([all_inputs[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_outputs = torch.cat([all_outputs[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_targets = torch.cat([all_targets[i] for i,a in enumerate(authors) if a == ca])\n",
    "    # print(ca_inputs.shape,ca_outputs.shape,ca_targets.shape,ca_predicted_pitches.shape)\n",
    "    ca_acc = accuracy_score(ca_outputs,ca_targets)\n",
    "    accuracy_per_author[ca] = float(ca_acc)\n",
    "    errors_per_author[ca] = int(len(ca_targets) - sum(torch.eq(ca_outputs,ca_targets)))\n",
    "    notes_per_author[ca] = len(ca_targets)\n",
    "\n",
    "print(errors_per_author)\n",
    "print(accuracy_per_author)\n",
    "print(notes_per_author)\n",
    "print(\"Total errors :\", sum([e for e in errors_per_author.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5s0d56evrje"
   },
   "source": [
    "### Best accuracy for now\n",
    "for now best accuracy is with  no CRF (but considering durations) n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.09\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "duration_delimiter = [0.125,0.25,0.5,1,2,4]\n",
    "\n",
    "Model available in: \"model_temp_epoch12-noCRFacc9575.pkl\"\n",
    "accuracy on validation set 0.9575\n",
    "Trained on all dataset\n",
    "\n",
    "\n",
    "{'moz': 98, 'tel': 116, 'bac': 93, 'hay': 200, 'cor': 25, 'bee': 127, 'viv': 45, 'han': 52}\n",
    "{'moz': 0.9959990201682044, 'tel': 0.995265306122449, 'bac': 0.9962048561518058, 'hay': 0.9918334013883218, 'cor': 0.9989793002082228, 'bee': 0.9948148450577716, 'viv': 0.9981630403722905, 'han': 0.9978775510204082}\n",
    "{'moz': 24494, 'tel': 24500, 'bac': 24505, 'hay': 24490, 'cor': 24493, 'bee': 24493, 'viv': 24497, 'han': 24500}\n",
    "Total errors : 756\n",
    "\n",
    "This win by far against ps13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVvP1eH8vrl3"
   },
   "source": [
    "### Best accuracy with CRF\n",
    "n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "model = RNNCRFTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "\n",
    "Model available in: \"\"./models/model_temp_CRFacc9548.pkl\"\"\n",
    "accuracy on validation set 0.9548586557910835\n",
    "Trained on all asap dataset\n",
    "\n",
    "{'viv': 36, 'tel': 41, 'bee': 185, 'bac': 101, 'han': 44, 'cor': 14, 'moz': 131, 'hay': 376}\n",
    "{'viv': 0.9985304322978323, 'tel': 0.9983265306122449, 'bee': 0.9924468215408484, 'bac': 0.9958783921648643, 'han': 0.9982040816326531, 'cor': 0.9994284081166047, 'moz': 0.9946517514493345, 'hay': 0.9846467946100449}\n",
    "{'viv': 24497, 'tel': 24500, 'bee': 24493, 'bac': 24505, 'han': 24500, 'cor': 24493, 'moz': 24494, 'hay': 24490}\n",
    "Total errors : 928\n",
    "\n",
    "Still win against ps13"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTDh55Hh2OWCo66dGc+SES",
   "include_colab_link": true,
   "name": "rnncrf_pitch_spelling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
