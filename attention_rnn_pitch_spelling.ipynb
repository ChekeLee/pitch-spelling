{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fosfrancesco/pitch-spelling/blob/main/rnncrf_pitch_spelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fifMg_hxNSOg",
    "outputId": "a8b320f9-4231-40d9-8f10-a7b8acd1da37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hsciybUBNkur"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "import music21 as m21\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn\n",
    "import music21 as m21\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchcrf import CRF\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, notebook\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Naq6UG5jRbyK"
   },
   "source": [
    "# RNN-CRF for Pitch Spelling\n",
    "\n",
    "Dataset: different authors from ASAP collection\n",
    "Challenges:\n",
    "- extremely long sequences\n",
    "- small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mHJvrZ2Wo_e",
    "outputId": "1319f78b-cfb1-4e2a-937c-c47cf952757c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'C'), (1, 'B#'), (2, 'D--'), (3, 'C#'), (4, 'B##'), (5, 'D-'), (6, 'D'), (7, 'C##'), (8, 'E--'), (9, 'D#'), (10, 'E-'), (11, 'F--'), (12, 'E'), (13, 'D##'), (14, 'F-'), (15, 'F'), (16, 'E#'), (17, 'G--'), (18, 'F#'), (19, 'E##'), (20, 'G-'), (21, 'G'), (22, 'F##'), (23, 'A--'), (24, 'G#'), (25, 'A-'), (26, 'A'), (27, 'G##'), (28, 'B--'), (29, 'A#'), (30, 'B-'), (31, 'C--'), (32, 'B'), (33, 'A##'), (34, 'C-')]\n",
      "['D--', 'B##', 'C##', 'E--', 'F--', 'D##', 'G--', 'E##', 'F##', 'A--', 'G##', 'B--', 'C--', 'A##']\n",
      "[(0, 'P1'), (1, 'd2'), (2, 'A7'), (3, 'm2'), (4, 'A1'), (5, 'M2'), (6, 'd3'), (7, 'AA1'), (8, 'm3'), (9, 'A2'), (10, 'M3'), (11, 'd4'), (12, 'AA2'), (13, 'P4'), (14, 'A3'), (15, 'd5'), (16, 'A4'), (17, 'P5'), (18, 'd6'), (19, 'AA4'), (20, 'm6'), (21, 'A5'), (22, 'M6'), (23, 'd7'), (24, 'AA5'), (25, 'm7'), (26, 'A6'), (27, 'M7'), (28, 'd1'), (29, 'AA6')]\n"
     ]
    }
   ],
   "source": [
    "pitches_dict = {\n",
    "    0 : [\"C\",\"B#\",\"D--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    1 : [\"C#\",\"B##\",\"D-\"], # nn.Linear(input_size+context_size, 2)\n",
    "    2 : [\"D\",\"C##\",\"E--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    3 : [\"D#\",\"E-\",\"F--\"],\n",
    "    4 : [\"E\",\"D##\",\"F-\"],\n",
    "    5 : [\"F\",\"E#\",\"G--\"],\n",
    "    6 : [\"F#\",\"E##\",\"G-\"],\n",
    "    7 : [\"G\",\"F##\",\"A--\"],\n",
    "    8 : [\"G#\",\"A-\"],\n",
    "    9 : [\"A\",\"G##\",\"B--\"],\n",
    "    10 : [\"A#\",\"B-\",\"C--\"],\n",
    "    11 : [\"B\",\"A##\",\"C-\"]\n",
    "}\n",
    "\n",
    "accepted_pitches = [ii for i in pitches_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_pitches)])\n",
    "\n",
    "double_acc_pitches = [ii for i in pitches_dict.values() for ii in i if ii.endswith(\"##\") or  ii.endswith(\"--\") ]\n",
    "print(double_acc_pitches)\n",
    "\n",
    "def score2midi_numbers(score):\n",
    "    return [p.midi%12 for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "def score2pitches(score):\n",
    "    return [p.name for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "interval_dict = {\n",
    "    0 : [\"P1\",\"d2\",\"A7\"], \n",
    "    1 : [\"m2\",\"A1\"], \n",
    "    2 : [\"M2\",\"d3\",\"AA1\"], \n",
    "    3 : [\"m3\",\"A2\"],\n",
    "    4 : [\"M3\",\"d4\",\"AA2\"],\n",
    "    5 : [\"P4\",\"A3\"],\n",
    "    6 : [\"d5\",\"A4\"],\n",
    "    7 : [\"P5\",\"d6\",\"AA4\"],\n",
    "    8 : [\"m6\",\"A5\"],\n",
    "    9 : [\"M6\",\"d7\",\"AA5\"],\n",
    "    10 : [\"m7\",\"A6\"],\n",
    "    11 : [\"M7\",\"d1\",\"AA6\"]\n",
    "}\n",
    "\n",
    "accepted_intervals = [ii for i in interval_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_intervals)])\n",
    "\n",
    "def transp_score(score):\n",
    "    \"\"\" For each input return len(accepted_intervals) transposed scores\"\"\"\n",
    "    return [score.transpose(interval) for interval in accepted_intervals]\n",
    "\n",
    "def smart_transp_score(score):\n",
    "    \"\"\" For each chromatic interval chose the interval that lead to the smallest number of accidentals\"\"\"\n",
    "    scores = []\n",
    "    for chromatic_int in interval_dict.keys():\n",
    "        temp_scores = []\n",
    "        temp_acc_number = []\n",
    "        for diat_interval in interval_dict[chromatic_int]:\n",
    "            new_score = score.transpose(diat_interval)\n",
    "            temp_scores.append(new_score)\n",
    "            temp_acc_number.append(sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in score2pitches(new_score)]))\n",
    "            # print(\"choice:\", [note.name for note in temp_scores[-1].flat.notes][0:10],\"acc:\",temp_acc_number[-1] )\n",
    "        #keep only the one with the lowest number of accidentals\n",
    "        min_index = np.argmin(temp_acc_number)\n",
    "        # print(\"preferred the number\", min_index)\n",
    "        scores.append(temp_scores[min_index])\n",
    "    return scores\n",
    "\n",
    "def acc_simple_enough(score,accepted_ratio = 0.2 ):\n",
    "    pitches = score2pitches(score)\n",
    "    double_acc = sum(el in double_acc_pitches for el in pitches)\n",
    "    if double_acc/len(pitches) < accepted_ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "diatonic_pitches = [\"C\",\"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n",
    "\n",
    "# #test acc_simple_enough()\n",
    "# score = m21.converter.parse(paths[356])\n",
    "# scores = smart_transp_score(score)\n",
    "# #delete the pieces with non accepted pitches (e.g. triple sharps)\n",
    "# scores = [s for s in scores if all(pitch in accepted_pitches for pitch in score2pitches(s))]\n",
    "# for s in scores:\n",
    "#     print(s.parts[0].flat.getElementsByClass(m21.key.KeySignature)[0], \"simple enough:\", acc_simple_enough(s))\n",
    "#     print([n.name for n in s.flat.notes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw-SuL4ZB_2C"
   },
   "source": [
    "## Import ASAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrZ0BwLEj2Gp",
    "outputId": "6b521f78-28db-4d4f-f4ef-9345c5d91db9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/fosfrancesco/pitch-spelling.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WaINjDS2kpxE"
   },
   "outputs": [],
   "source": [
    "basepath = \"./\" #to change if running locally or on colab\n",
    "\n",
    "# load the asap datasets\n",
    "with open(Path(basepath,'datasets','baroque_aug_asap.pkl'), 'rb') as fid:\n",
    "     dataset_baroque = pickle.load( fid)\n",
    "\n",
    "with open(Path(basepath,'datasets','classical_aug_asap.pkl'), 'rb') as fid:\n",
    "     dataset_classical = pickle.load( fid)\n",
    "\n",
    "with open(Path(basepath,'datasets','romantic_aug_asap.pkl'), 'rb') as fid:\n",
    "     dataset_romantic = pickle.load( fid)\n",
    "        \n",
    "# with open(Path(basepath,'datasets','remaining_aug_asap.pkl'), 'rb') as fid:\n",
    "#      dataset_remaining = pickle.load( fid)\n",
    "\n",
    "# merge the three files together\n",
    "# full_dict_dataset = dataset_baroque + dataset_classical + dataset_romantic + dataset_remaining\n",
    "full_dict_dataset = dataset_baroque + dataset_classical + dataset_romantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYQdkJAiTS6_",
    "outputId": "72835948-2884-40bb-e5b4-6ebbc8488895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 different pieces\n",
      "Average number of notes:  2219.6285387474977\n"
     ]
    }
   ],
   "source": [
    "paths = list(set([e[\"original_path\"] for e in full_dict_dataset ]))\n",
    "\n",
    "# print(paths)\n",
    "print(len(paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_dict_dataset ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMdKkzNnCTrK"
   },
   "source": [
    "## Chose the convenient data augmentation\n",
    "Two possibilities:\n",
    "- for each chromatic interval, take only the diatonic transposition that produce the smallest number of accidentals (or the original if present)\n",
    "- take only a certain interval of time signatures\n",
    "\n",
    "The second is probably better for a smaller and simple dataset, but gives the problem of chosing between for example F# and G# that are both present in this bigger dataset. So we go for the first criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oign7EZ9BSX4",
    "outputId": "9aa3f694-289a-43fd-fd14-9cad3b8dd3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  1\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  4\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  6\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_2/1st_no_repeat/xml_score.musicxml . Chromatic:  1\n",
      "No options for Chopin/Ballades/4/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  1\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  3\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  8\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  10\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  11\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml P1 4\n",
      "['C#', 'B#', 'C#', 'D#', 'C#', 'D#', 'G#', 'A#', 'B#', 'C#']\n",
      "[1, 0, 1, 3, 1, 3, 8, 10, 0, 1]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml m2 -1\n",
      "['D', 'C#', 'D', 'E', 'D', 'E', 'A', 'B', 'C#', 'D']\n",
      "[2, 1, 2, 4, 2, 4, 9, 11, 1, 2]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml d3 -6\n",
      "['E-', 'D', 'E-', 'F', 'E-', 'F', 'B-', 'C', 'D', 'E-']\n",
      "[3, 2, 3, 5, 3, 5, 10, 0, 2, 3]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml m3 1\n",
      "['E', 'D#', 'E', 'F#', 'E', 'F#', 'B', 'C#', 'D#', 'E']\n",
      "[4, 3, 4, 6, 4, 6, 11, 1, 3, 4]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml d4 -4\n",
      "['F', 'E', 'F', 'G', 'F', 'G', 'C', 'D', 'E', 'F']\n",
      "[5, 4, 5, 7, 5, 7, 0, 2, 4, 5]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml P4 3\n",
      "['F#', 'E#', 'F#', 'G#', 'F#', 'G#', 'C#', 'D#', 'E#', 'F#']\n",
      "[6, 5, 6, 8, 6, 8, 1, 3, 5, 6]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml d5 -2\n",
      "['G', 'F#', 'G', 'A', 'G', 'A', 'D', 'E', 'F#', 'G']\n",
      "[7, 6, 7, 9, 7, 9, 2, 4, 6, 7]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml P5 5\n",
      "['G#', 'F##', 'G#', 'A#', 'G#', 'A#', 'D#', 'E#', 'F##', 'G#']\n",
      "[8, 7, 8, 10, 8, 10, 3, 5, 7, 8]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml m6 0\n",
      "['A', 'G#', 'A', 'B', 'A', 'B', 'E', 'F#', 'G#', 'A']\n",
      "[9, 8, 9, 11, 9, 11, 4, 6, 8, 9]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml d7 -5\n",
      "['B-', 'A', 'B-', 'C', 'B-', 'C', 'F', 'G', 'A', 'B-']\n",
      "[10, 9, 10, 0, 10, 0, 5, 7, 9, 10]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml m7 2\n",
      "['B', 'A#', 'B', 'C#', 'B', 'C#', 'F#', 'G#', 'A#', 'B']\n",
      "[11, 10, 11, 1, 11, 1, 6, 8, 10, 11]\n",
      "Bach/Fugue/bwv_873/xml_score.musicxml d1 -3\n",
      "['C', 'B', 'C', 'D', 'C', 'D', 'G', 'A', 'B', 'C']\n",
      "[0, 11, 0, 2, 0, 2, 7, 9, 11, 0]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml P1 3\n",
      "['C#', 'A', 'F#', 'D', 'D', 'C#', 'B', 'C#', 'C#', 'F#']\n",
      "[1, 9, 6, 2, 2, 1, 11, 1, 1, 6]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml m2 -2\n",
      "['D', 'B-', 'G', 'E-', 'E-', 'D', 'C', 'D', 'D', 'G']\n",
      "[2, 10, 7, 3, 3, 2, 0, 2, 2, 7]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml M2 5\n",
      "['D#', 'B', 'G#', 'E', 'E', 'D#', 'C#', 'D#', 'D#', 'G#']\n",
      "[3, 11, 8, 4, 4, 3, 1, 3, 3, 8]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml m3 0\n",
      "['E', 'C', 'A', 'F', 'F', 'E', 'D', 'E', 'E', 'A']\n",
      "[4, 0, 9, 5, 5, 4, 2, 4, 4, 9]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml d4 -5\n",
      "['F', 'D-', 'B-', 'G-', 'G-', 'F', 'E-', 'F', 'F', 'B-']\n",
      "[5, 1, 10, 6, 6, 5, 3, 5, 5, 10]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml P4 2\n",
      "['F#', 'D', 'B', 'G', 'G', 'F#', 'E', 'F#', 'F#', 'B']\n",
      "[6, 2, 11, 7, 7, 6, 4, 6, 6, 11]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml d5 -3\n",
      "['G', 'E-', 'C', 'A-', 'A-', 'G', 'F', 'G', 'G', 'C']\n",
      "[7, 3, 0, 8, 8, 7, 5, 7, 7, 0]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml P5 4\n",
      "['G#', 'E', 'C#', 'A', 'A', 'G#', 'F#', 'G#', 'G#', 'C#']\n",
      "[8, 4, 1, 9, 9, 8, 6, 8, 8, 1]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml m6 -1\n",
      "['A', 'F', 'D', 'B-', 'B-', 'A', 'G', 'A', 'A', 'D']\n",
      "[9, 5, 2, 10, 10, 9, 7, 9, 9, 2]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml d7 -6\n",
      "['B-', 'G-', 'E-', 'C-', 'C-', 'B-', 'A-', 'B-', 'B-', 'E-']\n",
      "[10, 6, 3, 11, 11, 10, 8, 10, 10, 3]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml m7 1\n",
      "['B', 'G', 'E', 'C', 'C', 'B', 'A', 'B', 'B', 'E']\n",
      "[11, 7, 4, 0, 0, 11, 9, 11, 11, 4]\n",
      "Bach/Fugue/bwv_883/xml_score.musicxml d1 -4\n",
      "['C', 'A-', 'F', 'D-', 'D-', 'C', 'B-', 'C', 'C', 'F']\n",
      "[0, 8, 5, 1, 1, 0, 10, 0, 0, 5]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml P1 -3\n",
      "['G', 'C', 'D', 'E-', 'C', 'E-', 'G', 'F', 'C', 'D']\n",
      "[7, 0, 2, 3, 0, 3, 7, 5, 0, 2]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml A1 4\n",
      "['G#', 'C#', 'D#', 'E', 'C#', 'E', 'G#', 'F#', 'C#', 'D#']\n",
      "[8, 1, 3, 4, 1, 4, 8, 6, 1, 3]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml M2 -1\n",
      "['A', 'D', 'E', 'F', 'D', 'F', 'A', 'G', 'D', 'E']\n",
      "[9, 2, 4, 5, 2, 5, 9, 7, 2, 4]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml m3 -6\n",
      "['B-', 'E-', 'F', 'G-', 'E-', 'G-', 'B-', 'A-', 'E-', 'F']\n",
      "[10, 3, 5, 6, 3, 6, 10, 8, 3, 5]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml M3 1\n",
      "['B', 'E', 'F#', 'G', 'E', 'G', 'B', 'A', 'E', 'F#']\n",
      "[11, 4, 6, 7, 4, 7, 11, 9, 4, 6]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml P4 -4\n",
      "['C', 'F', 'G', 'A-', 'F', 'A-', 'C', 'B-', 'F', 'G']\n",
      "[0, 5, 7, 8, 5, 8, 0, 10, 5, 7]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml A4 3\n",
      "['C#', 'F#', 'G#', 'A', 'F#', 'A', 'C#', 'B', 'F#', 'G#']\n",
      "[1, 6, 8, 9, 6, 9, 1, 11, 6, 8]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml P5 -2\n",
      "['D', 'G', 'A', 'B-', 'G', 'B-', 'D', 'C', 'G', 'A']\n",
      "[2, 7, 9, 10, 7, 10, 2, 0, 7, 9]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml A5 5\n",
      "['D#', 'G#', 'A#', 'B', 'G#', 'B', 'D#', 'C#', 'G#', 'A#']\n",
      "[3, 8, 10, 11, 8, 11, 3, 1, 8, 10]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml M6 0\n",
      "['E', 'A', 'B', 'C', 'A', 'C', 'E', 'D', 'A', 'B']\n",
      "[4, 9, 11, 0, 9, 0, 4, 2, 9, 11]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml m7 -5\n",
      "['F', 'B-', 'C', 'D-', 'B-', 'D-', 'F', 'E-', 'B-', 'C']\n",
      "[5, 10, 0, 1, 10, 1, 5, 3, 10, 0]\n",
      "Beethoven/Piano_Sonatas/8-3/xml_score.musicxml M7 2\n",
      "['F#', 'B', 'C#', 'D', 'B', 'D', 'F#', 'E', 'B', 'C#']\n",
      "[6, 11, 1, 2, 11, 2, 6, 4, 11, 1]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml P1 -2\n",
      "['G', 'G', 'D', 'F', 'E-', 'B-', 'A', 'C', 'E-', 'F#']\n",
      "[7, 7, 2, 5, 3, 10, 9, 0, 3, 6]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml A1 5\n",
      "['G#', 'G#', 'D#', 'F#', 'E', 'B', 'A#', 'C#', 'E', 'F##']\n",
      "[8, 8, 3, 6, 4, 11, 10, 1, 4, 7]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml M2 0\n",
      "['A', 'A', 'E', 'G', 'F', 'C', 'B', 'D', 'F', 'G#']\n",
      "[9, 9, 4, 7, 5, 0, 11, 2, 5, 8]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml m3 -5\n",
      "['B-', 'B-', 'F', 'A-', 'G-', 'D-', 'C', 'E-', 'G-', 'A']\n",
      "[10, 10, 5, 8, 6, 1, 0, 3, 6, 9]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml M3 2\n",
      "['B', 'B', 'F#', 'A', 'G', 'D', 'C#', 'E', 'G', 'A#']\n",
      "[11, 11, 6, 9, 7, 2, 1, 4, 7, 10]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml P4 -3\n",
      "['C', 'C', 'G', 'B-', 'A-', 'E-', 'D', 'F', 'A-', 'B']\n",
      "[0, 0, 7, 10, 8, 3, 2, 5, 8, 11]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml A4 4\n",
      "['C#', 'C#', 'G#', 'B', 'A', 'E', 'D#', 'F#', 'A', 'B#']\n",
      "[1, 1, 8, 11, 9, 4, 3, 6, 9, 0]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml P5 -1\n",
      "['D', 'D', 'A', 'C', 'B-', 'F', 'E', 'G', 'B-', 'C#']\n",
      "[2, 2, 9, 0, 10, 5, 4, 7, 10, 1]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml m6 -6\n",
      "['E-', 'E-', 'B-', 'D-', 'C-', 'G-', 'F', 'A-', 'C-', 'D']\n",
      "[3, 3, 10, 1, 11, 6, 5, 8, 11, 2]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml M6 1\n",
      "['E', 'E', 'B', 'D', 'C', 'G', 'F#', 'A', 'C', 'D#']\n",
      "[4, 4, 11, 2, 0, 7, 6, 9, 0, 3]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml m7 -4\n",
      "['F', 'F', 'C', 'E-', 'D-', 'A-', 'G', 'B-', 'D-', 'E']\n",
      "[5, 5, 0, 3, 1, 8, 7, 10, 1, 4]\n",
      "Bach/Prelude/bwv_885/xml_score.musicxml M7 3\n",
      "['F#', 'F#', 'C#', 'E', 'D', 'A', 'G#', 'B', 'D', 'E#']\n",
      "[6, 6, 1, 4, 2, 9, 8, 11, 2, 5]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml P1 -2\n",
      "['B-', 'B-', 'D', 'C', 'C', 'F', 'D', 'D', 'B-', 'B-']\n",
      "[10, 10, 2, 0, 0, 5, 2, 2, 10, 10]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml A1 5\n",
      "['B', 'B', 'D#', 'C#', 'C#', 'F#', 'D#', 'D#', 'B', 'B']\n",
      "[11, 11, 3, 1, 1, 6, 3, 3, 11, 11]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml M2 0\n",
      "['C', 'C', 'E', 'D', 'D', 'G', 'E', 'E', 'C', 'C']\n",
      "[0, 0, 4, 2, 2, 7, 4, 4, 0, 0]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml m3 -5\n",
      "['D-', 'D-', 'F', 'E-', 'E-', 'A-', 'F', 'F', 'D-', 'D-']\n",
      "[1, 1, 5, 3, 3, 8, 5, 5, 1, 1]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml M3 2\n",
      "['D', 'D', 'F#', 'E', 'E', 'A', 'F#', 'F#', 'D', 'D']\n",
      "[2, 2, 6, 4, 4, 9, 6, 6, 2, 2]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml P4 -3\n",
      "['E-', 'E-', 'G', 'F', 'F', 'B-', 'G', 'G', 'E-', 'E-']\n",
      "[3, 3, 7, 5, 5, 10, 7, 7, 3, 3]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml A4 4\n",
      "['E', 'E', 'G#', 'F#', 'F#', 'B', 'G#', 'G#', 'E', 'E']\n",
      "[4, 4, 8, 6, 6, 11, 8, 8, 4, 4]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml P5 -1\n",
      "['F', 'F', 'A', 'G', 'G', 'C', 'A', 'A', 'F', 'F']\n",
      "[5, 5, 9, 7, 7, 0, 9, 9, 5, 5]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml m6 -6\n",
      "['G-', 'G-', 'B-', 'A-', 'A-', 'D-', 'B-', 'B-', 'G-', 'G-']\n",
      "[6, 6, 10, 8, 8, 1, 10, 10, 6, 6]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml M6 1\n",
      "['G', 'G', 'B', 'A', 'A', 'D', 'B', 'B', 'G', 'G']\n",
      "[7, 7, 11, 9, 9, 2, 11, 11, 7, 7]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml m7 -4\n",
      "['A-', 'A-', 'C', 'B-', 'B-', 'E-', 'C', 'C', 'A-', 'A-']\n",
      "[8, 8, 0, 10, 10, 3, 0, 0, 8, 8]\n",
      "Schumann/Kreisleriana/2/xml_score.musicxml M7 3\n",
      "['A', 'A', 'C#', 'B', 'B', 'E', 'C#', 'C#', 'A', 'A']\n",
      "[9, 9, 1, 11, 11, 4, 1, 1, 9, 9]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml P1 -3\n",
      "['E-', 'G', 'B-', 'E-', 'G', 'D', 'E-', 'G', 'B-', 'E-']\n",
      "[3, 7, 10, 3, 7, 2, 3, 7, 10, 3]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml A1 4\n",
      "['E', 'G#', 'B', 'E', 'G#', 'D#', 'E', 'G#', 'B', 'E']\n",
      "[4, 8, 11, 4, 8, 3, 4, 8, 11, 4]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml M2 -1\n",
      "['F', 'A', 'C', 'F', 'A', 'E', 'F', 'A', 'C', 'F']\n",
      "[5, 9, 0, 5, 9, 4, 5, 9, 0, 5]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml A2 6\n",
      "['F#', 'A#', 'C#', 'F#', 'A#', 'E#', 'F#', 'A#', 'C#', 'F#']\n",
      "[6, 10, 1, 6, 10, 5, 6, 10, 1, 6]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml M3 1\n",
      "['G', 'B', 'D', 'G', 'B', 'F#', 'G', 'B', 'D', 'G']\n",
      "[7, 11, 2, 7, 11, 6, 7, 11, 2, 7]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml P4 -4\n",
      "['A-', 'C', 'E-', 'A-', 'C', 'G', 'A-', 'C', 'E-', 'A-']\n",
      "[8, 0, 3, 8, 0, 7, 8, 0, 3, 8]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml A4 3\n",
      "['A', 'C#', 'E', 'A', 'C#', 'G#', 'A', 'C#', 'E', 'A']\n",
      "[9, 1, 4, 9, 1, 8, 9, 1, 4, 9]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml P5 -2\n",
      "['B-', 'D', 'F', 'B-', 'D', 'A', 'B-', 'D', 'F', 'B-']\n",
      "[10, 2, 5, 10, 2, 9, 10, 2, 5, 10]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml A5 5\n",
      "['B', 'D#', 'F#', 'B', 'D#', 'A#', 'B', 'D#', 'F#', 'B']\n",
      "[11, 3, 6, 11, 3, 10, 11, 3, 6, 11]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml M6 0\n",
      "['C', 'E', 'G', 'C', 'E', 'B', 'C', 'E', 'G', 'C']\n",
      "[0, 4, 7, 0, 4, 11, 0, 4, 7, 0]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml m7 -5\n",
      "['D-', 'F', 'A-', 'D-', 'F', 'C', 'D-', 'F', 'A-', 'D-']\n",
      "[1, 5, 8, 1, 5, 0, 1, 5, 8, 1]\n",
      "Beethoven/Piano_Sonatas/18-4/xml_score.musicxml M7 2\n",
      "['D', 'F#', 'A', 'D', 'F#', 'C#', 'D', 'F#', 'A', 'D']\n",
      "[2, 6, 9, 2, 6, 1, 2, 6, 9, 2]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml P1 -2\n",
      "['D', 'B-', 'A', 'B-', 'F', 'F', 'D', 'B-', 'B-', 'D']\n",
      "[2, 10, 9, 10, 5, 5, 2, 10, 10, 2]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml A1 5\n",
      "['D#', 'B', 'A#', 'B', 'F#', 'F#', 'D#', 'B', 'B', 'D#']\n",
      "[3, 11, 10, 11, 6, 6, 3, 11, 11, 3]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml M2 0\n",
      "['E', 'C', 'B', 'C', 'G', 'G', 'E', 'C', 'C', 'E']\n",
      "[4, 0, 11, 0, 7, 7, 4, 0, 0, 4]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml m3 -5\n",
      "['F', 'D-', 'C', 'D-', 'A-', 'A-', 'F', 'D-', 'D-', 'F']\n",
      "[5, 1, 0, 1, 8, 8, 5, 1, 1, 5]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml M3 2\n",
      "['F#', 'D', 'C#', 'D', 'A', 'A', 'F#', 'D', 'D', 'F#']\n",
      "[6, 2, 1, 2, 9, 9, 6, 2, 2, 6]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml P4 -3\n",
      "['G', 'E-', 'D', 'E-', 'B-', 'B-', 'G', 'E-', 'E-', 'G']\n",
      "[7, 3, 2, 3, 10, 10, 7, 3, 3, 7]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml A4 4\n",
      "['G#', 'E', 'D#', 'E', 'B', 'B', 'G#', 'E', 'E', 'G#']\n",
      "[8, 4, 3, 4, 11, 11, 8, 4, 4, 8]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml P5 -1\n",
      "['A', 'F', 'E', 'F', 'C', 'C', 'A', 'F', 'F', 'A']\n",
      "[9, 5, 4, 5, 0, 0, 9, 5, 5, 9]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml m6 -6\n",
      "['B-', 'G-', 'F', 'G-', 'D-', 'D-', 'B-', 'G-', 'G-', 'B-']\n",
      "[10, 6, 5, 6, 1, 1, 10, 6, 6, 10]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml M6 1\n",
      "['B', 'G', 'F#', 'G', 'D', 'D', 'B', 'G', 'G', 'B']\n",
      "[11, 7, 6, 7, 2, 2, 11, 7, 7, 11]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml m7 -4\n",
      "['C', 'A-', 'G', 'A-', 'E-', 'E-', 'C', 'A-', 'A-', 'C']\n",
      "[0, 8, 7, 8, 3, 3, 0, 8, 8, 0]\n",
      "Beethoven/Piano_Sonatas/11-1/xml_score.musicxml M7 3\n",
      "['C#', 'A', 'G#', 'A', 'E', 'E', 'C#', 'A', 'A', 'C#']\n",
      "[1, 9, 8, 9, 4, 4, 1, 9, 9, 1]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml P1 -4\n",
      "['E-', 'A-', 'A-', 'C', 'A-', 'C', 'B-', 'A-', 'D-', 'B-']\n",
      "[3, 8, 8, 0, 8, 0, 10, 8, 1, 10]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml A1 3\n",
      "['E', 'A', 'A', 'C#', 'A', 'C#', 'B', 'A', 'D', 'B']\n",
      "[4, 9, 9, 1, 9, 1, 11, 9, 2, 11]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml M2 -2\n",
      "['F', 'B-', 'B-', 'D', 'B-', 'D', 'C', 'B-', 'E-', 'C']\n",
      "[5, 10, 10, 2, 10, 2, 0, 10, 3, 0]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml A2 5\n",
      "['F#', 'B', 'B', 'D#', 'B', 'D#', 'C#', 'B', 'E', 'C#']\n",
      "[6, 11, 11, 3, 11, 3, 1, 11, 4, 1]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml M3 0\n",
      "['G', 'C', 'C', 'E', 'C', 'E', 'D', 'C', 'F', 'D']\n",
      "[7, 0, 0, 4, 0, 4, 2, 0, 5, 2]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml P4 -5\n",
      "['A-', 'D-', 'D-', 'F', 'D-', 'F', 'E-', 'D-', 'G-', 'E-']\n",
      "[8, 1, 1, 5, 1, 5, 3, 1, 6, 3]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml A4 2\n",
      "['A', 'D', 'D', 'F#', 'D', 'F#', 'E', 'D', 'G', 'E']\n",
      "[9, 2, 2, 6, 2, 6, 4, 2, 7, 4]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml P5 -3\n",
      "['B-', 'E-', 'E-', 'G', 'E-', 'G', 'F', 'E-', 'A-', 'F']\n",
      "[10, 3, 3, 7, 3, 7, 5, 3, 8, 5]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml A5 4\n",
      "['B', 'E', 'E', 'G#', 'E', 'G#', 'F#', 'E', 'A', 'F#']\n",
      "[11, 4, 4, 8, 4, 8, 6, 4, 9, 6]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml M6 -1\n",
      "['C', 'F', 'F', 'A', 'F', 'A', 'G', 'F', 'B-', 'G']\n",
      "[0, 5, 5, 9, 5, 9, 7, 5, 10, 7]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml m7 -6\n",
      "['D-', 'G-', 'G-', 'B-', 'G-', 'B-', 'A-', 'G-', 'C-', 'A-']\n",
      "[1, 6, 6, 10, 6, 10, 8, 6, 11, 8]\n",
      "Chopin/Etudes_op_10/10/xml_score.musicxml M7 1\n",
      "['D', 'G', 'G', 'B', 'G', 'B', 'A', 'G', 'C', 'A']\n",
      "[2, 7, 7, 11, 7, 11, 9, 7, 0, 9]\n",
      "Chopin/Ballades/1/xml_score.musicxml P1 -2\n",
      "['C', 'C', 'C', 'C', 'E-', 'E-', 'A-', 'A-', 'B-', 'B-']\n",
      "[0, 0, 0, 0, 3, 3, 8, 8, 10, 10]\n",
      "Chopin/Ballades/1/xml_score.musicxml m2 -7\n",
      "['D-', 'D-', 'D-', 'D-', 'F-', 'F-', 'B--', 'B--', 'C-', 'C-']\n",
      "[1, 1, 1, 1, 4, 4, 9, 9, 11, 11]\n",
      "Chopin/Ballades/1/xml_score.musicxml M2 0\n",
      "['D', 'D', 'D', 'D', 'F', 'F', 'B-', 'B-', 'C', 'C']\n",
      "[2, 2, 2, 2, 5, 5, 10, 10, 0, 0]\n",
      "Chopin/Ballades/1/xml_score.musicxml m3 -5\n",
      "['E-', 'E-', 'E-', 'E-', 'G-', 'G-', 'C-', 'C-', 'D-', 'D-']\n",
      "[3, 3, 3, 3, 6, 6, 11, 11, 1, 1]\n",
      "Chopin/Ballades/1/xml_score.musicxml M3 2\n",
      "['E', 'E', 'E', 'E', 'G', 'G', 'C', 'C', 'D', 'D']\n",
      "[4, 4, 4, 4, 7, 7, 0, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# choose only one enharmonic version for each chromatic interval for each piece\n",
    "dict_dataset = []\n",
    "for path in paths:\n",
    "    for c in range(12):\n",
    "        pieces_to_consider = [opus for opus in full_dict_dataset \n",
    "                              if (opus[\"original_path\"] == path and opus[\"transposed_of\"] in interval_dict[c])  ]\n",
    "        # if the original is in pieces_to_consider, go with the original\n",
    "        originals = [opus for opus in pieces_to_consider if opus[\"transposed_of\"] == \"P1\"]\n",
    "        if len(originals) == 1:\n",
    "            dict_dataset.append(originals[0])\n",
    "        else: #we go with the accidental minization criteria\n",
    "            n_accidentals = [sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in opus[\"pitches\"]]) \n",
    "                            for opus in pieces_to_consider]\n",
    "            if len(pieces_to_consider)>0:\n",
    "                dict_dataset.append(pieces_to_consider[np.argmin(n_accidentals)])\n",
    "            else:\n",
    "                print(\"No options for\", path, \". Chromatic: \",c )\n",
    "\n",
    "# accepted_ks = range(-5,6)\n",
    "# dict_dataset = [e for e in full_dict_dataset if e[\"key_signature\"] in accepted_ks]\n",
    "\n",
    "#test if it worked\n",
    "for i,e in enumerate(dict_dataset):\n",
    "    print(e[\"original_path\"], e[\"transposed_of\"], e[\"key_signature\"])\n",
    "    print(e[\"pitches\"][:10])\n",
    "    print(e[\"midi_number\"][:10])\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgHkMeoJdb42",
    "outputId": "1f3d172f-3f93-47ce-aa1b-e37a2afba24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2271\n",
      "Counter({'Bach': 59, 'Beethoven': 57, 'Chopin': 34, 'Schubert': 13, 'Haydn': 11, 'Schumann': 10, 'Mozart': 6, 'Brahms': 1})\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_dataset))\n",
    "\n",
    "c = Counter()\n",
    "for p in paths:\n",
    "    c[p.split(\"/\")[0]] +=1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 initial pieces\n",
      "190 pieces after removing overlapping with musedata\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove pieces from asap that are in Musedata\n",
    "print(len(paths), \"initial pieces\")\n",
    "paths = [p for p in paths if p!= \"Bach/Prelude/bwv_865/xml_score.musicxml\"]\n",
    "print(len(paths), \"pieces after removing overlapping with musedata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GRCM2W3qqeW",
    "outputId": "d53c8146-8b32-43df-cd69-b7423f1ceda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation lenghts:  160 29\n"
     ]
    }
   ],
   "source": [
    "# Temporary remove composer with only one piece, because they create problems with sklearn stratify\n",
    "one_piece_composers = ['Balakirev','Prokofiev','Brahms','Glinka']\n",
    "paths = [p for p in paths if p.split(\"/\")[0] not in one_piece_composers]\n",
    "\n",
    "# Divide train and validation set\n",
    "path_train, path_validation = sklearn.model_selection.train_test_split(paths, test_size=0.15,stratify=[p.split(\"/\")[0] for p in paths ])\n",
    "print(\"Train and validation lenghts: \",len(path_train),len(path_validation))\n",
    "\n",
    "#Put back one piece composers in the validation dataset\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "KIZ2_bX1bom5",
    "outputId": "ae8d5b50-ed4d-4376-f9d6-e96c6aa73046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mozart', 'Bach', 'Schubert', 'Haydn', 'Chopin', 'Schumann', 'Beethoven']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 383
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to find a better way to visualize this\n",
    "composers = list(set([p.split(\"/\")[0] for p in paths ]))\n",
    "print(composers)\n",
    "\n",
    "train_composer = [composers.index(p.split(\"/\")[0]) for p in path_train]\n",
    "val_composer = [composers.index(p.split(\"/\")[0]) for p in path_validation]\n",
    "\n",
    "_ = plt.hist([train_composer, val_composer], label=['train', 'validation'])\n",
    "_ = plt.legend(loc='upper left')\n",
    "_ = plt.xticks(list(range(len(composers))), composers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-qaT10ApkCY"
   },
   "source": [
    "## Transform the input into a convenient format for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEYec2TDOWvj",
    "outputId": "1b69573b-cfeb-4dc9-8882-da42f9c25537"
   },
   "outputs": [],
   "source": [
    "# Helper functions to feed the correct input into the NN \n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "tag_to_ix = {p: accepted_pitches.index(p) for p in accepted_pitches}\n",
    "#add PADDING TAD\n",
    "tag_to_ix[PAD] = len(accepted_pitches)\n",
    "\n",
    "midi_to_ix = {m: m for m in range(12)}\n",
    "# #add PADDING TAD\n",
    "# midi_to_ix[PAD] = 12\n",
    "\n",
    "# print(midi_to_ix[1])\n",
    "# print(len(midi_to_ix))\n",
    "\n",
    "duration_delimiter = [0.125,0.25,0.5,1,2,4]\n",
    "\n",
    "\n",
    "class Pitch2Diatonic():\n",
    "    def __call__(self, in_seq,weights = None):\n",
    "        return [p for p in in_seq]\n",
    "\n",
    "class Diatonic2Int():\n",
    "    def __call__(self, in_seq,weights = None):\n",
    "        idxs = [tag_to_ix[w] for w in in_seq]\n",
    "        return idxs\n",
    "\n",
    "class Int2Pitch():\n",
    "    def __call__(self, in_seq):\n",
    "        return [accepted_pitches[i] for i in in_seq]\n",
    "\n",
    "class OneHotEncoder():\n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample,weights = None):\n",
    "        onehot = np.zeros([len(sample), self.alphabet_len])\n",
    "        tot_chars = len(sample)\n",
    "        onehot[np.arange(tot_chars), sample] = 1\n",
    "        return onehot\n",
    "\n",
    "# class WeightedOneHotEncoder():\n",
    "#     def __init__(self, alphabet_len):\n",
    "#         self.alphabet_len = alphabet_len\n",
    "        \n",
    "#     def __call__(self, sample, weights=None):\n",
    "#         if weights == None:\n",
    "#             weights = np.ones(len(sample))\n",
    "#         onehot = torch.nn.functional.one_hot(sample,self.alphabet_len)\n",
    "#         return (onehot.t()*torch.Tensor(weights)).t() #transpositions to allow the broadcasting\n",
    "    \n",
    "class DurationOneHotEncoder():\n",
    "    def __init__(self, pitch_alphabet_len, duration_delimiter):\n",
    "        self.pitch_alphabet_len = pitch_alphabet_len\n",
    "        self.dur_alphabet_len = len(duration_delimiter)+2\n",
    "        self.duration_delimiter = duration_delimiter\n",
    "        \n",
    "    def __call__(self, sample, durs):\n",
    "        sample = torch.tensor(sample,dtype=torch.long)\n",
    "        onehot_pitch = torch.nn.functional.one_hot(sample,self.pitch_alphabet_len)\n",
    "        quantized_durations = np.digitize(durs,self.duration_delimiter)\n",
    "        quantized_durations = torch.tensor(quantized_durations,dtype=torch.long)\n",
    "        onehot_duration = torch.nn.functional.one_hot(quantized_durations,self.dur_alphabet_len)\n",
    "        return torch.cat([onehot_pitch,onehot_duration],1)\n",
    "        \n",
    "class ToTensorFloat():\n",
    "    def __call__(self, sample, weight = None):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.float()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.float)\n",
    "\n",
    "class ToTensorLong():\n",
    "    def __call__(self, sample, weights = None):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.long()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.long)\n",
    "    \n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, sample, weights):\n",
    "        for t in self.transforms:\n",
    "            sample = t(sample, weights)\n",
    "        return sample\n",
    "\n",
    "pitches_len = len(accepted_pitches)\n",
    "midinote_len = 12\n",
    "\n",
    "### Define the preprocessing pipeline\n",
    "transform_diat = Compose([Pitch2Diatonic(),Diatonic2Int(),ToTensorLong()])\n",
    "transform_chrom = Compose([DurationOneHotEncoder(len(midi_to_ix),duration_delimiter),ToTensorFloat()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =[list(range(10)) for e in range(4)]\n",
    "torch.Tensor(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jV9boYaToUs2",
    "outputId": "92bcd8d3-87a2-4d2c-8e92-4a106139f59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905 29\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n",
      "tensor([ 1,  0,  1,  3,  1,  3,  8, 10,  0,  1,  3,  4,  6,  4,  6,  3,  8,  6,\n",
      "         8,  4,  7,  3,  8,  4, 10,  1,  8, 10,  3,  1])\n",
      "['C#', 'B#', 'C#', 'D#', 'C#', 'D#', 'G#', 'A#', 'B#', 'C#', 'D#', 'E', 'F#', 'E', 'F#', 'D#', 'G#', 'F#', 'G#', 'E', 'F##', 'D#', 'G#', 'E', 'A#', 'C#', 'G#', 'A#', 'D#', 'C#']\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "# TODO: order sequences for better memory efficiency\n",
    "\n",
    "class PSDataset(Dataset):\n",
    "    def __init__(self, dict_dataset, paths, transf_c, transf_d, augment_dataset, truncate = None):\n",
    "        if augment_dataset:\n",
    "            self.chromatic_sequences = [e[\"midi_number\"] for e in dict_dataset if e[\"original_path\"] in paths]\n",
    "            self.diatonic_sequences = [e[\"pitches\"]\n",
    "                                       for e in dict_dataset \n",
    "                                       if e[\"original_path\"] in paths]\n",
    "            self.durations = [e[\"duration\"] for e in dict_dataset if e[\"original_path\"] in paths]\n",
    "        else: #consider only non transposed pieces\n",
    "            self.chromatic_sequences = [e[\"midi_number\"] for e in dict_dataset \n",
    "                                        if (e[\"original_path\"] in paths and e[\"transposed_of\"]==\"P1\")]\n",
    "            self.diatonic_sequences = [e[\"pitches\"]\n",
    "                                       for e in dict_dataset \n",
    "                                       if (e[\"original_path\"] in paths and e[\"transposed_of\"]==\"P1\")]\n",
    "            self.durations = [e[\"duration\"] \n",
    "                              for e in dict_dataset \n",
    "                              if (e[\"original_path\"] in paths and e[\"transposed_of\"]==\"P1\")]\n",
    "        #the transformations to apply to data\n",
    "        self.transf_c = transf_c\n",
    "        self.transf_d = transf_d\n",
    "        self.truncate = truncate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chromatic_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chromatic_seq = self.chromatic_sequences[idx]\n",
    "        diatonic_seq = self.diatonic_sequences[idx]\n",
    "        duration_seq = self.durations[idx]\n",
    "        weights = [dur/4  if dur<=4 else 1 for dur in duration_seq  ] # limit the weights to (0,4)       \n",
    "\n",
    "        #transform\n",
    "        chromatic_seq = self.transf_c(chromatic_seq,weights)\n",
    "        diatonic_seq = self.transf_d(diatonic_seq,None)\n",
    "\n",
    "        if not self.truncate is None:\n",
    "            if len(diatonic_seq) > self.truncate:\n",
    "                chromatic_seq = chromatic_seq[0:self.truncate]\n",
    "                diatonic_seq = diatonic_seq[0:self.truncate]\n",
    "\n",
    "        #sanity check\n",
    "        assert len(chromatic_seq) == len(diatonic_seq)\n",
    "        seq_len = len(diatonic_seq)\n",
    "        \n",
    "        return chromatic_seq, diatonic_seq, seq_len\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,True)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat, False)\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset))\n",
    "\n",
    "\n",
    "\n",
    "# test if it works\n",
    "for chrom,diat,seq_len in train_dataset:\n",
    "    print(chrom[0:30])\n",
    "    print(torch.argmax(chrom[0:30],1))\n",
    "    # print([diatonic_pitches[p.item()] for p in diat[0:30]])\n",
    "    print([accepted_pitches[p.item()] for p in diat[0:30]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAubyjw2LC8P",
    "outputId": "d6f8574f-8901-41e7-fd7a-9a0c6091eafa"
   },
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy, l) = zip(*batch)\n",
    "    \n",
    "    xx_pad = pad_sequence(xx)\n",
    "    yy_pad = pad_sequence(yy, padding_value=tag_to_ix[PAD])\n",
    "\n",
    "    #sort the sequences by length\n",
    "    seq_lengths, perm_idx = torch.Tensor(l).sort(0, descending=True)\n",
    "    xx_pad = xx_pad[:,perm_idx,:]\n",
    "    yy_pad = yy_pad[:,perm_idx]\n",
    "\n",
    "    return xx_pad, yy_pad, seq_lengths\n",
    "\n",
    "# data_loader = DataLoader(dataset=validation_dataset, batch_size=4, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "\n",
    "# #test if it work\n",
    "# for batch in data_loader:\n",
    "#     print(batch[0].shape,batch[1].shape,batch[2])\n",
    "#     print(batch[1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O1OA1AjGWO-"
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QEkFBY5cUsmN"
   },
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum',ignore_index = tag_to_ix[PAD])\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out = self.top_layer(rnn_out)\n",
    "       \n",
    "        # # Find the positions where the token is a dummy padding token.\n",
    "        # pad_mask = (sentences == self.pad_word_id).float()\n",
    "\n",
    "        # # For these positions, we add some large number in the column corresponding\n",
    "        # # to the dummy padding label.\n",
    "        # out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "            \n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        return [predicted[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QE7itQ88Qx17"
   },
   "outputs": [],
   "source": [
    "class RNNCRFTagger(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNCRFTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        self.crf = CRF(self.n_labels)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        ## should I initialize here??\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out = self.top_layer(rnn_out)\n",
    "      \n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # Compute the outputs of the lower layers, which will be used as emission\n",
    "        # scores for the CRF.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
    "        # the *negative* log likelihood as the loss value.            \n",
    "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
    "        # log likelihood.\n",
    "\n",
    "        pad_mask = torch.arange(max(sentences_len))[:, None] < sentences_len[None, :]\n",
    "        pad_mask = pad_mask.byte().to(device)\n",
    "        return -self.crf(scores, labels, mask = pad_mask )\n",
    "            \n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the emission scores, as above.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
    "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
    "        # of shape (n_sentences, max_len).\n",
    "        pad_mask = torch.arange(max(sentences_len))[:, None] < sentences_len[None, :]\n",
    "        pad_mask = pad_mask.byte().to(device)\n",
    "        return self.crf.decode(scores,mask = pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttentionTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum',ignore_index = tag_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        # TODO : set right parameters\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        # use attention\n",
    "        attn_applied = self.attention(rnn_out,rnn_out,sentences_len)\n",
    "        \n",
    "        out = self.top_layer(attn_applied) #maybe remove this one?\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "            \n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        return [predicted[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention,self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "            hidden_dim, hidden_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "    def forward(self,\n",
    "                query, # [seq_len, batch, hidden_dim]\n",
    "                values, # [seq_len, batch, hidden_dim]\n",
    "                sentences_len\n",
    "               ):\n",
    "        weights = self._get_weights(query, values) # [batch,seq_length,hidden_dim]\n",
    "        \n",
    "        weights = torch.nn.functional.softmax(weights, dim=2)\n",
    "        # mask the weights\n",
    "        inverted_pad_mask = torch.arange(max(sentences_len))[None,:] > sentences_len[:,None]\n",
    "        inverted_pad_mask = (inverted_pad_mask.float()*(-10000))[:,:,None].to(device)\n",
    "#         print(weights.shape,inverted_pad_mask.shape )\n",
    "        #apply the mast\n",
    "        weights = weights - inverted_pad_mask\n",
    "        out = torch.transpose((weights @ torch.transpose(values,0,1)),0,1)\n",
    "#         print(\"ATT out shape\", out.shape)\n",
    "        return out # [seq_len,batch,encoder_dim]\n",
    "\n",
    "    def _get_weights(self,\n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        #transpose to batch first to correctly handle batch multiplications\n",
    "#         print(\"shape\",query.shape,self.W.shape, values.shape)\n",
    "        query,values = torch.transpose(query,0,1),torch.transpose(values,0,1)\n",
    "#         print(\"shape\",query.shape,self.W.shape, values.shape)\n",
    "#         print(\"stape values.t\", torch.transpose(values,1,2).shape)\n",
    "        weights = query @ self.W @ torch.transpose(values,1,2)  # [seq_length]\n",
    "#         print(\"out att shape\", weights.shape)\n",
    "        return weights/np.sqrt(self.hidden_dim)  # [seq_length]\n",
    "    \n",
    "# class Attention(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, encoder_dim: int, decoder_dim: int):\n",
    "#         super().__init__()\n",
    "#         self.encoder_dim = encoder_dim\n",
    "#         self.decoder_dim = decoder_dim\n",
    "#         self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "#             self.decoder_dim, self.encoder_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "#     def forward(self, \n",
    "#         query: torch.Tensor,  # [decoder_dim]\n",
    "#         values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "#         ):\n",
    "#         weights = self._get_weights(query, values) # [seq_length]\n",
    "#         weights = torch.nn.functional.softmax(weights, dim=0)\n",
    "#         return weights @ values  # [encoder_dim]\n",
    "    \n",
    "#     def _get_weights(self,\n",
    "#         query: torch.Tensor,  # [decoder_dim]\n",
    "#         values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "#     ):\n",
    "#         weights = query @ self.W @ values.T  # [seq_length]\n",
    "#         return weights/np.sqrt(self.decoder_dim)  # [seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXixmVQfvw8T"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3m_nj4HCuCe1"
   },
   "outputs": [],
   "source": [
    "# TODO: search over the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uAMSIlw0AJb6"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_dataloader, val_dataloader, n_epochs):\n",
    "    history = defaultdict(list)  \n",
    "    for i_epoch in range(1,n_epochs +1):\n",
    "        t0 = time.time()\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        model.train()\n",
    "        for seqs, targets, lens in train_dataloader: #seqs, targets, lens are batches\n",
    "            seqs, targets = seqs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "#             print(\"input seq shape:\",seqs.shape)\n",
    "\n",
    "            loss = model(seqs,targets,lens) / sum(lens) #normalize for the number of symbol considered (without padding)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            predicted = model.predict(seqs,lens)\n",
    "            for i,p in enumerate(predicted):\n",
    "                acc= accuracy_score(p,targets[:,i][:len(p)].cpu()) #compute the accuracy without considering the padding\n",
    "                accuracy_sum += acc/len(lens) #normalize according to the number of sequences in the batch\n",
    "\n",
    "        train_loss = loss_sum/len(train_dataloader)\n",
    "        train_accuracy = accuracy_sum/len(train_dataloader) #normalize according to the number of batches\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        all_predicted = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for seqs,targets, lens in val_dataloader:\n",
    "                # Predict the model's output on a batch\n",
    "                predicted = model.predict(seqs.to(device),lens)                   \n",
    "                # Update the lists that will be used to compute the accuracy\n",
    "                for i,p in enumerate(predicted):\n",
    "                    all_predicted.append(torch.Tensor(p))\n",
    "                    all_targets.append(targets[0:int(lens[i]),i])\n",
    "                \n",
    "        # Compute the overall accuracy for the validation set\n",
    "        val_accuracy = accuracy_score(torch.cat(all_predicted),torch.cat(all_targets))\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "#         save the model\n",
    "        torch.save(model, \"./models/temp/model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "#         files.download(\"model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "\n",
    "    \n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {i_epoch}: train loss = {train_loss:.4f}, train_accuracy: {train_accuracy:.4f},val_accuracy: {val_accuracy:.4f}, time = {t1-t0:.4f}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "2g2st8znpGW_",
    "outputId": "fe232e42-e270-4f5d-9bd8-718bba5e7151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n",
      "Epoch 1: train loss = 1.7062, train_accuracy: 0.4953,val_accuracy: 0.9117, time = 136.6816\n",
      "Epoch 2: train loss = 0.3436, train_accuracy: 0.8998,val_accuracy: 0.9240, time = 138.4452\n",
      "Epoch 3: train loss = 0.2872, train_accuracy: 0.9120,val_accuracy: 0.9250, time = 137.3443\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.09\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,True, truncate = None)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat, False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False,collate_fn=pad_collate)\n",
    "\n",
    "# model = torch.load(\"./models/temp/model_temp_epoch30-to_restart.pkl\")\n",
    "# model = RNNCRFTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "model = RNNAttentionTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "model = model.to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_WEIGHT)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_WEIGHT, momentum = 0.9,weight_decay=WEIGHT_DECAY)\n",
    "# otimizer = torch.optim.Adam(model.parameters(),lr = 0.05, weight_decay=1e-4)\n",
    "\n",
    "history = training_loop(model,optimizer,train_dataloader,val_dataloader, n_epochs)\n",
    "\n",
    "# After the final evaluation, we print more detailed evaluation statistics,\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best working model on the accuracy\n",
    "max_accuracy = np.max(history['val_accuracy'])\n",
    "best_epoch = np.argmax(history['val_accuracy'])\n",
    "print(\"Best validation accuracy: \",max_accuracy, \"at epoch\",best_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tTv2BrGbvSHh",
    "outputId": "801ff32d-24dc-4434-a436-17d9ca832214"
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"./models/model_asap_crf200dur.pkl\")\n",
    "# files.download(\"model_asap_crf300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXcmLpCKt28l"
   },
   "source": [
    "## Test on Mdata dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./models/model_temp_CRFacc9548.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BacUqgD5usGL"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open(Path(basepath,'./datasets/musedata.pkl'), 'rb') as fid:\n",
    "     full_mdata_dict_dataset = pickle.load( fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgaUwW2fuwg0",
    "outputId": "89fc5f7d-884a-4ee6-fc34-b3d512f0c4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 different pieces\n",
      "Average number of notes:  858.6319771007974\n"
     ]
    }
   ],
   "source": [
    "mdata_paths = list(set([e[\"original_path\"] for e in full_mdata_dict_dataset ]))\n",
    "\n",
    "# # remove the symbphony No.100 from Haydn because of the enharmonic transposition\n",
    "# paths.remove(\"datasets\\\\opnd\\\\haydndoversyms-10004m.opnd-m\")\n",
    "\n",
    "# print(paths)\n",
    "print(len(mdata_paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_mdata_dict_dataset ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "f-Fg6gJKvNLt"
   },
   "outputs": [],
   "source": [
    "mdata_dataset = PSDataset(full_mdata_dict_dataset,mdata_paths, transform_chrom,transform_diat,False)\n",
    "mdata_dataloader  = DataLoader(mdata_dataset,  batch_size=16, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ml905Mtdvj-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "all_targets = []\n",
    "model.eval() # Evaluation mode (e.g. disable dropout)\n",
    "with torch.no_grad(): # Disable gradient tracking\n",
    "    for seqs, targets,lens in mdata_dataloader:\n",
    "        # Move data to device\n",
    "        seqs = seqs.to(device)\n",
    "\n",
    "        # Predict the model's output on a batch.\n",
    "        predicted = model.predict(seqs,lens)                   \n",
    "        # Update the evaluation statistics.\n",
    "        for i,p in enumerate(predicted):\n",
    "            all_inputs.append(torch.argmax(seqs[0:int(lens[i]),i,:].cpu(),1).numpy())\n",
    "            all_outputs.append(torch.Tensor(p))\n",
    "            all_targets.append(targets[0:int(lens[i]),i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsefdSHxvrWy",
    "outputId": "33785b90-5e03-4a6d-86dd-92b6b763a505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['viv', 'tel', 'bee', 'bac', 'han', 'cor', 'moz', 'hay']\n"
     ]
    }
   ],
   "source": [
    "# Divide accuracy according to author\n",
    "authors = []\n",
    "\n",
    "for sequence in all_inputs:\n",
    "    author = [e[\"original_path\"].split(\"\\\\\")[-1][:3] for e in full_mdata_dict_dataset\n",
    "              if len(e[\"midi_number\"]) == len(sequence) and\n",
    "              list(e[\"midi_number\"]) ==list(sequence) ]\n",
    "    # assert len(author) == 1\n",
    "    authors.append(author[0])\n",
    "\n",
    "considered_authors = list(set(authors))\n",
    "print(considered_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgNt_yG7vrfd",
    "outputId": "8473393f-dac6-4930-9d9e-da9c1046f1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'viv': 36, 'tel': 41, 'bee': 185, 'bac': 101, 'han': 44, 'cor': 14, 'moz': 131, 'hay': 376}\n",
      "{'viv': 0.9985304322978323, 'tel': 0.9983265306122449, 'bee': 0.9924468215408484, 'bac': 0.9958783921648643, 'han': 0.9982040816326531, 'cor': 0.9994284081166047, 'moz': 0.9946517514493345, 'hay': 0.9846467946100449}\n",
      "{'viv': 24497, 'tel': 24500, 'bee': 24493, 'bac': 24505, 'han': 24500, 'cor': 24493, 'moz': 24494, 'hay': 24490}\n",
      "Total errors : 928\n"
     ]
    }
   ],
   "source": [
    "errors_per_author = {}\n",
    "accuracy_per_author = {}\n",
    "notes_per_author = {}\n",
    "for ca in considered_authors:\n",
    "    # ca_inputs = torch.cat([all_inputs[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_outputs = torch.cat([all_outputs[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_targets = torch.cat([all_targets[i] for i,a in enumerate(authors) if a == ca])\n",
    "    # print(ca_inputs.shape,ca_outputs.shape,ca_targets.shape,ca_predicted_pitches.shape)\n",
    "    ca_acc = accuracy_score(ca_outputs,ca_targets)\n",
    "    accuracy_per_author[ca] = float(ca_acc)\n",
    "    errors_per_author[ca] = int(len(ca_targets) - sum(torch.eq(ca_outputs,ca_targets)))\n",
    "    notes_per_author[ca] = len(ca_targets)\n",
    "\n",
    "print(errors_per_author)\n",
    "print(accuracy_per_author)\n",
    "print(notes_per_author)\n",
    "print(\"Total errors :\", sum([e for e in errors_per_author.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5s0d56evrje"
   },
   "source": [
    "### Best accuracy for now\n",
    "for now best accuracy is with  no CRF (but considering durations) n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.09\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "duration_delimiter = [0.125,0.25,0.5,1,2,4]\n",
    "\n",
    "Model available in: \"model_temp_epoch12-noCRFacc9575.pkl\"\n",
    "accuracy on validation set 0.9575\n",
    "Trained on all dataset\n",
    "\n",
    "\n",
    "{'moz': 98, 'tel': 116, 'bac': 93, 'hay': 200, 'cor': 25, 'bee': 127, 'viv': 45, 'han': 52}\n",
    "{'moz': 0.9959990201682044, 'tel': 0.995265306122449, 'bac': 0.9962048561518058, 'hay': 0.9918334013883218, 'cor': 0.9989793002082228, 'bee': 0.9948148450577716, 'viv': 0.9981630403722905, 'han': 0.9978775510204082}\n",
    "{'moz': 24494, 'tel': 24500, 'bac': 24505, 'hay': 24490, 'cor': 24493, 'bee': 24493, 'viv': 24497, 'han': 24500}\n",
    "Total errors : 756\n",
    "\n",
    "This win by far against ps13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVvP1eH8vrl3"
   },
   "source": [
    "### Best accuracy with CRF\n",
    "n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "model = RNNCRFTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "\n",
    "Model available in: \"\"./models/model_temp_CRFacc9548.pkl\"\"\n",
    "accuracy on validation set 0.9548586557910835\n",
    "Trained on all asap dataset\n",
    "\n",
    "{'viv': 36, 'tel': 41, 'bee': 185, 'bac': 101, 'han': 44, 'cor': 14, 'moz': 131, 'hay': 376}\n",
    "{'viv': 0.9985304322978323, 'tel': 0.9983265306122449, 'bee': 0.9924468215408484, 'bac': 0.9958783921648643, 'han': 0.9982040816326531, 'cor': 0.9994284081166047, 'moz': 0.9946517514493345, 'hay': 0.9846467946100449}\n",
    "{'viv': 24497, 'tel': 24500, 'bee': 24493, 'bac': 24505, 'han': 24500, 'cor': 24493, 'moz': 24494, 'hay': 24490}\n",
    "Total errors : 928\n",
    "\n",
    "Still win against ps13"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTDh55Hh2OWCo66dGc+SES",
   "include_colab_link": true,
   "name": "rnncrf_pitch_spelling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
