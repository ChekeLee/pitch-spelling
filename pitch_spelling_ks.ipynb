{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fosfrancesco/pitch-spelling/blob/main/rnncrf_pitch_spelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fifMg_hxNSOg",
    "outputId": "a8b320f9-4231-40d9-8f10-a7b8acd1da37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hsciybUBNkur"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "import music21 as m21\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn\n",
    "import music21 as m21\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchcrf import CRF\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, notebook\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "import kmeans1d\n",
    "import jenkspy\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Naq6UG5jRbyK"
   },
   "source": [
    "# Pitch Spelling and ks Prediction\n",
    "\n",
    "Dataset: different authors from ASAP collection\n",
    "Challenges:\n",
    "- extremely long sequences\n",
    "- small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mHJvrZ2Wo_e",
    "outputId": "1319f78b-cfb1-4e2a-937c-c47cf952757c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'C'), (1, 'B#'), (2, 'D--'), (3, 'C#'), (4, 'B##'), (5, 'D-'), (6, 'D'), (7, 'C##'), (8, 'E--'), (9, 'D#'), (10, 'E-'), (11, 'F--'), (12, 'E'), (13, 'D##'), (14, 'F-'), (15, 'F'), (16, 'E#'), (17, 'G--'), (18, 'F#'), (19, 'E##'), (20, 'G-'), (21, 'G'), (22, 'F##'), (23, 'A--'), (24, 'G#'), (25, 'A-'), (26, 'A'), (27, 'G##'), (28, 'B--'), (29, 'A#'), (30, 'B-'), (31, 'C--'), (32, 'B'), (33, 'A##'), (34, 'C-')]\n",
      "['D--', 'B##', 'C##', 'E--', 'F--', 'D##', 'G--', 'E##', 'F##', 'A--', 'G##', 'B--', 'C--', 'A##']\n",
      "[(0, 'P1'), (1, 'd2'), (2, 'A7'), (3, 'm2'), (4, 'A1'), (5, 'M2'), (6, 'd3'), (7, 'AA1'), (8, 'm3'), (9, 'A2'), (10, 'M3'), (11, 'd4'), (12, 'AA2'), (13, 'P4'), (14, 'A3'), (15, 'd5'), (16, 'A4'), (17, 'P5'), (18, 'd6'), (19, 'AA4'), (20, 'm6'), (21, 'A5'), (22, 'M6'), (23, 'd7'), (24, 'AA5'), (25, 'm7'), (26, 'A6'), (27, 'M7'), (28, 'd1'), (29, 'AA6')]\n",
      "[-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "pitches_dict = {\n",
    "    0 : [\"C\",\"B#\",\"D--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    1 : [\"C#\",\"B##\",\"D-\"], # nn.Linear(input_size+context_size, 2)\n",
    "    2 : [\"D\",\"C##\",\"E--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    3 : [\"D#\",\"E-\",\"F--\"],\n",
    "    4 : [\"E\",\"D##\",\"F-\"],\n",
    "    5 : [\"F\",\"E#\",\"G--\"],\n",
    "    6 : [\"F#\",\"E##\",\"G-\"],\n",
    "    7 : [\"G\",\"F##\",\"A--\"],\n",
    "    8 : [\"G#\",\"A-\"],\n",
    "    9 : [\"A\",\"G##\",\"B--\"],\n",
    "    10 : [\"A#\",\"B-\",\"C--\"],\n",
    "    11 : [\"B\",\"A##\",\"C-\"]\n",
    "}\n",
    "\n",
    "accepted_pitches = [ii for i in pitches_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_pitches)])\n",
    "\n",
    "double_acc_pitches = [ii for i in pitches_dict.values() for ii in i if ii.endswith(\"##\") or  ii.endswith(\"--\") ]\n",
    "print(double_acc_pitches)\n",
    "\n",
    "def score2midi_numbers(score):\n",
    "    return [p.midi%12 for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "def score2pitches(score):\n",
    "    return [p.name for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "interval_dict = {\n",
    "    0 : [\"P1\",\"d2\",\"A7\"], \n",
    "    1 : [\"m2\",\"A1\"], \n",
    "    2 : [\"M2\",\"d3\",\"AA1\"], \n",
    "    3 : [\"m3\",\"A2\"],\n",
    "    4 : [\"M3\",\"d4\",\"AA2\"],\n",
    "    5 : [\"P4\",\"A3\"],\n",
    "    6 : [\"d5\",\"A4\"],\n",
    "    7 : [\"P5\",\"d6\",\"AA4\"],\n",
    "    8 : [\"m6\",\"A5\"],\n",
    "    9 : [\"M6\",\"d7\",\"AA5\"],\n",
    "    10 : [\"m7\",\"A6\"],\n",
    "    11 : [\"M7\",\"d1\",\"AA6\"]\n",
    "}\n",
    "\n",
    "accepted_intervals = [ii for i in interval_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_intervals)])\n",
    "\n",
    "def transp_score(score):\n",
    "    \"\"\" For each input return len(accepted_intervals) transposed scores\"\"\"\n",
    "    return [score.transpose(interval) for interval in accepted_intervals]\n",
    "\n",
    "def smart_transp_score(score):\n",
    "    \"\"\" For each chromatic interval chose the interval that lead to the smallest number of accidentals\"\"\"\n",
    "    scores = []\n",
    "    for chromatic_int in interval_dict.keys():\n",
    "        temp_scores = []\n",
    "        temp_acc_number = []\n",
    "        for diat_interval in interval_dict[chromatic_int]:\n",
    "            new_score = score.transpose(diat_interval)\n",
    "            temp_scores.append(new_score)\n",
    "            temp_acc_number.append(sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in score2pitches(new_score)]))\n",
    "            # print(\"choice:\", [note.name for note in temp_scores[-1].flat.notes][0:10],\"acc:\",temp_acc_number[-1] )\n",
    "        #keep only the one with the lowest number of accidentals\n",
    "        min_index = np.argmin(temp_acc_number)\n",
    "        # print(\"preferred the number\", min_index)\n",
    "        scores.append(temp_scores[min_index])\n",
    "    return scores\n",
    "\n",
    "def acc_simple_enough(score,accepted_ratio = 0.2 ):\n",
    "    pitches = score2pitches(score)\n",
    "    double_acc = sum(el in double_acc_pitches for el in pitches)\n",
    "    if double_acc/len(pitches) < accepted_ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "diatonic_pitches = [\"C\",\"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n",
    "\n",
    "accepted_ks = list(range(-7,8))\n",
    "\n",
    "print(accepted_ks)\n",
    "\n",
    "# #test acc_simple_enough()\n",
    "# score = m21.converter.parse(paths[356])\n",
    "# scores = smart_transp_score(score)\n",
    "# #delete the pieces with non accepted pitches (e.g. triple sharps)\n",
    "# scores = [s for s in scores if all(pitch in accepted_pitches for pitch in score2pitches(s))]\n",
    "# for s in scores:\n",
    "#     print(s.parts[0].flat.getElementsByClass(m21.key.KeySignature)[0], \"simple enough:\", acc_simple_enough(s))\n",
    "#     print([n.name for n in s.flat.notes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw-SuL4ZB_2C"
   },
   "source": [
    "## Import ASAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrZ0BwLEj2Gp",
    "outputId": "6b521f78-28db-4d4f-f4ef-9345c5d91db9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/fosfrancesco/pitch-spelling.git\n",
    "\n",
    "basepath = \"./\" #to change if running locally or on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WaINjDS2kpxE"
   },
   "outputs": [],
   "source": [
    "# load the asap datasets with ks\n",
    "with open(Path('../asapks.pkl'), 'rb') as fid:\n",
    "     full_dict_dataset = pickle.load( fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYQdkJAiTS6_",
    "outputId": "72835948-2884-40bb-e5b4-6ebbc8488895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 different pieces\n",
      "Average number of notes:  2410.253424657534\n"
     ]
    }
   ],
   "source": [
    "paths = list(set([e[\"original_path\"] for e in full_dict_dataset ]))\n",
    "\n",
    "# print(paths)\n",
    "print(len(paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_dict_dataset ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMdKkzNnCTrK"
   },
   "source": [
    "## Chose the convenient data augmentation\n",
    "Two possibilities:\n",
    "- for each chromatic interval, take only the diatonic transposition that produce the smallest number of accidentals (or the original if present)\n",
    "- take only a certain interval of time signatures\n",
    "\n",
    "The second is probably better for a smaller and simple dataset, but gives the problem of chosing between for example F# and G# that are both present in this bigger dataset. So we go for the first criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oign7EZ9BSX4",
    "outputId": "9aa3f694-289a-43fd-fd14-9cad3b8dd3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Transcendental_Etudes/11/xml_score.musicxml . Chromatic:  1\n",
      "No options for Liszt/Transcendental_Etudes/11/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  1\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  4\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  6\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Transcendental_Etudes/9/xml_score.musicxml . Chromatic:  6\n",
      "No options for Liszt/Transcendental_Etudes/9/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Ballades/4/xml_score.musicxml . Chromatic:  6\n",
      "No options for Ravel/Miroirs/4_Alborada_del_gracioso/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_2/1st_no_repeat/xml_score.musicxml . Chromatic:  1\n",
      "No options for Liszt/Transcendental_Etudes/4/xml_score.musicxml . Chromatic:  2\n",
      "No options for Liszt/Transcendental_Etudes/4/xml_score.musicxml . Chromatic:  4\n",
      "No options for Liszt/Transcendental_Etudes/4/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  9\n",
      "No options for Debussy/Images_Book_1/1_Reflets_dans_lEau/xml_score.musicxml . Chromatic:  3\n",
      "No options for Debussy/Images_Book_1/1_Reflets_dans_lEau/xml_score.musicxml . Chromatic:  8\n",
      "No options for Ravel/Gaspard_de_la_Nuit/1_Ondine/xml_score.musicxml . Chromatic:  4\n",
      "No options for Liszt/Sonata/xml_score.musicxml . Chromatic:  4\n",
      "No options for Liszt/Sonata/xml_score.musicxml . Chromatic:  6\n",
      "No options for Liszt/Sonata/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Transcendental_Etudes/10/xml_score.musicxml . Chromatic:  1\n",
      "No options for Liszt/Transcendental_Etudes/10/xml_score.musicxml . Chromatic:  6\n",
      "No options for Scriabin/Sonatas/5/xml_score.musicxml . Chromatic:  1\n",
      "No options for Scriabin/Sonatas/5/xml_score.musicxml . Chromatic:  6\n",
      "No options for Scriabin/Sonatas/5/xml_score.musicxml . Chromatic:  11\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  1\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  4\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  6\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  9\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  1\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  3\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  8\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  10\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Mephisto_Waltz/xml_score.musicxml . Chromatic:  11\n",
      "Before removing according to ks: 2618\n",
      "After removing according to ks: 2406\n"
     ]
    }
   ],
   "source": [
    "# choose only one enharmonic version for each chromatic interval for each piece\n",
    "dict_dataset = []\n",
    "for path in paths:\n",
    "    for c in range(12):\n",
    "        pieces_to_consider = [opus for opus in full_dict_dataset \n",
    "                              if (opus[\"original_path\"] == path and opus[\"transposed_of\"] in interval_dict[c])  ]\n",
    "        # if the original is in pieces_to_consider, go with the original\n",
    "        originals = [opus for opus in pieces_to_consider if opus[\"transposed_of\"] == \"P1\"]\n",
    "        if len(originals) == 1:\n",
    "            dict_dataset.append(originals[0])\n",
    "        else: #we go with the accidental minization criteria\n",
    "            n_accidentals = [sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in opus[\"pitches\"]]) \n",
    "                            for opus in pieces_to_consider]\n",
    "            if len(pieces_to_consider)>0:\n",
    "                dict_dataset.append(pieces_to_consider[np.argmin(n_accidentals)])\n",
    "            else:\n",
    "                print(\"No options for\", path, \". Chromatic: \",c )\n",
    "                \n",
    "#also remove unaccepted ks\n",
    "print(\"Before removing according to ks:\", len(dict_dataset))\n",
    "dict_dataset = [e for e in dict_dataset if all([k in accepted_ks for k in e[\"key_signatures\"]])]\n",
    "print(\"After removing according to ks:\", len(dict_dataset))\n",
    "                                                    \n",
    "\n",
    "# #test if it worked\n",
    "# for i,e in enumerate(dict_dataset):\n",
    "#     print(e[\"original_path\"], e[\"transposed_of\"], e[\"key_signatures\"])\n",
    "#     print(e[\"pitches\"][:10])\n",
    "#     print(e[\"midi_number\"][:10])\n",
    "#     if i == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgHkMeoJdb42",
    "outputId": "1f3d172f-3f93-47ce-aa1b-e37a2afba24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2406\n",
      "Counter({'Bach': 59, 'Beethoven': 57, 'Chopin': 34, 'Liszt': 16, 'Schubert': 13, 'Haydn': 11, 'Schumann': 10, 'Mozart': 6, 'Ravel': 4, 'Rachmaninoff': 4, 'Scriabin': 2, 'Debussy': 2, 'Brahms': 1, 'Glinka': 1, 'Balakirev': 1, 'Prokofiev': 1})\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_dataset))\n",
    "\n",
    "c = Counter()\n",
    "for p in paths:\n",
    "    c[p.split(\"/\")[0]] +=1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 initial pieces\n",
      "220 pieces after removing overlapping with musedata and Mozart Fantasie\n"
     ]
    }
   ],
   "source": [
    "# remove pieces from asap that are in Musedata\n",
    "print(len(paths), \"initial pieces\")\n",
    "paths = [p for p in paths if p!= \"Bach/Prelude/bwv_865/xml_score.musicxml\"]\n",
    "\n",
    "#remove mozart Fantasie because of incoherent key signature\n",
    "paths = [p for p in paths if p!= 'Mozart/Fantasie_475/xml_score.musicxml']\n",
    "\n",
    "print(len(paths), \"pieces after removing overlapping with musedata and Mozart Fantasie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GRCM2W3qqeW",
    "outputId": "d53c8146-8b32-43df-cd69-b7423f1ceda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation lenghts:  163 29\n"
     ]
    }
   ],
   "source": [
    "# Temporary remove composer with only one piece, because they create problems with sklearn stratify\n",
    "one_piece_composers = ['Balakirev','Prokofiev','Brahms','Glinka', 'Debussy', 'Ravel', 'Scriabin','Liszt']\n",
    "paths = [p for p in paths if p.split(\"/\")[0] not in one_piece_composers]\n",
    "\n",
    "# Divide train and validation set\n",
    "path_train, path_validation = sklearn.model_selection.train_test_split(paths, test_size=0.15,stratify=[p.split(\"/\")[0] for p in paths ])\n",
    "print(\"Train and validation lenghts: \",len(path_train),len(path_validation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "KIZ2_bX1bom5",
    "outputId": "ae8d5b50-ed4d-4376-f9d6-e96c6aa73046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mozart', 'Schumann', 'Haydn', 'Schubert', 'Rachmaninoff', 'Chopin', 'Bach', 'Beethoven']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 383
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to find a better way to visualize this\n",
    "composers = list(set([p.split(\"/\")[0] for p in paths ]))\n",
    "print(composers)\n",
    "\n",
    "train_composer = [composers.index(p.split(\"/\")[0]) for p in path_train]\n",
    "val_composer = [composers.index(p.split(\"/\")[0]) for p in path_validation]\n",
    "\n",
    "_ = plt.hist([train_composer, val_composer], label=['train', 'validation'])\n",
    "_ = plt.legend(loc='upper left')\n",
    "_ = plt.xticks(list(range(len(composers))), composers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-qaT10ApkCY"
   },
   "source": [
    "## Transform the input into a convenient format for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEYec2TDOWvj",
    "outputId": "1b69573b-cfeb-4dc9-8882-da42f9c25537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to feed the correct input into the NN \n",
    "\n",
    "N_DURATION_CLASSES = 4\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "pitch_to_ix = {p: accepted_pitches.index(p) for p in accepted_pitches}\n",
    "ks_to_ix = {k: accepted_ks.index(k) for k in accepted_ks}\n",
    "#add PADDING TAD\n",
    "pitch_to_ix[PAD] = len(accepted_pitches)\n",
    "ks_to_ix[PAD] = len(accepted_ks)\n",
    "\n",
    "midi_to_ix = {m: m for m in range(12)}\n",
    "#add PADDING TAD\n",
    "midi_to_ix[PAD] = 12\n",
    "\n",
    "# print(midi_to_ix[1])\n",
    "# print(len(midi_to_ix))\n",
    "\n",
    "\n",
    "\n",
    "# class Pitch2Diatonic():\n",
    "#     def __call__(self, in_seq):\n",
    "#         return [p for p in in_seq]\n",
    "\n",
    "class Pitch2Int():\n",
    "    def __call__(self, in_seq):\n",
    "        idxs = [pitch_to_ix[w] for w in in_seq]\n",
    "        return idxs\n",
    "    \n",
    "class Ks2Int():\n",
    "    def __call__(self, in_seq):\n",
    "        idxs = [ks_to_ix[w] for w in in_seq]\n",
    "        return idxs\n",
    "\n",
    "class Int2Pitch():\n",
    "    def __call__(self, in_seq):\n",
    "        return [accepted_pitches[i] for i in in_seq]\n",
    "\n",
    "class OneHotEncoder():\n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample,weights = None):\n",
    "        onehot = np.zeros([len(sample), self.alphabet_len])\n",
    "        tot_chars = len(sample)\n",
    "        onehot[np.arange(tot_chars), sample] = 1\n",
    "        return onehot\n",
    "    \n",
    "# class DurationOneHotEncoder():\n",
    "#     def __init__(self, pitch_alphabet_len, n_dur_class = 4):\n",
    "#         self.pitch_alphabet_len = pitch_alphabet_len\n",
    "#         self.dur_alphabet_len = n_dur_class\n",
    "        \n",
    "#     def __call__(self, sample, durs):\n",
    "#         sample = torch.tensor(sample,dtype=torch.long)\n",
    "#         onehot_pitch = torch.nn.functional.one_hot(sample,self.pitch_alphabet_len)\n",
    "#         #compute breaks in duration list\n",
    "#         if len(set(durs)) > N_DURATION_CLASSES: \n",
    "#             breaks = jenkspy.jenks_breaks(list(set(durs)), nb_class=N_DURATION_CLASSES)\n",
    "#             #quantize according to the breaks selected\n",
    "#             quantized_durations = np.digitize(durs,breaks[1:-1])\n",
    "#         elif len(set(durs)) > 2 : # in this case jenks breaks would throw an exception \n",
    "#             temp_n_classes = len(set(durs)) -1\n",
    "#             breaks = jenkspy.jenks_breaks(list(set(durs)), nb_class=temp_n_classes)\n",
    "#             # add lower classes to have the same number for all dataset\n",
    "#             for __ in range(N_DURATION_CLASSES-temp_n_classes):\n",
    "#                 breaks = [breaks[0]/2] + breaks\n",
    "#             #quantize according to the breaks selected\n",
    "#             quantized_durations = np.digitize(durs,breaks[1:-1])\n",
    "#         else: # just use custom default\n",
    "#             #quantize according to the breaks selected\n",
    "#             quantized_durations = [1 for d in durs]        \n",
    "#         quantized_durations = torch.tensor(quantized_durations,dtype=torch.long)\n",
    "#         onehot_duration = torch.nn.functional.one_hot(quantized_durations,self.dur_alphabet_len)\n",
    "#         return torch.cat([onehot_pitch,onehot_duration],1)\n",
    "\n",
    "class DurationOneHotEncoder():\n",
    "    def __init__(self, pitch_alphabet_len, n_dur_class = 4):\n",
    "        self.pitch_alphabet_len = pitch_alphabet_len\n",
    "        self.dur_alphabet_len = n_dur_class\n",
    "        \n",
    "    def __call__(self, sample, durs):\n",
    "        sample = torch.tensor(sample,dtype=torch.long)\n",
    "        onehot_pitch = torch.nn.functional.one_hot(sample,self.pitch_alphabet_len)\n",
    "        #compute breaks in duration list\n",
    "        clusters, centroids = kmeans1d.cluster(durs, N_DURATION_CLASSES)   \n",
    "        quantized_durations = torch.tensor(clusters,dtype=torch.long)\n",
    "        onehot_duration = torch.nn.functional.one_hot(quantized_durations,self.dur_alphabet_len)\n",
    "        return torch.cat([onehot_pitch,onehot_duration],1)\n",
    "    \n",
    "        \n",
    "class ToTensorFloat():\n",
    "    def __call__(self, sample, durs = None):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.float()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.float)\n",
    "\n",
    "class ToTensorLong():\n",
    "    def __call__(self, sample):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.long()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.long)\n",
    "    \n",
    "class MultInputCompose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, sample, durs):\n",
    "        for t in self.transforms:\n",
    "            sample = t(sample, durs)\n",
    "        return sample\n",
    "\n",
    "\n",
    "### Define the preprocessing pipeline\n",
    "transform_diat = transforms.Compose([Pitch2Int(),ToTensorLong()])\n",
    "transform_chrom = MultInputCompose([DurationOneHotEncoder(len(midi_to_ix),N_DURATION_CLASSES),ToTensorFloat()])\n",
    "transform_key = transforms.Compose([Ks2Int(),ToTensorLong()])\n",
    "\n",
    "\n",
    "print(set([ks_to_ix[ks] for piece in dict_dataset for ks in piece[\"key_signatures\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jV9boYaToUs2",
    "outputId": "92bcd8d3-87a2-4d2c-8e92-4a106139f59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834 28\n",
      "torch.Size([611, 17])\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "611\n",
      "torch.Size([2776, 17])\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "2776\n",
      "torch.Size([1423, 17])\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "1423\n",
      "torch.Size([804, 17])\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "804\n",
      "torch.Size([847, 17])\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "847\n",
      "torch.Size([3878, 17])\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "3878\n",
      "torch.Size([1827, 17])\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "1827\n",
      "torch.Size([1722, 17])\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "1722\n",
      "torch.Size([518, 17])\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "518\n",
      "torch.Size([1549, 17])\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "1549\n",
      "torch.Size([1535, 17])\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "1535\n",
      "torch.Size([1637, 17])\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1637\n",
      "torch.Size([4254, 17])\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "4254\n",
      "torch.Size([2084, 17])\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "2084\n",
      "torch.Size([1829, 17])\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "1829\n",
      "torch.Size([2566, 17])\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "2566\n",
      "torch.Size([2988, 17])\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "2988\n",
      "torch.Size([5076, 17])\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "5076\n",
      "torch.Size([3430, 17])\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "3430\n",
      "torch.Size([1960, 17])\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "1960\n",
      "torch.Size([5154, 17])\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "5154\n",
      "torch.Size([927, 17])\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "927\n",
      "torch.Size([4074, 17])\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "4074\n",
      "torch.Size([643, 17])\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "643\n",
      "torch.Size([2030, 17])\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "2030\n",
      "torch.Size([1104, 17])\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "1104\n",
      "torch.Size([2242, 17])\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "2242\n",
      "torch.Size([3124, 17])\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "3124\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "\n",
    "class PSDataset(Dataset):\n",
    "    def __init__(self, dict_dataset, paths, transf_c, transf_d, transf_k, augment_dataset, sort =False, truncate = None):\n",
    "        if sort:\n",
    "            dict_dataset = sorted(dict_dataset, key = lambda e: (len(e['midi_number'])),reverse=True)\n",
    "        if not augment_dataset: #remove the transposed pieces\n",
    "            dict_dataset = [e for e in dict_dataset if e[\"transposed_of\"]==\"P1\"]\n",
    "        #consider only pieces in paths\n",
    "        dict_dataset = [e for e in dict_dataset if e[\"original_path\"] in paths]\n",
    "        \n",
    "        #extract the useful data from dataset        \n",
    "        self.chromatic_sequences = [e[\"midi_number\"] for e in dict_dataset]\n",
    "        self.diatonic_sequences = [e[\"pitches\"] for e in dict_dataset]\n",
    "        self.durations = [e[\"duration\"] for e in dict_dataset]\n",
    "        self.ks = [e[\"key_signatures\"] for e in dict_dataset]\n",
    "\n",
    "        #the transformations to apply to data\n",
    "        self.transf_c = transf_c\n",
    "        self.transf_d = transf_d\n",
    "        self.transf_ks = transf_k\n",
    "        self.truncate = truncate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chromatic_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chromatic_seq = self.chromatic_sequences[idx]\n",
    "        diatonic_seq = self.diatonic_sequences[idx]\n",
    "        duration_seq = self.durations[idx]    \n",
    "        ks_seq = self.ks[idx] \n",
    "\n",
    "        #transform\n",
    "        chromatic_seq = self.transf_c(chromatic_seq,duration_seq)\n",
    "        diatonic_seq = self.transf_d(diatonic_seq)\n",
    "        ks_seq = self.transf_ks(ks_seq)\n",
    "\n",
    "        if not self.truncate is None:\n",
    "            if len(diatonic_seq) > self.truncate:\n",
    "                chromatic_seq = chromatic_seq[0:self.truncate]\n",
    "                diatonic_seq = diatonic_seq[0:self.truncate]\n",
    "                ks_seq = ks_seq[0:self.truncate]\n",
    "\n",
    "        #sanity check\n",
    "        assert len(chromatic_seq) == len(diatonic_seq) == len(ks_seq)\n",
    "        seq_len = len(diatonic_seq)\n",
    "        \n",
    "        return chromatic_seq, diatonic_seq, ks_seq, seq_len\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,transform_key,True, sort = True)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat,transform_key, False)\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset))\n",
    "\n",
    "\n",
    "# test if it works\n",
    "for chrom,diat,ks,seq_len in validation_dataset:\n",
    "    print(chrom.shape)\n",
    "#     print(torch.argmax(chrom[0:30],1))\n",
    "#     # print([diatonic_pitches[p.item()] for p in diat[0:30]])\n",
    "#     print([accepted_pitches[p.item()] for p in diat[0:30]])\n",
    "    print([p.item() for p in ks[0:30]])\n",
    "    print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAubyjw2LC8P",
    "outputId": "d6f8574f-8901-41e7-fd7a-9a0c6091eafa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2566, 4, 17]) torch.Size([2566, 4]) torch.Size([2566, 4])\n",
      "tensor([[[ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]],\n",
      "\n",
      "        [[ 1.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]],\n",
      "\n",
      "        [[ 1.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy,zz, l) = zip(*batch)\n",
    "    \n",
    "    xx_pad = pad_sequence(xx, padding_value=midi_to_ix[PAD])\n",
    "    yy_pad = pad_sequence(yy, padding_value=pitch_to_ix[PAD])\n",
    "    zz_pad = pad_sequence(zz, padding_value=ks_to_ix[PAD])\n",
    "\n",
    "    #sort the sequences by length\n",
    "    seq_lengths, perm_idx = torch.Tensor(l).sort(0, descending=True)\n",
    "    xx_pad = xx_pad[:,perm_idx,:]\n",
    "    yy_pad = yy_pad[:,perm_idx]\n",
    "    zz_pad = zz_pad[:,perm_idx]\n",
    "\n",
    "    return xx_pad, yy_pad,zz_pad, seq_lengths\n",
    "\n",
    "data_loader = DataLoader(dataset=validation_dataset, batch_size=4, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "#test if it work\n",
    "for batch in data_loader:\n",
    "    print(batch[0].shape,batch[1].shape,batch[2].shape)\n",
    "    print(batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O1OA1AjGWO-"
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QEkFBY5cUsmN"
   },
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix,n_layers =1):\n",
    "        super(RNNTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim, self.n_out_ks)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss_pitch = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = pitch_to_ix[PAD])\n",
    "        self.loss_ks = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = ks_to_ix[PAD])\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out_pitch = self.top_layer_pitch(rnn_out)\n",
    "        out_ks = self.top_layer_ks(rnn_out)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores_pitch = scores_pitch.view(-1, self.n_out_pitch)\n",
    "        scores_ks = scores_ks.view(-1, self.n_out_ks)\n",
    "        pitches = pitches.view(-1)\n",
    "        keysignatures = keysignatures.view(-1)\n",
    "        return self.loss_pitch(scores_pitch, pitches) + self.loss_ks(scores_ks,keysignatures)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted_pitch = scores_pitch.argmax(dim=2)\n",
    "        predicted_ks = scores_ks.argmax(dim=2)\n",
    "        return [predicted_pitch[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)],[predicted_ks[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMultiTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix,hidden_dim2 = 24, n_layers =1):\n",
    "        super(RNNMultiTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        self.rnn2 = nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim2//2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim2, self.n_out_ks)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss_pitch = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = pitch_to_ix[PAD])\n",
    "        self.loss_ks = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = ks_to_ix[PAD])\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out_pitch = self.top_layer_pitch(rnn_out)\n",
    "        \n",
    "        #pass the ks information into the second rnn\n",
    "        rnn_out = torch.nn.utils.rnn.pack_padded_sequence(rnn_out, sentences_len)\n",
    "        rnn_out, _ = self.rnn2(rnn_out)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        \n",
    "        out_ks = self.top_layer_ks(rnn_out)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores_pitch = scores_pitch.view(-1, self.n_out_pitch)\n",
    "        scores_ks = scores_ks.view(-1, self.n_out_ks)\n",
    "        pitches = pitches.view(-1)\n",
    "        keysignatures = keysignatures.view(-1)\n",
    "        return self.loss_pitch(scores_pitch, pitches) + self.loss_ks(scores_ks,keysignatures)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted_pitch = scores_pitch.argmax(dim=2)\n",
    "        predicted_ks = scores_ks.argmax(dim=2)\n",
    "        return [predicted_pitch[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)],[predicted_ks[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QE7itQ88Qx17"
   },
   "outputs": [],
   "source": [
    "class RNNCRFTagger(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix,n_layers =1):\n",
    "        super(RNNCRFTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim, self.n_out_ks)\n",
    "    \n",
    "        self.crf_pitch = CRF(self.n_out_pitch)\n",
    "        self.crf_ks = CRF(self.n_out_ks)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "\n",
    "        out_pitch = self.top_layer_pitch(rnn_out)\n",
    "        out_ks = self.top_layer_ks(rnn_out)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # Compute the outputs of the lower layers, which will be used as emission\n",
    "        # scores for the CRF.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
    "        # the *negative* log likelihood as the loss value.            \n",
    "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
    "        # log likelihood.\n",
    "        pad_mask = torch.arange(max(sentences_len))[:, None] < sentences_len[None, :]\n",
    "        pad_mask = pad_mask.byte().to(device)\n",
    "        return - self.crf_pitch(scores_pitch, pitches, mask = pad_mask, reduction ='mean' ) - self.crf_ks(scores_ks, keysignatures, mask = pad_mask, reduction ='mean' )\n",
    "            \n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the emission scores, as above.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
    "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
    "        # of shape (n_sentences, max_len).\n",
    "        pad_mask = torch.arange(max(sentences_len))[:, None] < sentences_len[None, :]\n",
    "        pad_mask = pad_mask.byte().to(device)\n",
    "        return self.crf_pitch.decode(scores_pitch,mask = pad_mask), self.crf_ks.decode(scores_ks,mask = pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttentionTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, n_labels,n_layers =1):\n",
    "        super(RNNAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(hidden_dim, self.n_labels)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index = tag_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        # TODO : set right parameters\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        # use attention\n",
    "        attn_applied = self.attention(rnn_out,rnn_out,sentences_len)\n",
    "        \n",
    "        out = self.top_layer(attn_applied) #maybe remove this one?\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "            \n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        return [predicted[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention,self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = torch.nn.Parameter(torch.FloatTensor(\n",
    "            hidden_dim, hidden_dim).uniform_(-0.1, 0.1))\n",
    "\n",
    "    def forward(self,\n",
    "                query, # [seq_len, batch, hidden_dim]\n",
    "                values, # [seq_len, batch, hidden_dim]\n",
    "                sentences_len\n",
    "               ):\n",
    "        weights = self._get_weights(query, values) # [batch,seq_length,hidden_dim]\n",
    "        # mask the weights\n",
    "        inverted_pad_mask = torch.arange(max(sentences_len))[None,:] > sentences_len[:,None]\n",
    "        inverted_pad_mask = (inverted_pad_mask.float()*(-10000)).unsqueeze(1).to(device)\n",
    "#         print(weights.shape,inverted_pad_mask.shape )\n",
    "        #apply the mask\n",
    "        weights = weights - inverted_pad_mask\n",
    "        \n",
    "        weights = torch.nn.functional.softmax(weights, dim=-1)\n",
    "        \n",
    "        out = torch.transpose((weights @ torch.transpose(values,0,1)),0,1)\n",
    "#         print(\"ATT out shape\", out.shape)\n",
    "        return out # [seq_len,batch,encoder_dim]\n",
    "\n",
    "    def _get_weights(self,\n",
    "        query: torch.Tensor,  # [decoder_dim]\n",
    "        values: torch.Tensor, # [seq_length, encoder_dim]\n",
    "    ):\n",
    "        #transpose to batch first to correctly handle batch multiplications\n",
    "#         print(\"shape\",query.shape,self.W.shape, values.shape)\n",
    "        query,values = torch.transpose(query,0,1),torch.transpose(values,0,1)\n",
    "#         print(\"shape\",query.shape,self.W.shape, values.shape)\n",
    "#         print(\"stape values.t\", torch.transpose(values,1,2).shape)\n",
    "        weights = query @ self.W @ torch.transpose(values,1,2)  # [seq_length]\n",
    "#         print(\"out att shape\", weights.shape)\n",
    "        return weights/np.sqrt(self.hidden_dim)  # [seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMultAttentionTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix, sliding_attn_dim = 11,n_layers =1):\n",
    "        super(RNNMultAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim, self.n_out_ks)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss_pitch = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = pitch_to_ix[PAD])\n",
    "        self.loss_ks = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = ks_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        self.sliding_attn_dim = sliding_attn_dim\n",
    "        self.attention = torch.nn.MultiheadAttention(hidden_dim,num_heads= 1)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        # compute padding mask (True when padded, ignore True)\n",
    "        inverted_pad_mask = torch.arange(max(sentences_len))[None,:] > sentences_len[:,None]\n",
    "        \n",
    "        # use attention\n",
    "        attn_mask = torch.tensor(sliding_attn_mask(len(rnn_out),self.sliding_attn_dim),dtype = torch.bool).to(device)\n",
    "        attn_applied, _ = self.attention(rnn_out,rnn_out,rnn_out,key_padding_mask = inverted_pad_mask.to(device),attn_mask = attn_mask)\n",
    "        \n",
    "        out_pitch = self.top_layer_pitch(attn_applied)\n",
    "        out_ks = self.top_layer_ks(attn_applied)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "         # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores_pitch = scores_pitch.view(-1, self.n_out_pitch)\n",
    "        scores_ks = scores_ks.view(-1, self.n_out_ks)\n",
    "        pitches = pitches.view(-1)\n",
    "        keysignatures = keysignatures.view(-1)\n",
    "        return self.loss_pitch(scores_pitch, pitches) + self.loss_ks(scores_ks,keysignatures)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted_pitch = scores_pitch.argmax(dim=2)\n",
    "        predicted_ks = scores_ks.argmax(dim=2)\n",
    "        return [predicted_pitch[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)],[predicted_ks[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 ns ± 3.16 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def sliding_attn_mask(S:int,window_width:int):\n",
    "    # (N∗num_heads,L,S) where N is the batch size, L is the target sequence length, S is the source sequence length\n",
    "    #  If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged\n",
    "    out = np.ones((S,S),dtype = np.int8)\n",
    "    #fill the exception at the end and beginning of the vectors\n",
    "    for i in range(window_width-1):\n",
    "        out[i,:window_width] = 0\n",
    "        out[S-i-1,-window_width:] = 0\n",
    "    for i1 in range(window_width-1,S-window_width+1):\n",
    "        for i2 in range(S):\n",
    "            if i2 in range(i1-(window_width-1)//2,i1+(window_width-1)//2+1):\n",
    "                out[i1,i2] = 0\n",
    "        \n",
    "    return out\n",
    "\n",
    "def sparse_sliding_attn_mask(S:int,window_width:int):\n",
    "    # (N∗num_heads,L,S) where N is the batch size, L is the target sequence length, S is the source sequence length\n",
    "    #  If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged\n",
    "    sparse_indices = torch.zeros([2, window_width * S], dtype=torch.long) \n",
    "    \n",
    "    i = 0\n",
    "    #fill the exception at the end and beginning of the vectors\n",
    "    for iw1 in range(window_width-1):\n",
    "        for iw2 in range(window_width):\n",
    "            sparse_indices[0,i] = iw1\n",
    "            sparse_indices[1,i] = iw2\n",
    "            i +=1\n",
    "            sparse_indices[0,i] = S-iw1-1\n",
    "            sparse_indices[1,i] = S-iw2 -1\n",
    "            i +=1\n",
    "    #fill for the elements in the middle\n",
    "    for i1 in range(window_width-1,S-window_width+1):\n",
    "        for i2 in range(S):\n",
    "            if i2 in range(i1-(window_width-1)//2,i1+(window_width-1)//2+1):\n",
    "                sparse_indices[0,i] = i1\n",
    "                sparse_indices[1,i] = i2\n",
    "                i+=1 \n",
    "    v = torch.ones(window_width * S, dtype=torch.long)\n",
    "\n",
    "    return torch.sparse.FloatTensor(sparse_indices, v, torch.Size([S,S]))\n",
    "\n",
    "%timeit sliding_attn_mask(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NystromAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = config[\"head_dim\"]\n",
    "        self.num_head = config[\"num_head\"]\n",
    "\n",
    "        self.num_landmarks = config[\"num_landmarks\"]\n",
    "        self.seq_len = config[\"seq_len\"]\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        Q = Q * mask[:, None, :, None] / np.sqrt(np.sqrt(self.head_dim))\n",
    "        K = K * mask[:, None, :, None] / np.sqrt(np.sqrt(self.head_dim))\n",
    "#         Q = Q / np.sqrt(np.sqrt(self.head_dim))\n",
    "#         K = K / np.sqrt(np.sqrt(self.head_dim))\n",
    "\n",
    "        if self.num_landmarks == self.seq_len:\n",
    "            attn = torch.nn.functional.softmax(torch.matmul(Q, K.transpose(-1, -2)) - 1e9 * (1 - mask[:, None, None, :]), dim = -1)\n",
    "            X = torch.matmul(attn, V)\n",
    "        else:\n",
    "            Q_landmarks = Q.reshape(-1, self.num_head, self.num_landmarks, self.seq_len // self.num_landmarks, self.head_dim).mean(dim = -2)\n",
    "            K_landmarks = K.reshape(-1, self.num_head, self.num_landmarks, self.seq_len // self.num_landmarks, self.head_dim).mean(dim = -2)\n",
    "\n",
    "            kernel_1 = torch.nn.functional.softmax(torch.matmul(Q, K_landmarks.transpose(-1, -2)), dim = -1)\n",
    "            kernel_2 = torch.nn.functional.softmax(torch.matmul(Q_landmarks, K_landmarks.transpose(-1, -2)), dim = -1)\n",
    "            kernel_3 = torch.nn.functional.softmax(torch.matmul(Q_landmarks, K.transpose(-1, -2)) - 1e9 * (1 - mask[:, None, None, :]), dim = -1)\n",
    "            X = torch.matmul(torch.matmul(kernel_1, self.iterative_inv(kernel_2)), torch.matmul(kernel_3, V))\n",
    "\n",
    "        return X\n",
    "\n",
    "    def iterative_inv(self, mat, n_iter = 6):\n",
    "        I = torch.eye(mat.size(-1), device = mat.device)\n",
    "        K = mat\n",
    "        V = 1 / (torch.max(torch.sum(torch.abs(K), dim = -2)) * torch.max(torch.sum(torch.abs(K), dim = -1))) * K.transpose(-1, -2)\n",
    "        for _ in range(n_iter):\n",
    "            KV = torch.matmul(K, V)\n",
    "            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n",
    "        return V\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'num_landmarks={self.num_landmarks}, seq_len={self.seq_len}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNystromAttentionTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix, n_layers =1):\n",
    "        super(RNNNystromAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim, self.n_out_ks)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss_pitch = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = pitch_to_ix[PAD])\n",
    "        self.loss_ks = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = ks_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        config = {}\n",
    "        config[\"head_dim\"] = self.hidden_dim\n",
    "        config[\"num_head\"] = 1\n",
    "        config[\"num_landmarks\"] = 64\n",
    "        config[\"seq_len\"] = 512\n",
    "        self.attention = NystromAttention(config)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        # compute padding mask (True when padded, ignore True)\n",
    "        inverted_pad_mask = torch.arange(max(sentences_len))[None,:] > sentences_len[:,None]\n",
    "        pad_mask = torch.arange(max(sentences_len))[None,:] < sentences_len[:,None]\n",
    "        \n",
    "        # use attention\n",
    "        attn_applied, _ = self.attention(rnn_out,rnn_out,rnn_out,pad_mask.byte())\n",
    "        \n",
    "        out_pitch = self.top_layer_pitch(attn_applied)\n",
    "        out_ks = self.top_layer_ks(attn_applied)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "         # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores_pitch = scores_pitch.view(-1, self.n_out_pitch)\n",
    "        scores_ks = scores_ks.view(-1, self.n_out_ks)\n",
    "        pitches = pitches.view(-1)\n",
    "        keysignatures = keysignatures.view(-1)\n",
    "        return self.loss_pitch(scores_pitch, pitches) + self.loss_ks(scores_ks,keysignatures)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted_pitch = scores_pitch.argmax(dim=2)\n",
    "        predicted_ks = scores_ks.argmax(dim=2)\n",
    "        return [predicted_pitch[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)],[predicted_ks[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXixmVQfvw8T"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "3m_nj4HCuCe1"
   },
   "outputs": [],
   "source": [
    "# TODO: search over the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "uAMSIlw0AJb6"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_dataloader, val_dataloader, n_epochs):\n",
    "    history = defaultdict(list)  \n",
    "    for i_epoch in range(1,n_epochs +1):\n",
    "        t0 = time.time()\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        model.train()\n",
    "        for seqs, pitches,keysignatures, lens in train_dataloader: #seqs, pitches, keysignatures, lens are batches\n",
    "            seqs, pitches,keysignatures  = seqs.to(device), pitches.to(device),keysignatures.to(device)\n",
    "            optimizer.zero_grad()\n",
    "#             print(\"input seq shape:\",seqs.shape)\n",
    "\n",
    "#             loss = model(seqs,targets,lens) / sum(lens) #normalize for the number of symbol considered (without padding)\n",
    "            loss = model(seqs,pitches,keysignatures,lens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            predicted_pitch,predicted_ks = model.predict(seqs,lens)\n",
    "            for i,p in enumerate(predicted_pitch):\n",
    "                acc= accuracy_score(p,pitches[:,i][:len(p)].cpu()) #compute the accuracy without considering the padding\n",
    "                accuracy_sum += acc/len(lens) #normalize according to the number of sequences in the batch\n",
    "\n",
    "        train_loss = loss_sum/len(train_dataloader)\n",
    "        train_accuracy = accuracy_sum/len(train_dataloader) #normalize according to the number of batches\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        all_predicted_pitch = []\n",
    "        all_predicted_ks = []\n",
    "        all_pitches = []\n",
    "        all_ks = []\n",
    "        with torch.no_grad():\n",
    "            for seqs,pitches,keysignatures, lens in val_dataloader:\n",
    "                # Predict the model's output on a batch\n",
    "                predicted_pitch,predicted_ks = model.predict(seqs.to(device),lens)                   \n",
    "                # Update the lists that will be used to compute the accuracy\n",
    "                for i,(p,k) in enumerate(zip(predicted_pitch,predicted_ks)):\n",
    "                    all_predicted_pitch.append(torch.Tensor(p))\n",
    "                    all_predicted_ks.append(torch.Tensor(k))\n",
    "                    all_pitches.append(pitches[0:int(lens[i]),i])\n",
    "                    all_ks.append(keysignatures[0:int(lens[i]),i])\n",
    "                \n",
    "        # Compute the overall accuracy for the validation set\n",
    "        val_accuracy_pitch = accuracy_score(torch.cat(all_predicted_pitch),torch.cat(all_pitches))\n",
    "        val_accuracy_ks = accuracy_score(torch.cat(all_predicted_ks),torch.cat(all_ks))\n",
    "        history[\"val_accuracy_pitch\"].append(val_accuracy_pitch)\n",
    "        history[\"val_accuracy_ks\"].append(val_accuracy_ks)\n",
    "\n",
    "#         save the model\n",
    "        torch.save(model, \"./models/temp/model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "#         files.download(\"model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "\n",
    "    \n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {i_epoch}: train loss = {train_loss:.4f}, train_accuracy: {train_accuracy:.4f},val_accuracy_pitch: {val_accuracy_pitch:.4f},val_accuracy_ks: {val_accuracy_ks:.4f}, time = {t1-t0:.4f}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del train_dataset\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(midi_to_ix)+N_DURATION_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "2g2st8znpGW_",
    "outputId": "fe232e42-e270-4f5d-9bd8-718bba5e7151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n",
      "Shapes: torch.Size([512, 2, 96]) torch.Size([512, 2, 96]) torch.Size([512, 2, 96]) torch.Size([2, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-7478a86ae8f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_WEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMOMENTUM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWEIGHT_DECAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# After the final evaluation, we print more detailed evaluation statistics,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-03260c3a03ec>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#             loss = model(seqs,targets,lens) / sum(lens) #normalize for the number of symbol considered (without padding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpitches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeysignatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-032974195c51>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, pitches, keysignatures, sentences_len)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m          \u001b[0;31m# Compute the outputs. The shape is (max_len, n_sentences, n_labels).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mscores_pitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_ks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Flatten the outputs and the gold-standard labels, to compute the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-032974195c51>\u001b[0m in \u001b[0;36mcompute_outputs\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# use attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mattn_applied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mout_pitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_layer_pitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-8997f36c0d4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         Q = Q / np.sqrt(np.sqrt(self.head_dim))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "n_epochs = 30\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.05\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 2\n",
    "MOMENTUM = 0.9\n",
    "RNN_LAYERS = 1\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,transform_key,True,sort=True, truncate = 512)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat,transform_key, False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False,collate_fn=pad_collate)\n",
    "\n",
    "# model = torch.load(\"./models/temp/model_temp_epoch30-to_restart.pkl\")\n",
    "# model = RNNTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "# model = RNNMultiTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "# model = RNNCRFTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "# model = RNNAttentionTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,len(tag_to_ix), n_layers =RNN_LAYERS)\n",
    "# model = RNNMultAttentionTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, sliding_attn_dim =11, n_layers =RNN_LAYERS)\n",
    "model = RNNNystromAttentionTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_WEIGHT, momentum = MOMENTUM,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = training_loop(model,optimizer,train_dataloader,val_dataloader, n_epochs)\n",
    "\n",
    "# After the final evaluation, we print more detailed evaluation statistics,\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy_pitch'])\n",
    "plt.plot(history['val_accuracy_ks'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy_pitch','validation_accuracy_ks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best working model on the accuracy\n",
    "max_accuracy = np.max(history['val_accuracy_pitch'])\n",
    "best_epoch = np.argmax(history['val_accuracy_pitch'])\n",
    "print(\"Best validation accuracy: \",max_accuracy, \"at epoch\",best_epoch)\n",
    "\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy_pitch'])\n",
    "plt.plot(history['val_accuracy_ks'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy_pitch','validation_accuracy_ks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tTv2BrGbvSHh",
    "outputId": "801ff32d-24dc-4434-a436-17d9ca832214"
   },
   "outputs": [],
   "source": [
    "# torch.save(model, \"./models/model_asap_crf200dur.pkl\")\n",
    "# files.download(\"model_asap_crf300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXcmLpCKt28l"
   },
   "source": [
    "## Test on Mdata dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./models/temp/model_temp_epoch9.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "BacUqgD5usGL"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open(Path(basepath,'./datasets/musedata_noisy.pkl'), 'rb') as fid:\n",
    "     full_mdata_dict_dataset = pickle.load( fid)\n",
    "        \n",
    "# add dummy ks to have the same format as asap\n",
    "for e in full_mdata_dict_dataset:\n",
    "    e[\"key_signatures\"] = np.zeros(len(e[\"pitches\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgaUwW2fuwg0",
    "outputId": "89fc5f7d-884a-4ee6-fc34-b3d512f0c4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 different pieces\n",
      "Average number of notes:  907.2777777777778\n"
     ]
    }
   ],
   "source": [
    "mdata_paths = list(set([e[\"original_path\"] for e in full_mdata_dict_dataset ]))\n",
    "\n",
    "# # remove the symbphony No.100 from Haydn because of the enharmonic transposition\n",
    "# paths.remove(\"datasets\\\\opnd\\\\haydndoversyms-10004m.opnd-m\")\n",
    "\n",
    "# print(paths)\n",
    "print(len(mdata_paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_mdata_dict_dataset ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "f-Fg6gJKvNLt"
   },
   "outputs": [],
   "source": [
    "mdata_dataset = PSDataset(full_mdata_dict_dataset,mdata_paths, transform_chrom,transform_diat,transform_key,sort=False, augment_dataset=False)\n",
    "mdata_dataloader = DataLoader(mdata_dataset,  batch_size=2, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ml905Mtdvj-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "all_inputs = []\n",
    "all_predicted_pitch = []\n",
    "all_predicted_ks = []\n",
    "all_pitches = []\n",
    "all_ks = []\n",
    "model.eval() # Evaluation mode (e.g. disable dropout)\n",
    "with torch.no_grad(): # Disable gradient tracking\n",
    "    for seqs, pitches,ks,lens in mdata_dataloader:\n",
    "        # Move data to device\n",
    "        seqs = seqs.to(device)\n",
    "\n",
    "        # Predict the model's output on a batch.\n",
    "        predicted_pitch,predicted_ks = model.predict(seqs,lens)                   \n",
    "        # Update the evaluation statistics.\n",
    "        for i,p in enumerate(predicted_pitch):\n",
    "            all_inputs.append(torch.argmax(seqs[0:int(lens[i]),i,:].cpu(),1).numpy())\n",
    "            all_predicted_pitch.append(p)\n",
    "            all_predicted_ks.append(predicted_ks[i])\n",
    "            all_pitches.append(pitches[0:int(lens[i]),i])\n",
    "            all_ks.append(ks[0:int(lens[i]),i])\n",
    "# model.eval()\n",
    "# all_input = []\n",
    "# all_predicted_pitch = []\n",
    "# all_predicted_ks = []\n",
    "# all_pitches = []\n",
    "# all_ks = []\n",
    "# with torch.no_grad():\n",
    "#     for seqs,pitches,keysignatures, lens in mdata_dataloader:\n",
    "#         # Predict the model's output on a batch\n",
    "#         predicted_pitch,predicted_ks = model.predict(seqs.to(device),lens)                   \n",
    "#         # Update the lists that will be used to compute the accuracy\n",
    "#         for i,(p,k) in enumerate(zip(predicted_pitch,predicted_ks)):\n",
    "#             all_inputs.append(torch.argmax(seqs[0:int(lens[i]),i,:],1).numpy())\n",
    "#             all_predicted_pitch.append(torch.Tensor(p))\n",
    "#             all_predicted_ks.append(torch.Tensor(k))\n",
    "#             all_pitches.append(pitches[0:int(lens[i]),i])\n",
    "#             all_ks.append(keysignatures[0:int(lens[i]),i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsefdSHxvrWy",
    "outputId": "33785b90-5e03-4a6d-86dd-92b6b763a505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cor', 'viv', 'moz', 'han', 'tel', 'bee', 'bac', 'hay']\n"
     ]
    }
   ],
   "source": [
    "# # Divide accuracy according to author\n",
    "authors = []\n",
    "\n",
    "for sequence in all_inputs:\n",
    "#     print(sequence)\n",
    "    author = [e[\"original_path\"].split(\"\\\\\")[-1][:3] for e in full_mdata_dict_dataset\n",
    "              if len(e[\"midi_number\"]) == len(sequence) and\n",
    "              list(e[\"midi_number\"]) ==list(sequence) ]\n",
    "    assert len(author) == 1\n",
    "    authors.append(author[0])\n",
    "\n",
    "considered_authors = list(set(authors))\n",
    "print(considered_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgNt_yG7vrfd",
    "outputId": "8473393f-dac6-4930-9d9e-da9c1046f1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch Statistics----------------\n",
      "{'cor': 4, 'viv': 28, 'moz': 75, 'han': 8, 'tel': 15, 'bee': 135, 'bac': 26, 'hay': 275}\n",
      "{'cor': 0.9998366880333156, 'viv': 0.998857002898314, 'moz': 0.996938025638932, 'han': 0.9996734693877551, 'tel': 0.9993877551020408, 'bee': 0.9944882211244029, 'bac': 0.9989389920424403, 'hay': 0.9887709269089424}\n",
      "{'cor': 24493, 'viv': 24497, 'moz': 24494, 'han': 24500, 'tel': 24500, 'bee': 24493, 'bac': 24505, 'hay': 24490}\n",
      "Total errors : 566\n"
     ]
    }
   ],
   "source": [
    "errors_per_author_pitch = {}\n",
    "accuracy_per_author_pitch = {}\n",
    "notes_per_author = {}\n",
    "for ca in considered_authors:\n",
    "    ca_predicted_pitch = np.concatenate([all_predicted_pitch[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_predicted_ks = np.concatenate([all_predicted_ks[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_pitches = np.concatenate([all_pitches[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_ks = np.concatenate([all_ks[i] for i,a in enumerate(authors) if a == ca])\n",
    "\n",
    "    ca_acc_pitch = accuracy_score(ca_predicted_pitch,ca_pitches)\n",
    "    \n",
    "    accuracy_per_author_pitch[ca] = float(ca_acc_pitch)\n",
    "    errors_per_author_pitch[ca] = int(len(ca_pitches) - sum(np.equal(ca_predicted_pitch,ca_pitches)))\n",
    "    notes_per_author[ca] = len(ca_pitches)\n",
    "\n",
    "print(\"Pitch Statistics----------------\")\n",
    "print(errors_per_author_pitch)\n",
    "print(accuracy_per_author_pitch)\n",
    "print(notes_per_author)\n",
    "print(\"Total errors :\", sum([e for e in errors_per_author_pitch.values()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5s0d56evrje"
   },
   "source": [
    "### Best accuracy for now\n",
    "for now best accuracy is with  no CRF (but considering durations) n_epochs = 20\n",
    "n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.05\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "duration_delimiter = automatic calculated\n",
    "\n",
    "Model available in: \"model_RNNTagger9736.pkl\"\n",
    "\n",
    "Epoch 16: train loss = 0.2798, train_accuracy: 0.9245,val_accuracy: 0.9736, time = 78.1199\n",
    "Trained on all dataset\n",
    "\n",
    "\n",
    "{'moz': 65, 'bac': 40, 'bee': 88, 'cor': 6, 'han': 8, 'viv': 63, 'tel': 21, 'hay': 262}\n",
    "{'moz': 0.9973462888870744, 'bac': 0.9983676800652928, 'bee': 0.9964071367329441, 'cor': 0.9997550320499735, 'han': 0.9996734693877551, 'viv': 0.9974282565212067, 'tel': 0.9991428571428571, 'hay': 0.9893017558187015}\n",
    "{'moz': 24494, 'bac': 24505, 'bee': 24493, 'cor': 24493, 'han': 24500, 'viv': 24497, 'tel': 24500, 'hay': 24490}\n",
    "Total errors : 553\n",
    "\n",
    "This win by far against ps13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVvP1eH8vrl3"
   },
   "source": [
    "### Best accuracy with CRF\n",
    "n_epochs = 20\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "model = RNNCRFTagger(len(midi_to_ix)+len(duration_delimiter)+2,HIDDEN_DIM,len(tag_to_ix), n_layers =1)\n",
    "\n",
    "Model available in: \"\"./models/model_temp_CRFacc9548.pkl\"\"\n",
    "accuracy on validation set 0.9548586557910835\n",
    "Trained on all asap dataset\n",
    "\n",
    "{'viv': 36, 'tel': 41, 'bee': 185, 'bac': 101, 'han': 44, 'cor': 14, 'moz': 131, 'hay': 376}\n",
    "{'viv': 0.9985304322978323, 'tel': 0.9983265306122449, 'bee': 0.9924468215408484, 'bac': 0.9958783921648643, 'han': 0.9982040816326531, 'cor': 0.9994284081166047, 'moz': 0.9946517514493345, 'hay': 0.9846467946100449}\n",
    "{'viv': 24497, 'tel': 24500, 'bee': 24493, 'bac': 24505, 'han': 24500, 'cor': 24493, 'moz': 24494, 'hay': 24490}\n",
    "Total errors : 928\n",
    "\n",
    "Still win against ps13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEst accuracy with ks\n",
    "n_epochs = 30\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.05\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "MOMENTUM = 0.9\n",
    "RNN_LAYERS = 1\n",
    "\n",
    "model = RNNMultiTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "\n",
    "Model available in: \"\"./models/model_RNNks.pkl\"\"\n",
    "accuracy on validation set 0.9424\n",
    "Trained on all asap dataset\n",
    "\n",
    "{'cor': 4, 'viv': 29, 'moz': 70, 'bac': 13, 'han': 15, 'bee': 99, 'hay': 273, 'tel': 8}\n",
    "{'cor': 0.9998366880333156, 'viv': 0.9988161815732539, 'moz': 0.9971421572630031, 'bac': 0.9994694960212201, 'han': 0.9993877551020408, 'bee': 0.9959580288245621, 'hay': 0.9888525928950592, 'tel': 0.9996734693877551}\n",
    "{'cor': 24493, 'viv': 24497, 'moz': 24494, 'bac': 24505, 'han': 24500, 'bee': 24493, 'hay': 24490, 'tel': 24500}\n",
    "Total errors : 511\n",
    "\n",
    "\n",
    "Epoch 22: train loss = 0.4723, train_accuracy: 0.9533,val_accuracy_pitch: 0.9424,val_accuracy_ks: 0.7938, time = 106.7093\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTDh55Hh2OWCo66dGc+SES",
   "include_colab_link": true,
   "name": "rnncrf_pitch_spelling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
