{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fosfrancesco/pitch-spelling/blob/main/rnncrf_pitch_spelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fifMg_hxNSOg",
    "outputId": "a8b320f9-4231-40d9-8f10-a7b8acd1da37"
   },
   "outputs": [],
   "source": [
    "#! pip install --upgrade pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hsciybUBNkur"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "import music21 as m21\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn\n",
    "import music21 as m21\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "#from torchcrf import CRF\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, notebook\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "import kmeans1d\n",
    "import jenkspy\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from numba import njit\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Naq6UG5jRbyK"
   },
   "source": [
    "# Pitch Spelling and ks Prediction\n",
    "\n",
    "Dataset: different authors from ASAP collection\n",
    "Challenges:\n",
    "- extremely long sequences\n",
    "- small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mHJvrZ2Wo_e",
    "outputId": "1319f78b-cfb1-4e2a-937c-c47cf952757c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 'C'), (1, 'B#'), (2, 'D--'), (3, 'C#'), (4, 'B##'), (5, 'D-'), (6, 'D'), (7, 'C##'), (8, 'E--'), (9, 'D#'), (10, 'E-'), (11, 'F--'), (12, 'E'), (13, 'D##'), (14, 'F-'), (15, 'F'), (16, 'E#'), (17, 'G--'), (18, 'F#'), (19, 'E##'), (20, 'G-'), (21, 'G'), (22, 'F##'), (23, 'A--'), (24, 'G#'), (25, 'A-'), (26, 'A'), (27, 'G##'), (28, 'B--'), (29, 'A#'), (30, 'B-'), (31, 'C--'), (32, 'B'), (33, 'A##'), (34, 'C-')]\n['D--', 'B##', 'C##', 'E--', 'F--', 'D##', 'G--', 'E##', 'F##', 'A--', 'G##', 'B--', 'C--', 'A##']\n[(0, 'P1'), (1, 'd2'), (2, 'A7'), (3, 'm2'), (4, 'A1'), (5, 'M2'), (6, 'd3'), (7, 'AA1'), (8, 'm3'), (9, 'A2'), (10, 'M3'), (11, 'd4'), (12, 'AA2'), (13, 'P4'), (14, 'A3'), (15, 'd5'), (16, 'A4'), (17, 'P5'), (18, 'd6'), (19, 'AA4'), (20, 'm6'), (21, 'A5'), (22, 'M6'), (23, 'd7'), (24, 'AA5'), (25, 'm7'), (26, 'A6'), (27, 'M7'), (28, 'd1'), (29, 'AA6')]\n[-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "pitches_dict = {\n",
    "    0 : [\"C\",\"B#\",\"D--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    1 : [\"C#\",\"B##\",\"D-\"], # nn.Linear(input_size+context_size, 2)\n",
    "    2 : [\"D\",\"C##\",\"E--\"], # nn.Linear(input_size+context_size, 3)\n",
    "    3 : [\"D#\",\"E-\",\"F--\"],\n",
    "    4 : [\"E\",\"D##\",\"F-\"],\n",
    "    5 : [\"F\",\"E#\",\"G--\"],\n",
    "    6 : [\"F#\",\"E##\",\"G-\"],\n",
    "    7 : [\"G\",\"F##\",\"A--\"],\n",
    "    8 : [\"G#\",\"A-\"],\n",
    "    9 : [\"A\",\"G##\",\"B--\"],\n",
    "    10 : [\"A#\",\"B-\",\"C--\"],\n",
    "    11 : [\"B\",\"A##\",\"C-\"]\n",
    "}\n",
    "\n",
    "accepted_pitches = [ii for i in pitches_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_pitches)])\n",
    "\n",
    "double_acc_pitches = [ii for i in pitches_dict.values() for ii in i if ii.endswith(\"##\") or  ii.endswith(\"--\") ]\n",
    "print(double_acc_pitches)\n",
    "\n",
    "def score2midi_numbers(score):\n",
    "    return [p.midi%12 for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "def score2pitches(score):\n",
    "    return [p.name for n in score.flat.notes for p in n.pitches]\n",
    "\n",
    "interval_dict = {\n",
    "    0 : [\"P1\",\"d2\",\"A7\"], \n",
    "    1 : [\"m2\",\"A1\"], \n",
    "    2 : [\"M2\",\"d3\",\"AA1\"], \n",
    "    3 : [\"m3\",\"A2\"],\n",
    "    4 : [\"M3\",\"d4\",\"AA2\"],\n",
    "    5 : [\"P4\",\"A3\"],\n",
    "    6 : [\"d5\",\"A4\"],\n",
    "    7 : [\"P5\",\"d6\",\"AA4\"],\n",
    "    8 : [\"m6\",\"A5\"],\n",
    "    9 : [\"M6\",\"d7\",\"AA5\"],\n",
    "    10 : [\"m7\",\"A6\"],\n",
    "    11 : [\"M7\",\"d1\",\"AA6\"]\n",
    "}\n",
    "\n",
    "accepted_intervals = [ii for i in interval_dict.values() for ii in i]\n",
    "print([e for e in enumerate(accepted_intervals)])\n",
    "\n",
    "def transp_score(score):\n",
    "    \"\"\" For each input return len(accepted_intervals) transposed scores\"\"\"\n",
    "    return [score.transpose(interval) for interval in accepted_intervals]\n",
    "\n",
    "def smart_transp_score(score):\n",
    "    \"\"\" For each chromatic interval chose the interval that lead to the smallest number of accidentals\"\"\"\n",
    "    scores = []\n",
    "    for chromatic_int in interval_dict.keys():\n",
    "        temp_scores = []\n",
    "        temp_acc_number = []\n",
    "        for diat_interval in interval_dict[chromatic_int]:\n",
    "            new_score = score.transpose(diat_interval)\n",
    "            temp_scores.append(new_score)\n",
    "            temp_acc_number.append(sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in score2pitches(new_score)]))\n",
    "            # print(\"choice:\", [note.name for note in temp_scores[-1].flat.notes][0:10],\"acc:\",temp_acc_number[-1] )\n",
    "        #keep only the one with the lowest number of accidentals\n",
    "        min_index = np.argmin(temp_acc_number)\n",
    "        # print(\"preferred the number\", min_index)\n",
    "        scores.append(temp_scores[min_index])\n",
    "    return scores\n",
    "\n",
    "def acc_simple_enough(score,accepted_ratio = 0.2 ):\n",
    "    pitches = score2pitches(score)\n",
    "    double_acc = sum(el in double_acc_pitches for el in pitches)\n",
    "    if double_acc/len(pitches) < accepted_ratio:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "diatonic_pitches = [\"C\",\"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n",
    "\n",
    "accepted_ks = list(range(-7,8))\n",
    "\n",
    "print(accepted_ks)\n",
    "\n",
    "# #test acc_simple_enough()\n",
    "# score = m21.converter.parse(paths[356])\n",
    "# scores = smart_transp_score(score)\n",
    "# #delete the pieces with non accepted pitches (e.g. triple sharps)\n",
    "# scores = [s for s in scores if all(pitch in accepted_pitches for pitch in score2pitches(s))]\n",
    "# for s in scores:\n",
    "#     print(s.parts[0].flat.getElementsByClass(m21.key.KeySignature)[0], \"simple enough:\", acc_simple_enough(s))\n",
    "#     print([n.name for n in s.flat.notes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw-SuL4ZB_2C"
   },
   "source": [
    "## Import ASAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrZ0BwLEj2Gp",
    "outputId": "6b521f78-28db-4d4f-f4ef-9345c5d91db9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/fosfrancesco/pitch-spelling.git\n",
    "\n",
    "basepath = \"./\" #to change if running locally or on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WaINjDS2kpxE"
   },
   "outputs": [],
   "source": [
    "# load the asap datasets with ks\n",
    "with open(Path('./asapks.pkl'), 'rb') as fid:\n",
    "     full_dict_dataset = pickle.load( fid)\n",
    "\n",
    "        \n",
    "######## Note for Nicolas: I called it \"dict_dataset\", but it is a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYQdkJAiTS6_",
    "outputId": "72835948-2884-40bb-e5b4-6ebbc8488895"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "222 different pieces\nAverage number of notes:  2410.253424657534\n"
     ]
    }
   ],
   "source": [
    "paths = list(set([e[\"original_path\"] for e in full_dict_dataset ]))\n",
    "\n",
    "# print(paths)\n",
    "print(len(paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_dict_dataset ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMdKkzNnCTrK"
   },
   "source": [
    "## Chose the convenient data augmentation\n",
    "For each chromatic interval, take only the diatonic transposition that produce the smallest number of accidentals (or the original if present).\n",
    "\n",
    "Then remove the pieces with ks that have more than 7 sharps or 7 flats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oign7EZ9BSX4",
    "outputId": "9aa3f694-289a-43fd-fd14-9cad3b8dd3b7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No options for Liszt/Transcendental_Etudes/10/xml_score.musicxml . Chromatic:  1\n",
      "No options for Liszt/Transcendental_Etudes/10/xml_score.musicxml . Chromatic:  6\n",
      "No options for Debussy/Images_Book_1/1_Reflets_dans_lEau/xml_score.musicxml . Chromatic:  3\n",
      "No options for Debussy/Images_Book_1/1_Reflets_dans_lEau/xml_score.musicxml . Chromatic:  8\n",
      "No options for Ravel/Gaspard_de_la_Nuit/1_Ondine/xml_score.musicxml . Chromatic:  4\n",
      "No options for Ravel/Miroirs/4_Alborada_del_gracioso/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Sonata_2/2nd/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Transcendental_Etudes/4/xml_score.musicxml . Chromatic:  2\n",
      "No options for Liszt/Transcendental_Etudes/4/xml_score.musicxml . Chromatic:  4\n",
      "No options for Liszt/Transcendental_Etudes/4/xml_score.musicxml . Chromatic:  9\n",
      "No options for Chopin/Sonata_2/1st_no_repeat/xml_score.musicxml . Chromatic:  1\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  1\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  4\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  6\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  9\n",
      "No options for Schubert/Wanderer_fantasie/xml_score.musicxml . Chromatic:  11\n",
      "No options for Scriabin/Sonatas/5/xml_score.musicxml . Chromatic:  1\n",
      "No options for Scriabin/Sonatas/5/xml_score.musicxml . Chromatic:  6\n",
      "No options for Scriabin/Sonatas/5/xml_score.musicxml . Chromatic:  11\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  1\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  4\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  6\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  9\n",
      "No options for Balakirev/Islamey/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Ballades/4/xml_score.musicxml . Chromatic:  6\n",
      "No options for Liszt/Mephisto_Waltz/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Transcendental_Etudes/11/xml_score.musicxml . Chromatic:  1\n",
      "No options for Liszt/Transcendental_Etudes/11/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_10/4/xml_score.musicxml . Chromatic:  9\n",
      "No options for Liszt/Sonata/xml_score.musicxml . Chromatic:  4\n",
      "No options for Liszt/Sonata/xml_score.musicxml . Chromatic:  6\n",
      "No options for Liszt/Sonata/xml_score.musicxml . Chromatic:  11\n",
      "No options for Liszt/Transcendental_Etudes/9/xml_score.musicxml . Chromatic:  6\n",
      "No options for Liszt/Transcendental_Etudes/9/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  1\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  3\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  6\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  8\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  10\n",
      "No options for Chopin/Sonata_3/4th/xml_score.musicxml . Chromatic:  11\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  4\n",
      "No options for Chopin/Etudes_op_25/10/xml_score.musicxml . Chromatic:  9\n",
      "Before removing according to ks: 2618\n",
      "After removing according to ks: 2406\n"
     ]
    }
   ],
   "source": [
    "# choose only one enharmonic version for each chromatic interval for each piece\n",
    "dict_dataset = []\n",
    "for path in paths:\n",
    "    for c in range(12):\n",
    "        pieces_to_consider = [opus for opus in full_dict_dataset \n",
    "                              if (opus[\"original_path\"] == path and opus[\"transposed_of\"] in interval_dict[c])  ]\n",
    "        # if the original is in pieces_to_consider, go with the original\n",
    "        originals = [opus for opus in pieces_to_consider if opus[\"transposed_of\"] == \"P1\"]\n",
    "        if len(originals) == 1:\n",
    "            dict_dataset.append(originals[0])\n",
    "        else: #we go with the accidental minization criteria\n",
    "            n_accidentals = [sum([pitch.count(\"#\") + pitch.count(\"-\") for pitch in opus[\"pitches\"]]) \n",
    "                            for opus in pieces_to_consider]\n",
    "            if len(pieces_to_consider)>0:\n",
    "                dict_dataset.append(pieces_to_consider[np.argmin(n_accidentals)])\n",
    "            else:\n",
    "                print(\"No options for\", path, \". Chromatic: \",c )\n",
    "                \n",
    "#also remove unaccepted ks\n",
    "print(\"Before removing according to ks:\", len(dict_dataset))\n",
    "dict_dataset = [e for e in dict_dataset if all([k in accepted_ks for k in e[\"key_signatures\"]])]\n",
    "print(\"After removing according to ks:\", len(dict_dataset))\n",
    "                                                    \n",
    "\n",
    "# #test if it worked\n",
    "# for i,e in enumerate(dict_dataset):\n",
    "#     print(e[\"original_path\"], e[\"transposed_of\"], e[\"key_signatures\"])\n",
    "#     print(e[\"pitches\"][:10])\n",
    "#     print(e[\"midi_number\"][:10])\n",
    "#     if i == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgHkMeoJdb42",
    "outputId": "1f3d172f-3f93-47ce-aa1b-e37a2afba24e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2406\nCounter({'Bach': 59, 'Beethoven': 57, 'Chopin': 34, 'Liszt': 16, 'Schubert': 13, 'Haydn': 11, 'Schumann': 10, 'Mozart': 6, 'Ravel': 4, 'Rachmaninoff': 4, 'Debussy': 2, 'Scriabin': 2, 'Glinka': 1, 'Prokofiev': 1, 'Brahms': 1, 'Balakirev': 1})\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_dataset))\n",
    "\n",
    "c = Counter()\n",
    "for p in paths:\n",
    "    c[p.split(\"/\")[0]] +=1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "222 initial pieces\n220 pieces after removing overlapping with musedata and Mozart Fantasie\n"
     ]
    }
   ],
   "source": [
    "# remove pieces from asap that are in Musedata\n",
    "print(len(paths), \"initial pieces\")\n",
    "paths = [p for p in paths if p!= \"Bach/Prelude/bwv_865/xml_score.musicxml\"]\n",
    "\n",
    "#remove mozart Fantasie because of incoherent key signature\n",
    "paths = [p for p in paths if p!= 'Mozart/Fantasie_475/xml_score.musicxml']\n",
    "\n",
    "print(len(paths), \"pieces after removing overlapping with musedata and Mozart Fantasie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GRCM2W3qqeW",
    "outputId": "d53c8146-8b32-43df-cd69-b7423f1ceda2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train and validation lenghts:  163 29\n"
     ]
    }
   ],
   "source": [
    "# Temporary remove composer with only one piece, because they create problems with sklearn stratify\n",
    "one_piece_composers = ['Balakirev','Prokofiev','Brahms','Glinka', 'Debussy', 'Ravel', 'Scriabin','Liszt']\n",
    "paths = [p for p in paths if p.split(\"/\")[0] not in one_piece_composers]\n",
    "\n",
    "# Divide train and validation set\n",
    "path_train, path_validation = sklearn.model_selection.train_test_split(paths, test_size=0.15,stratify=[p.split(\"/\")[0] for p in paths ])\n",
    "print(\"Train and validation lenghts: \",len(path_train),len(path_validation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "KIZ2_bX1bom5",
    "outputId": "ae8d5b50-ed4d-4376-f9d6-e96c6aa73046"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Schubert', 'Haydn', 'Schumann', 'Bach', 'Mozart', 'Chopin', 'Rachmaninoff', 'Beethoven']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 383,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#need to find a better way to visualize this\n",
    "composers = list(set([p.split(\"/\")[0] for p in paths ]))\n",
    "print(composers)\n",
    "\n",
    "train_composer = [composers.index(p.split(\"/\")[0]) for p in path_train]\n",
    "val_composer = [composers.index(p.split(\"/\")[0]) for p in path_validation]\n",
    "\n",
    "_ = plt.hist([train_composer, val_composer], label=['train', 'validation'])\n",
    "_ = plt.legend(loc='upper left')\n",
    "_ = plt.xticks(list(range(len(composers))), composers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-qaT10ApkCY"
   },
   "source": [
    "## Transform the input into a convenient format for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEYec2TDOWvj",
    "outputId": "1b69573b-cfeb-4dc9-8882-da42f9c25537"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}\n15\n"
     ]
    }
   ],
   "source": [
    "from utils import PAD\n",
    "# Helper functions to feed the correct input into the NN \n",
    "\n",
    "N_DURATION_CLASSES = 4\n",
    "\n",
    "pitch_to_ix = {p: accepted_pitches.index(p) for p in accepted_pitches}\n",
    "ks_to_ix = {k: accepted_ks.index(k) for k in accepted_ks}\n",
    "#add PADDING TAD\n",
    "pitch_to_ix[PAD] = len(accepted_pitches)\n",
    "ks_to_ix[PAD] = len(accepted_ks)\n",
    "\n",
    "midi_to_ix = {m: m for m in range(12)}\n",
    "#add PADDING TAD\n",
    "midi_to_ix[PAD] = 12\n",
    "\n",
    "# print(midi_to_ix[1])\n",
    "# print(len(midi_to_ix))\n",
    "\n",
    "\n",
    "\n",
    "# class Pitch2Diatonic():\n",
    "#     def __call__(self, in_seq):\n",
    "#         return [p for p in in_seq]\n",
    "\n",
    "class Pitch2Int():\n",
    "    def __call__(self, in_seq):\n",
    "        idxs = [pitch_to_ix[w] for w in in_seq]\n",
    "        return idxs\n",
    "    \n",
    "class Ks2Int():\n",
    "    def __call__(self, in_seq):\n",
    "        idxs = [ks_to_ix[w] for w in in_seq]\n",
    "        return idxs\n",
    "\n",
    "class Int2Pitch():\n",
    "    def __call__(self, in_seq):\n",
    "        return [accepted_pitches[i] for i in in_seq]\n",
    "\n",
    "class OneHotEncoder():\n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample,weights = None):\n",
    "        onehot = np.zeros([len(sample), self.alphabet_len])\n",
    "        tot_chars = len(sample)\n",
    "        onehot[np.arange(tot_chars), sample] = 1\n",
    "        return onehot\n",
    "    \n",
    "class DurationOneHotEncoder():\n",
    "    def __init__(self, pitch_alphabet_len, n_dur_class = 4):\n",
    "        self.pitch_alphabet_len = pitch_alphabet_len\n",
    "        self.dur_alphabet_len = n_dur_class\n",
    "        \n",
    "    def __call__(self, sample, durs):\n",
    "        sample = torch.tensor(sample,dtype=torch.long)\n",
    "        onehot_pitch = torch.nn.functional.one_hot(sample,self.pitch_alphabet_len)\n",
    "        #compute breaks in duration list\n",
    "        clusters, centroids = kmeans1d.cluster(durs, N_DURATION_CLASSES)   \n",
    "        quantized_durations = torch.tensor(clusters,dtype=torch.long)\n",
    "        onehot_duration = torch.nn.functional.one_hot(quantized_durations,self.dur_alphabet_len)\n",
    "        return torch.cat([onehot_pitch,onehot_duration],1)\n",
    "    \n",
    "        \n",
    "class ToTensorFloat():\n",
    "    def __call__(self, sample, durs = None):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.float()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.float)\n",
    "\n",
    "class ToTensorLong():\n",
    "    def __call__(self, sample):\n",
    "        if type(sample) is torch.Tensor:\n",
    "            return sample.long()\n",
    "        else:\n",
    "            return torch.tensor(sample,dtype=torch.long)\n",
    "    \n",
    "class MultInputCompose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, sample, durs):\n",
    "        for t in self.transforms:\n",
    "            sample = t(sample, durs)\n",
    "        return sample\n",
    "\n",
    "\n",
    "### Define the preprocessing pipeline\n",
    "transform_diat = transforms.Compose([Pitch2Int(),ToTensorLong()])\n",
    "transform_chrom = MultInputCompose([DurationOneHotEncoder(len(midi_to_ix),N_DURATION_CLASSES),ToTensorFloat()])\n",
    "transform_key = transforms.Compose([Ks2Int(),ToTensorLong()])\n",
    "\n",
    "\n",
    "print(set([ks_to_ix[ks] for piece in dict_dataset for ks in piece[\"key_signatures\"]]))\n",
    "print(ks_to_ix[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "@njit\n",
    "def closestMultiple(n : int, x: int):\n",
    "    if x > n: \n",
    "        return x \n",
    "    else:\n",
    "        return int(x*np.ceil(n/x))\n",
    "    \n",
    "\n",
    "closestMultiple(6900,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jV9boYaToUs2",
    "outputId": "92bcd8d3-87a2-4d2c-8e92-4a106139f59f",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "[tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "[tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "[tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([462, 17])\n",
      "torch.Size([462])\n",
      "Division 7.107692307692307\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "462\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "[tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "[tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "[tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n",
      "torch.Size([440, 17])\n",
      "torch.Size([440])\n",
      "Division 6.769230769230769\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])]\n",
      "440\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "from datasets import PSDataset\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,transform_key,True, sort = True)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat,transform_key, False)\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset))\n",
    "\n",
    "\n",
    "# test if it works\n",
    "for chrom,diat,ks,seq_len in train_dataset:\n",
    "    print(chrom.shape)\n",
    "    print(ks.shape)\n",
    "    print(\"Division\", diat.shape[0]/65)\n",
    "#     print(torch.argmax(chrom[0:30],1))\n",
    "#     # print([diatonic_pitches[p.item()] for p in diat[0:30]])\n",
    "#     print([accepted_pitches[p.item()] for p in diat[0:30]])\n",
    "    print([p.item() for p in ks[-20:]])\n",
    "    print([p for p in chrom[-10:,:]])\n",
    "    print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAubyjw2LC8P",
    "outputId": "d6f8574f-8901-41e7-fd7a-9a0c6091eafa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4219, 4, 17]) torch.Size([4219, 4]) torch.Size([4219, 4])\ntensor([[[ 1.,  0.,  0.,  ...,  1.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  1.,  0.,  ...,  1.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  1.,  0.]],\n\n        [[ 1.,  0.,  0.,  ...,  1.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  0.,  1.,  0.]],\n\n        [[ 1.,  0.,  0.,  ...,  0.,  1.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.]],\n\n        ...,\n\n        [[ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [12., 12., 12.,  ..., 12., 12., 12.]],\n\n        [[ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [12., 12., 12.,  ..., 12., 12., 12.]],\n\n        [[ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [12., 12., 12.,  ..., 12., 12., 12.],\n         [12., 12., 12.,  ..., 12., 12., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "def pad_collate(batch):\n",
    "    (chromatic_seq, diatonic_seq,ks_seq, l) = zip(*batch)\n",
    "    \n",
    "    chromatic_seq_pad = pad_sequence(chromatic_seq, padding_value=midi_to_ix[PAD])\n",
    "    diatonic_seq_pad = pad_sequence(diatonic_seq, padding_value=pitch_to_ix[PAD])\n",
    "    ks_seq_pad = pad_sequence(ks_seq, padding_value=ks_to_ix[PAD])\n",
    "\n",
    "    #sort the sequences by length\n",
    "    seq_lengths, perm_idx = torch.Tensor(l).sort(0, descending=True)\n",
    "    chromatic_seq_pad = chromatic_seq_pad[:,perm_idx,:]\n",
    "    diatonic_seq_pad = diatonic_seq_pad[:,perm_idx]\n",
    "    ks_seq_pad = ks_seq_pad[:,perm_idx]\n",
    "\n",
    "    return chromatic_seq_pad, diatonic_seq_pad,ks_seq_pad, seq_lengths\n",
    "\n",
    "data_loader = DataLoader(dataset=validation_dataset,  num_workers =1, batch_size=4, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "#test if it work\n",
    "for batch in data_loader:\n",
    "    print(batch[0].shape,batch[1].shape,batch[2].shape)\n",
    "    print(batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O1OA1AjGWO-"
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QEkFBY5cUsmN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NystromAttention(nn.Module):\n",
    "    def __init__(self, head_dim,num_head,num_landmarks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.num_landmarks = num_landmarks\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        seq_len = mask.shape[1]\n",
    "\n",
    "        Q = Q * mask[:, None, :, None] / np.sqrt(np.sqrt(self.head_dim))\n",
    "        K = K * mask[:, None, :, None] / np.sqrt(np.sqrt(self.head_dim))\n",
    "        \n",
    "        if self.num_landmarks == seq_len:\n",
    "            attn = torch.nn.functional.softmax(torch.matmul(Q, K.transpose(-1, -2)) - 1e9 * (1 - mask[:, None, None, :]), dim = -1)\n",
    "            X = torch.matmul(attn, V)\n",
    "        else:\n",
    "            Q_landmarks = Q.reshape(-1, self.num_head, self.num_landmarks, seq_len // self.num_landmarks, self.head_dim).mean(dim = -2)\n",
    "            K_landmarks = K.reshape(-1, self.num_head, self.num_landmarks, seq_len // self.num_landmarks, self.head_dim).mean(dim = -2)\n",
    "\n",
    "            kernel_1 = torch.nn.functional.softmax(torch.matmul(Q, K_landmarks.transpose(-1, -2)), dim = -1)\n",
    "            kernel_2 = torch.nn.functional.softmax(torch.matmul(Q_landmarks, K_landmarks.transpose(-1, -2)), dim = -1)\n",
    "            kernel_3 = torch.nn.functional.softmax(torch.matmul(Q_landmarks, K.transpose(-1, -2)) - 1e9 * (1 - mask[:, None, None, :]), dim = -1)\n",
    "            X = torch.matmul(torch.matmul(kernel_1, self.iterative_inv(kernel_2)), torch.matmul(kernel_3, V))\n",
    "\n",
    "        return X\n",
    "\n",
    "    def iterative_inv(self, mat, n_iter = 6):\n",
    "        I = torch.eye(mat.size(-1), device = mat.device)\n",
    "        K = mat\n",
    "        V = 1 / (torch.max(torch.sum(torch.abs(K), dim = -2)) * torch.max(torch.sum(torch.abs(K), dim = -1))) * K.transpose(-1, -2)\n",
    "        for _ in range(n_iter):\n",
    "            KV = torch.matmul(K, V)\n",
    "            V = torch.matmul(0.25 * V, 13 * I - torch.matmul(KV, 15 * I - torch.matmul(KV, 7 * I - KV)))\n",
    "        return V\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'num_landmarks={self.num_landmarks}, seq_len={seq_len}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,dim,num_head,num_landmarks):\n",
    "        \"\"\"\n",
    "        dim : if used after a rnn, dim is the hidden dim of the rnn\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "        self.num_landmarks = num_landmarks\n",
    "\n",
    "        self.W_q = nn.Linear(self.dim, self.num_head * self.dim)\n",
    "        self.W_k = nn.Linear(self.dim, self.num_head * self.dim)\n",
    "        self.W_v = nn.Linear(self.dim, self.num_head * self.dim)\n",
    "\n",
    "        self.attn = NystromAttention(self.dim,self.num_head,self.num_landmarks)\n",
    "\n",
    "    def forward(self,X, sentences_len):\n",
    "        #transpose matrix to shape [Batch,seq_len,features]\n",
    "        X = torch.transpose(X,0,1)\n",
    "        # pad to multiple of self.num_landmarks (pad with value \"0\", should we change it?)\n",
    "        padding_length = closestMultiple(X.shape[1],self.num_landmarks)- X.shape[1]\n",
    "        X = F.pad(X, (0,0,0,padding_length,0,0),\"constant\", 0)\n",
    "        # compute padding mask (1 for elements to consider, ignore 0)\n",
    "        pad_mask = torch.arange(X.shape[1])[None,:] < sentences_len[:,None]\n",
    "        \n",
    "        Q = self.split_heads(self.W_q(X))\n",
    "        K = self.split_heads(self.W_k(X))\n",
    "        V = self.split_heads(self.W_v(X))\n",
    "        with torch.cuda.amp.autocast(enabled = False):\n",
    "            attn_out = self.attn(Q.float(), K.float(), V.float(), pad_mask.float().to(device))\n",
    "        out = self.combine_heads(attn_out)\n",
    "        \n",
    "        #slice to the original shape\n",
    "        out = out[:,:int(torch.max(sentences_len)),:]\n",
    "        \n",
    "        #transpose back to shape [seq_len,batch,features]\n",
    "        out = torch.transpose(out,0,1)\n",
    "        return out\n",
    "\n",
    "    def combine_heads(self, X):\n",
    "        X = X.transpose(1, 2)\n",
    "        X = X.reshape(X.size(0), X.size(1), self.num_head * self.dim)\n",
    "        return X\n",
    "\n",
    "    def split_heads(self, X):\n",
    "        X = X.reshape(X.size(0), X.size(1), self.num_head, self.dim)\n",
    "        X = X.transpose(1, 2)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNystromAttentionTagger(nn.Module):\n",
    "    \"\"\"Vanilla RNN + Nystrom Attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix, n_layers =1, num_head =1, num_landmarks =64 ):\n",
    "        super(RNNNystromAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_landmarks = num_landmarks\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim*num_head, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim*num_head, self.n_out_ks)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss_pitch = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = pitch_to_ix[PAD])\n",
    "        self.loss_ks = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = ks_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        self.attention = Attention(self.hidden_dim,num_head,num_landmarks)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "               \n",
    "        # use attention\n",
    "        attn_applied = self.attention(rnn_out,sentences_len)\n",
    "        \n",
    "        out_pitch = self.top_layer_pitch(attn_applied)\n",
    "        out_ks = self.top_layer_ks(attn_applied)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "         # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores_pitch = scores_pitch.view(-1, self.n_out_pitch)\n",
    "        scores_ks = scores_ks.view(-1, self.n_out_ks)\n",
    "        pitches = pitches.view(-1)\n",
    "        keysignatures = keysignatures.view(-1)\n",
    "        return self.loss_pitch(scores_pitch, pitches) + self.loss_ks(scores_ks,keysignatures)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted_pitch = scores_pitch.argmax(dim=2)\n",
    "        predicted_ks = scores_ks.argmax(dim=2)\n",
    "        return [predicted_pitch[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)],[predicted_ks[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMultNystromAttentionTagger(nn.Module):\n",
    "    \"\"\"Pitch-key decoupled model + 2 Nystrom Attention (one att for pich + one att for key)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, pitch_to_ix,ks_to_ix, n_layers =1, hidden_dim2 = 24, num_head =1, num_landmarks =64 ):\n",
    "        super(RNNMultNystromAttentionTagger,self).__init__()    \n",
    "        \n",
    "        self.n_out_pitch = len(pitch_to_ix)\n",
    "        self.n_out_ks = len(ks_to_ix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.num_landmarks = num_landmarks\n",
    "\n",
    "        # RNN layer. Bidirectional GRU\n",
    "        self.rnn = nn.GRU(input_size=input_dim, hidden_size=hidden_dim //2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        self.rnn2 = nn.GRU(input_size=hidden_dim*num_head, hidden_size=hidden_dim2//2, \n",
    "                          bidirectional=True, num_layers=n_layers)\n",
    "        \n",
    "        # Output layer. The input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer_pitch = nn.Linear(hidden_dim*num_head, self.n_out_pitch)\n",
    "        self.top_layer_ks = nn.Linear(hidden_dim2*num_head, self.n_out_ks)\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss_pitch = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = pitch_to_ix[PAD])\n",
    "        self.loss_ks = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index = ks_to_ix[PAD])\n",
    "        \n",
    "        # attention function\n",
    "        self.attention_pitch = Attention(self.hidden_dim,num_head,num_landmarks)\n",
    "        self.attention_ks = Attention(self.hidden_dim2,num_head,num_landmarks)\n",
    "        \n",
    "    def compute_outputs(self, sentences,sentences_len):\n",
    "        sentences = torch.nn.utils.rnn.pack_padded_sequence(sentences, sentences_len)\n",
    "        rnn_out, _ = self.rnn(sentences)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        \n",
    "        # use attention pitch\n",
    "        rnn_out = self.attention_pitch(rnn_out,sentences_len)\n",
    "\n",
    "        out_pitch = self.top_layer_pitch(rnn_out)\n",
    "        \n",
    "        #pass the ks information into the second rnn\n",
    "        rnn_out = torch.nn.utils.rnn.pack_padded_sequence(rnn_out, sentences_len)\n",
    "        rnn_out, _ = self.rnn2(rnn_out)\n",
    "        rnn_out,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        \n",
    "        # use attention ks\n",
    "        attn_applied = self.attention_ks(rnn_out,sentences_len)\n",
    "        \n",
    "        out_ks = self.top_layer_ks(attn_applied)\n",
    "\n",
    "        return out_pitch,out_ks\n",
    "                \n",
    "    def forward(self, sentences, pitches, keysignatures, sentences_len):\n",
    "        # First computes the predictions, and then the loss function.\n",
    "        \n",
    "         # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores_pitch = scores_pitch.view(-1, self.n_out_pitch)\n",
    "        scores_ks = scores_ks.view(-1, self.n_out_ks)\n",
    "        pitches = pitches.view(-1)\n",
    "        keysignatures = keysignatures.view(-1)\n",
    "        return self.loss_pitch(scores_pitch, pitches) + self.loss_ks(scores_ks,keysignatures)\n",
    "\n",
    "    def predict(self, sentences,sentences_len):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores_pitch, scores_ks = self.compute_outputs(sentences,sentences_len)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted_pitch = scores_pitch.argmax(dim=2)\n",
    "        predicted_ks = scores_ks.argmax(dim=2)\n",
    "        return [predicted_pitch[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)],[predicted_ks[:int(l),i].cpu().numpy() for i,l in enumerate(sentences_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXixmVQfvw8T"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3m_nj4HCuCe1"
   },
   "outputs": [],
   "source": [
    "# TODO: search over the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uAMSIlw0AJb6"
   },
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_dataloader, val_dataloader, n_epochs):\n",
    "    history = defaultdict(list)  \n",
    "    for i_epoch in range(1,n_epochs +1):\n",
    "        t0 = time.time()\n",
    "        loss_sum = 0\n",
    "        accuracy_sum = 0\n",
    "        model.train()\n",
    "        for seqs, pitches,keysignatures, lens in train_dataloader: #seqs, pitches, keysignatures, lens are batches\n",
    "            seqs, pitches,keysignatures  = seqs.to(device), pitches.to(device),keysignatures.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(seqs,pitches,keysignatures,lens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_pitch,predicted_ks = model.predict(seqs,lens)\n",
    "                for i,p in enumerate(predicted_pitch):\n",
    "                    acc= accuracy_score(p,pitches[:,i][:len(p)].cpu()) #compute the accuracy without considering the padding\n",
    "                    accuracy_sum += acc/len(lens) #normalize according to the number of sequences in the batch\n",
    "\n",
    "        train_loss = loss_sum/len(train_dataloader)\n",
    "        train_accuracy = accuracy_sum/len(train_dataloader) #normalize according to the number of batches\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        all_predicted_pitch = []\n",
    "        all_predicted_ks = []\n",
    "        all_pitches = []\n",
    "        all_ks = []\n",
    "        with torch.no_grad():\n",
    "            for seqs,pitches,keysignatures, lens in val_dataloader:\n",
    "                # Predict the model's output on a batch\n",
    "                predicted_pitch,predicted_ks = model.predict(seqs.to(device),lens)                   \n",
    "                # Update the lists that will be used to compute the accuracy\n",
    "                for i,(p,k) in enumerate(zip(predicted_pitch,predicted_ks)):\n",
    "                    all_predicted_pitch.append(torch.Tensor(p))\n",
    "                    all_predicted_ks.append(torch.Tensor(k))\n",
    "                    all_pitches.append(pitches[0:int(lens[i]),i])\n",
    "                    all_ks.append(keysignatures[0:int(lens[i]),i])\n",
    "                \n",
    "        # Compute the overall accuracy for the validation set\n",
    "        val_accuracy_pitch = accuracy_score(torch.cat(all_predicted_pitch),torch.cat(all_pitches))\n",
    "        val_accuracy_ks = accuracy_score(torch.cat(all_predicted_ks),torch.cat(all_ks))\n",
    "        history[\"val_accuracy_pitch\"].append(val_accuracy_pitch)\n",
    "        history[\"val_accuracy_ks\"].append(val_accuracy_ks)\n",
    "\n",
    "#         save the model\n",
    "        torch.save(model, \"./models/temp/model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "#         files.download(\"model_temp_epoch{}.pkl\".format(i_epoch))\n",
    "\n",
    "    \n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {i_epoch}: train loss = {train_loss:.4f}, train_accuracy: {train_accuracy:.4f},val_accuracy_pitch: {val_accuracy_pitch:.4f},val_accuracy_ks: {val_accuracy_ks:.4f}, time = {t1-t0:.4f}')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del train_dataset\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(midi_to_ix)+N_DURATION_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "2g2st8znpGW_",
    "outputId": "fe232e42-e270-4f5d-9bd8-718bba5e7151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "n_epochs = 30\n",
    "HIDDEN_DIM = 96 #as it is implemented now, this is double the hidden_dim\n",
    "LEARNING_WEIGHT = 0.05\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "MOMENTUM = 0.9\n",
    "RNN_LAYERS = 1\n",
    "\n",
    "#ks rnn hyperparameter\n",
    "HIDDEN_DIM2= 48\n",
    "\n",
    "#attention hyperparameter\n",
    "NUM_HEAD = 2\n",
    "NUM_LANDMARKS = 64 #should we make this depending on the seq length for each batch? \n",
    "\n",
    "\n",
    "train_dataset = PSDataset(dict_dataset,path_train, transform_chrom,transform_diat,transform_key,True,sort=True, truncate = None)\n",
    "validation_dataset = PSDataset(dict_dataset,path_validation, transform_chrom,transform_diat,transform_key, False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False,collate_fn=pad_collate)\n",
    "\n",
    "# model = torch.load(\"./models/temp/model_temp_epoch30-to_restart.pkl\")\n",
    "\n",
    "# model = RNNTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "# model = RNNMultiTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "# model = RNNNystromAttentionTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS,num_head=NUM_HEAD,num_landmarks=NUM_LANDMARKS)\n",
    "model = RNNMultNystromAttentionTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS,hidden_dim2=HIDDEN_DIM2,num_head=NUM_HEAD,num_landmarks=NUM_LANDMARKS)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_WEIGHT, momentum = MOMENTUM,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = training_loop(model,optimizer,train_dataloader,val_dataloader, n_epochs)\n",
    "\n",
    "# After the final evaluation, we print more detailed evaluation statistics,\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy_pitch'])\n",
    "plt.plot(history['val_accuracy_ks'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy_pitch','validation_accuracy_ks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best working model on the accuracy\n",
    "max_accuracy = np.max(history['val_accuracy_pitch'])\n",
    "best_epoch = np.argmax(history['val_accuracy_pitch'])\n",
    "print(\"Best validation accuracy: \",max_accuracy, \"at epoch\",best_epoch)\n",
    "\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['train_accuracy'])\n",
    "plt.plot(history['val_accuracy_pitch'])\n",
    "plt.plot(history['val_accuracy_ks'])\n",
    "plt.legend(['training loss', 'training accuracy', 'validation_accuracy_pitch','validation_accuracy_ks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tTv2BrGbvSHh",
    "outputId": "801ff32d-24dc-4434-a436-17d9ca832214"
   },
   "outputs": [],
   "source": [
    "# torch.save(model, \"./models/model_asap_crf200dur.pkl\")\n",
    "# files.download(\"model_asap_crf300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXcmLpCKt28l"
   },
   "source": [
    "## Test on Mdata dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./models/model_RNNks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BacUqgD5usGL"
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "with open(Path(basepath,'./datasets/musedata_noisy.pkl'), 'rb') as fid:\n",
    "     full_mdata_dict_dataset = pickle.load( fid)\n",
    "        \n",
    "# add dummy ks to have the same format as asap\n",
    "for e in full_mdata_dict_dataset:\n",
    "    e[\"key_signatures\"] = np.zeros(len(e[\"pitches\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgaUwW2fuwg0",
    "outputId": "89fc5f7d-884a-4ee6-fc34-b3d512f0c4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 different pieces\n",
      "Average number of notes:  907.2777777777778\n"
     ]
    }
   ],
   "source": [
    "mdata_paths = list(set([e[\"original_path\"] for e in full_mdata_dict_dataset ]))\n",
    "\n",
    "# # remove the symbphony No.100 from Haydn because of the enharmonic transposition\n",
    "# paths.remove(\"datasets\\\\opnd\\\\haydndoversyms-10004m.opnd-m\")\n",
    "\n",
    "# print(paths)\n",
    "print(len(mdata_paths), \"different pieces\")\n",
    "print(\"Average number of notes: \", np.mean([len(e[\"midi_number\"]) for e in full_mdata_dict_dataset ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "f-Fg6gJKvNLt"
   },
   "outputs": [],
   "source": [
    "mdata_dataset = PSDataset(full_mdata_dict_dataset,mdata_paths, transform_chrom,transform_diat,transform_key,sort=False, augment_dataset=False)\n",
    "mdata_dataloader = DataLoader(mdata_dataset,  batch_size=2, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ml905Mtdvj-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "all_inputs = []\n",
    "all_predicted_pitch = []\n",
    "all_predicted_ks = []\n",
    "all_pitches = []\n",
    "all_ks = []\n",
    "model.eval() # Evaluation mode (e.g. disable dropout)\n",
    "with torch.no_grad(): # Disable gradient tracking\n",
    "    for seqs, pitches,ks,lens in mdata_dataloader:\n",
    "        # Move data to device\n",
    "        seqs = seqs.to(device)\n",
    "\n",
    "        # Predict the model's output on a batch.\n",
    "        predicted_pitch,predicted_ks = model.predict(seqs,lens)                   \n",
    "        # Update the evaluation statistics.\n",
    "        for i,p in enumerate(predicted_pitch):\n",
    "            all_inputs.append(torch.argmax(seqs[0:int(lens[i]),i,:].cpu(),1).numpy())\n",
    "            all_predicted_pitch.append(p)\n",
    "            all_predicted_ks.append(predicted_ks[i])\n",
    "            all_pitches.append(pitches[0:int(lens[i]),i])\n",
    "            all_ks.append(ks[0:int(lens[i]),i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsefdSHxvrWy",
    "outputId": "33785b90-5e03-4a6d-86dd-92b6b763a505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['han', 'bac', 'cor', 'bee', 'viv', 'moz', 'tel', 'hay']\n"
     ]
    }
   ],
   "source": [
    "# # Divide accuracy according to author\n",
    "authors = []\n",
    "\n",
    "for sequence in all_inputs:\n",
    "#     print(sequence)\n",
    "    author = [e[\"original_path\"].split(\"\\\\\")[-1][:3] for e in full_mdata_dict_dataset\n",
    "              if len(e[\"midi_number\"]) == len(sequence) and\n",
    "              list(e[\"midi_number\"]) ==list(sequence) ]\n",
    "    assert len(author) == 1\n",
    "    authors.append(author[0])\n",
    "\n",
    "considered_authors = list(set(authors))\n",
    "print(considered_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgNt_yG7vrfd",
    "outputId": "8473393f-dac6-4930-9d9e-da9c1046f1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch Statistics----------------\n",
      "{'han': 15, 'bac': 13, 'cor': 4, 'bee': 99, 'viv': 29, 'moz': 70, 'tel': 8, 'hay': 273}\n",
      "{'han': 0.9993877551020408, 'bac': 0.9994694960212201, 'cor': 0.9998366880333156, 'bee': 0.9959580288245621, 'viv': 0.9988161815732539, 'moz': 0.9971421572630031, 'tel': 0.9996734693877551, 'hay': 0.9888525928950592}\n",
      "{'han': 24500, 'bac': 24505, 'cor': 24493, 'bee': 24493, 'viv': 24497, 'moz': 24494, 'tel': 24500, 'hay': 24490}\n",
      "Total errors : 511\n",
      "Error rate:\n",
      "{'han': 0.0612244897959191, 'bac': 0.053050397877985045, 'cor': 0.01633119666843985, 'bee': 0.40419711754379195, 'viv': 0.1183818426746086, 'moz': 0.2857842736996852, 'tel': 0.03265306122448575, 'hay': 1.1147407104940776}\n",
      "Total error rate: 0.2607515359337048\n"
     ]
    }
   ],
   "source": [
    "errors_per_author_pitch = {}\n",
    "accuracy_per_author_pitch = {}\n",
    "notes_per_author = {}\n",
    "for ca in considered_authors:\n",
    "    ca_predicted_pitch = np.concatenate([all_predicted_pitch[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_predicted_ks = np.concatenate([all_predicted_ks[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_pitches = np.concatenate([all_pitches[i] for i,a in enumerate(authors) if a == ca])\n",
    "    ca_ks = np.concatenate([all_ks[i] for i,a in enumerate(authors) if a == ca])\n",
    "\n",
    "    ca_acc_pitch = accuracy_score(ca_predicted_pitch,ca_pitches)\n",
    "    \n",
    "    accuracy_per_author_pitch[ca] = float(ca_acc_pitch)\n",
    "    errors_per_author_pitch[ca] = int(len(ca_pitches) - sum(np.equal(ca_predicted_pitch,ca_pitches)))\n",
    "    notes_per_author[ca] = len(ca_pitches)\n",
    "\n",
    "print(\"Pitch Statistics----------------\")\n",
    "print(errors_per_author_pitch)\n",
    "print(accuracy_per_author_pitch)\n",
    "print(notes_per_author)\n",
    "print(\"Total errors :\", sum([e for e in errors_per_author_pitch.values()]))\n",
    "print(\"Error rate:\")\n",
    "print({k:(1-accuracy_per_author_pitch[k])*100 for k in accuracy_per_author_pitch.keys() })\n",
    "\n",
    "print(\"Total error rate:\", sum(errors_per_author_pitch.values())/sum(notes_per_author.values())*100 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5s0d56evrje"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVvP1eH8vrl3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEst accuracy with ks\n",
    "n_epochs = 30\n",
    "HIDDEN_DIM = 96\n",
    "LEARNING_WEIGHT = 0.05\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "MOMENTUM = 0.9\n",
    "RNN_LAYERS = 1\n",
    "\n",
    "model = RNNMultiTagger(len(midi_to_ix)+N_DURATION_CLASSES,HIDDEN_DIM,pitch_to_ix,ks_to_ix, n_layers =RNN_LAYERS)\n",
    "\n",
    "Model available in: \"\"./models/model_RNNks.pkl\"\"\n",
    "accuracy on validation set 0.9424\n",
    "Trained on all asap dataset\n",
    "\n",
    "{'cor': 4, 'viv': 29, 'moz': 70, 'bac': 13, 'han': 15, 'bee': 99, 'hay': 273, 'tel': 8}\n",
    "{'cor': 0.9998366880333156, 'viv': 0.9988161815732539, 'moz': 0.9971421572630031, 'bac': 0.9994694960212201, 'han': 0.9993877551020408, 'bee': 0.9959580288245621, 'hay': 0.9888525928950592, 'tel': 0.9996734693877551}\n",
    "{'cor': 24493, 'viv': 24497, 'moz': 24494, 'bac': 24505, 'han': 24500, 'bee': 24493, 'hay': 24490, 'tel': 24500}\n",
    "Total errors : 511\n",
    "\n",
    "\n",
    "Epoch 22: train loss = 0.4723, train_accuracy: 0.9533,val_accuracy_pitch: 0.9424,val_accuracy_ks: 0.7938, time = 106.7093\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTDh55Hh2OWCo66dGc+SES",
   "include_colab_link": true,
   "name": "rnncrf_pitch_spelling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('dl': venv)",
   "language": "python",
   "name": "python38364bitdlvenv228eca0427fb4ce49e2cb1133db5536b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}